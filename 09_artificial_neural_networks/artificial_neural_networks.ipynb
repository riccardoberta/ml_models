{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "An **Artificial Neural Networs** (ANN) is a ML model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "1. [Biological neurons](#Biological-neurons)\n",
    "2. [Perceptron](#Perceptron)\n",
    "3. [Multilayer Perceptron](#Multilayer-Perceptron)\n",
    "4. [Backpropagation](#Backpropagation)\n",
    "5. [Activation Functions](#Activation-Functions)\n",
    "6. [Regression and Classification](#Regression-and-Classification)\n",
    "7. [Tuning Hyperparameters](#Tuning-Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological neurons\n",
    "A neuron is a cell  found in animal brains. It’s composed of a **cell body** containing the **nucleus** and most of the cell’s complex components, many branching extensions called **dendrites**, plus one very long extension called the **axon**. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called **telodendria**, and at the tip of these branches are minuscule structures called **synapses**, which are\n",
    "connected to the dendrites or cell bodies of other neurons. \n",
    "\n",
    "<img src=\"biological-neuron.png\" width=\"600\">\n",
    "\n",
    "Biological neurons produce short electrical impulses called **action potentials** which travel along the axons and make the synapses release chemical signals called **neurotransmitters**. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing).\n",
    "\n",
    "Thus, individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons. The architecture of biological neural networks is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, especially in the cerebral cortex.\n",
    "\n",
    "ANN are inspired by this networks of biological neurons, however ANNs have\n",
    "gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "The Perceptron is one of the simplest ANN architectures, invented in 1957\n",
    "by Frank Rosenblatt. It is based on a artificial neuron called **threshold logic unit** (TLU) or **linear threshold unit** (LTU). The inputs ($x$) and output ($y$) are numbers, and each input connection is associated with a weight ($w$. The TLU computes a weighted sum of its inputs: \n",
    "\n",
    "$z = w _1 x_1 + w_2 x_2 + ⋯ + w_n x_n = X^TW$\n",
    "\n",
    "then applies a step function to that sum and outputs the result:\n",
    "\n",
    "$y=f(z)=\\text{step}(z)$\n",
    "\n",
    "<img src=\"tlu.png\" width=\"600\">\n",
    "\n",
    "The most common step function used in Perceptrons is the **heaviside step\n",
    "function**, sometimes the **sign function** is used instead.\n",
    "\n",
    "$\\text{heaviside}(z) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if} & z \\lt 0 \\\\\n",
    "    1 & \\text{if} & z \\gt 1\n",
    "\\end{cases}$\n",
    "\n",
    "$\\text{sgn}(z) = \n",
    "\\begin{cases}\n",
    "    -1 & \\text{if} & z \\lt 0 \\\\\n",
    "     0 & \\text{if} & z = 0 \\\\\n",
    "     1 & \\text{if} & z \\gt 1\n",
    "\\end{cases}$\n",
    "\n",
    "A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise it outputs the negative class (just like a Logistic Regression or linear SVM classifier). Training a TLU means finding the right values for $w_1$, $w_2$, ..., $w_n$.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a **fully connected layer**, or a **dense layer**. The inputs of the\n",
    "Perceptron are fed to special passthrough neurons called **input neurons**: they output whatever input they are fed. All the input neurons form the input\n",
    "layer. Moreover, an extra bias feature is generally added ($x_0=1$): it is\n",
    "typically represented using a special type of neuron called a **bias neuron**, which outputs 1 all the time. A Perceptron with two inputs and three outputs is represented in the following figure. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.\n",
    "\n",
    "<img src=\"perceptron.png\" width=\"500\">\n",
    "\n",
    "$h_{W,b}=\\phi(XW+b)$\n",
    "\n",
    "where:\n",
    "- X represents the matrix of input features, it has one row per instance and one column per feature;\n",
    "- W is the weight matrix, it contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer;\n",
    "- b is the bias vector, it contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron;\n",
    "- $\\phi$ is the activation function: when the artificial neurons are TLUs, it is a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are trained taking into account the error made by the network when it makes a prediction: the learning rule **reinforces** connections that help reduce the error. More specifically, the Perceptron is fed one training\n",
    "instance at a time, and for each instance it makes its predictions. For every\n",
    "output neuron that produced a wrong prediction, it reinforces the connection\n",
    "weights from the inputs that would have contributed to the correct prediction (**delta rule**):\n",
    "\n",
    "$\\boxed{w_{ij}=w_{ij}+\\eta(y_j-t_j)x_i}$\n",
    "\n",
    "where:\n",
    "- $w_{ij}$ is the connection weight between the input neuron $i$ and the output neuron $j$;\n",
    "- $x_i$ is the input value $i$ of the current training instance;\n",
    "- $y_j$ is the output of the output neuron $j$ for the current training instance;\n",
    "- $t_j$ is the target output of the output neuron $j$ for the current training instance;\n",
    "- $\\eta$ is the **learning rate**.\n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are\n",
    "incapable of learning complex patterns (just like Logistic Regression\n",
    "classifiers). However, if the training instances are linearly separable,\n",
    "Rosenblatt demonstrated that this algorithm would converge to a solution.\n",
    "This is called the **Perceptron convergence theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a **Perceptron class** that implements a single-TLU\n",
    "network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "perceptron_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "perceptron_clf.fit(X, y)\n",
    "\n",
    "y_pred = perceptron_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization). Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer Logistic Regression over Perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "A **Multilayer Perceptron** (MLP) is composed of one (passthrough) input layer, one or more layers of TLUs, called **hidden layers**, and one final layer of TLUs called the **output layer**. Note the signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a **feedforward neural network** (FNN).\n",
    "\n",
    "<img src=\"mlp.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "To train MLPs we can use the **backpropagation algorithm**, it is Gradient Descent using an efficient technique for computing the gradients automatically (**automatic differentiation** or **autodiff**): in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Let’s run through this algorithm in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Backpropagation handles one mini-batch at a time, and it goes through the full training set multiple times. Each pass is called an **epoch**. Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is **the forward pass**: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "<img src=\"backpropagation-1.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Next, the algorithm measures the network’s output error (i.e., it uses a **loss function** that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "<img src=\"backpropagation-2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) It is impossible to compute error signal for internal neurons directly, becouse output values of these neurons are unknown. The idea is to propagate error signal back to all neurons, working backward until the algorithm reaches the input layer. The weights used to propagate back are equal to these used during the computation of the output value. Only the direction of the data flow is changed. This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "<img src=\"backpropagation-3.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "The rule used is actually similar to the conventional delta rule used for the perceptron, it updates the weights in proportion to the learning rate, the input to which the weight is applied, and the error in the output of the unit. The major difference is that the simple error term is replaced by\n",
    "the more complex error term previously calculated. \n",
    "\n",
    "<img src=\"backpropagation-4.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to derive the backprogation training rule. Recall the notation:\n",
    "- $x_{ij}$ is the input $i$ to unit $j$\n",
    "- $w_{ij}$ is the weight associated with input $i$ to unit $j$\n",
    "- $z_j$ is the weighted sum of input for unit $j$ $z_j=\\sum\\limits_{i}{w_{ij}x_{ij}}$\n",
    "- $y_j$ is the output computeted by unit $j$ $y_j=f(z_j)$\n",
    "- $t_j$ is the target value of the output unit $j$\n",
    "\n",
    "We start from the **stochastic gradient descent** rule, which involves iterating through the examples in the training set, for each training example descending the gradient of the error function with respect to this example. More specifically, for each example, every weight $w_{ij}$ is updated by adding to it the value $\\Delta w_{ij}$:\n",
    "\n",
    "$\\begin{align}\n",
    "\\Delta w_{ij}=-\\eta \\frac{\\partial J}{\\partial w_{ij}}\n",
    "\\end{align}$      (1)\n",
    "\n",
    "where $J$ is the error on the training example, summed over all output units ($O$) in the output layer of the network:\n",
    "\n",
    "$\\begin{align}\n",
    "J=\\frac{1}{2}\\sum\\limits_{k\\epsilon O}{(t_k-y_k)^2}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now derive the partial derivative term in the gradient descent rule. Notice that weight $w_{ij}$ can influence the network output only through $y_j$. Using the chain rule we can write:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_{ij}}=\\frac{\\partial J}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ij}}=\\frac{\\partial J}{\\partial z_j}x_{ij}=\\delta_jx_{ij}\n",
    "\\end{align}$     (2)\n",
    "\n",
    "we can substitute (2) in (1):\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\Delta w_{ij}=-\\eta\\delta_jx_{ij}}\n",
    "\\end{align}$     \n",
    "\n",
    "the input $x_{ij}$ of an input unit is the input $x_i$ and of an internal or output units is the output $y_i$ of the previous layer.\n",
    "\n",
    "Our objective is now to derive the $\\delta_j$ term. We can consider two different case: the case for an output unit and the case for an internal unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1, output unit**\n",
    "\n",
    "Using the chain rule again we obtain:\n",
    "\n",
    "$\\begin{align}\n",
    "\\delta_j=\\frac{\\partial J}{\\partial z_j}=\\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j}=\\frac{\\partial J}{\\partial y_j}y_j(1-y_j)\n",
    "\\end{align}$     (3)\n",
    "\n",
    "notice that the derivative of $y_j$ is just the derivative of the activation function. In order for this algorithm to work properly, we need to replace the step function with the logistic function( **sigmoid**). This was essential because the step function contains only flat segments, so there is no gradient to work with, while the logistic function has a well-defined nonzero derivative everywhere.\n",
    "\n",
    "$\\begin{align}\n",
    "\\sigma (x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{d\\sigma (x)}{d(x)} = \\sigma (x)\\cdot (1-\\sigma(x))\n",
    "\\end{align}$\n",
    "\n",
    "we now proceed with finding the last derivative:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial y_j}=\\frac{\\partial}{\\partial y_j}\\frac{1}{2}\\sum\\limits_{k\\epsilon O}{(t_k-y_k)^2}=\\frac{\\partial}{\\partial y_j}\\frac{1}{2}(t_j-y_j)^2=\\frac{1}{2}2(t_j-y_j)\\frac{\\partial}{\\partial y_j}(t_j-y_j)=-(t_j-y_j)\n",
    "\\end{align}$     (4)\n",
    "\n",
    "the summation term over output units is dropped because the derivatives is zero for all output units except for the case when $k=j$. By substituting (4) into (3) we obtain:\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\delta_j=-(t_j-y_j)y_j(1-y_j)}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2, internal unit**\n",
    "\n",
    "When unit $j$ is internal, we must also consider every unit immediately downstream of it (all units whose direct input include the output that unit). This is because a change in $w_{ji}$ (through $z_j$) influences the network outputs through these units. Let $ds(j)$ denote units downstream of unit $j$, then:\n",
    "\n",
    "$\\begin{align}\n",
    "\\delta_j=\\frac{\\partial J}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)} \\frac{\\partial J}{\\partial z_k}\\frac{\\partial z_k}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)}\\delta _k \\frac{\\partial z_k}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)}\\delta_kw_{jk}y_j(1-y_j)  \n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\delta_j=y_j(1-y_j)\\sum\\limits_{k\\epsilon ds(j)}\\delta_kw_{jk}}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to initialize all the connection weights randomly, or else training will fail. For example, if we initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. If instead we randomly initialize the weights, we break the symmetry and allow backpropagation to train a diverse team of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "The backpropagation algorithm works well with many other activation functions, not just the logistic function. Here are two other popular choices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **hyperbolic tangent function** is a S-shaped, continuous and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "\n",
    "The **Rectified Linear Unit (ReLU) function** is continuous but unfortunately not differentiable at $x=0$ (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for $z<0$. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative(f, x, eps=0.000001):\n",
    "    return (f(x + eps) - f(x - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x, sign(x), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(x, sigmoid(x), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(x, tanh(x), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(x, relu(x), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(x, derivative(sign, x), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(x, derivative(sigmoid, x), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(x, derivative(tanh, x), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(x, derivative(relu, x), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need activation functions? Well, if we chain several linear transformations, all we get is a linear transformation. For example, if $f(x)=2x+3$ and $g(x)=5x–1$, then chaining these two linear functions gives us another linear function: $f(g(x))=2(5x–1)+3=10x+1$. So if we don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single\n",
    "layer, and we can’t solve very complex problems with that. Conversely, a large enough ANN with nonlinear activations can theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. \n",
    "For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around theo bject, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.\n",
    "In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values. If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Finally, if you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, and then scale the labels to the appropriate range.\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the **Huber loss**, which is a combination of both: it is quadratic when the error is smaller than a threshold, but linear when the error is larger than that. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error.\n",
    "The following are the typical hyperparameters of a regression MLP architecture:\n",
    "- number of input neurons: one per input feature (e.g. 28x28 for MNIST)\n",
    "- number of hidden layers: depends on the problem, typically 1 to 5\n",
    "- number of neurons per hidden layer: depends on the problem, typically 10 to 100\n",
    "- number of output neurons: 1 per prediction dimension\n",
    "- hidden activation function: ReLU\n",
    "- output activation function: ReLU, Logistic or Tanh\n",
    "- loss function: MSE ore Huber (if outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number. MLPs can also easily handle multilabel binary classification tasks: you would dedicate one output neuron for each positive class.  For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. \n",
    "Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1 (which is required if the classes are exclusive). This is called multiclass classification.\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss is generally a good choice.\n",
    "The following are the typical hyperparameters of a classification MLP architecture:\n",
    "- number of input neurons: one per input feature (e.g. 28x28 for MNIST)\n",
    "- number of hidden layers: depends on the problem, typically 1 to 5\n",
    "- number of neurons per hidden layer: depends on the problem, typically 10 to 100\n",
    "- number of output neurons: 1 (binary classification), 1 per label (multilabel binary), 1 per class (multiclass)\n",
    "- hidden activation function: ReLU\n",
    "- output activation function: Logistic (binary and multilabel) Softmax (multiclass)\n",
    "- loss function: cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "\n",
    "The flexibility of SNN is also one of their main drawbacks: there are many hyperparameters to tweak. We can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. \n",
    "\n",
    "How do we know what combination of hyperparameters is the best for our task? One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation). \n",
    "\n",
    "However, it  helps to have an idea of what values are reasonable for each hyperparameter so that we can build a quick prototype and restrict the search space. The following sections provide guidelines for choosing the number of hidden layers and neurons in an MLP and for selecting good values for some of the main hyperparameters.\n",
    "\n",
    "For a review of best practices regarding tuning neural network hyperparameters, check out this excellent paper [A Disciplined Approach to Neural Network Hyperparameters](https://arxiv.org/abs/1803.09820)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers\n",
    "For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, **deep networks have a much higher parameter efficiency than shallow ones**: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. The reason is that real-world data is often structured in a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces). This also improves the ability of deeper network to generalize well to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures, it will only have to learn the higher-level structures. This is called **transfer learning**.\n",
    "In summary, for many problems you can start with just one or two hidden\n",
    "layers, for more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set.\n",
    "Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds) and they need a huge amount of training data. We will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons per Hidden Layer\n",
    "The number of neurons in the input and output layers is determined by the type of input and output your task requires.  As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice, it’s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.\n",
    "We can adopt the **stretch pants approach**: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.\n",
    "Anywaym, in general it is better to increase the number of layers instead of the number of neurons per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., $10^-5$ ) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration. If we plot the loss as a function of the learning rate, we should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoo back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it\n",
    "normally using this good learning rate.\n",
    "Anyway, the optimal learning rate depends on the other hyperparameters (especially the batch size) so if you modify any hyperparameter, make sure to update the learning rate as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Choosing a better optimizer than plain old Mini-batch Gradient Descent\n",
    "is also quite important. We will see several advanced optimizers in more advanced topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size\n",
    "The batch size can have a significant impact on our model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators (like GPUs) can process them efficiently. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. However, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. \n",
    "A possible strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "In general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of iterations\n",
    "In most cases, the number of training iterations does not actually need to\n",
    "be tweaked: just use early stopping instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
