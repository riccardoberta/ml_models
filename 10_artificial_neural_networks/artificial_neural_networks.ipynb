{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/riccardoberta/machine-learning/blob/master/11_artificial_neural_networks/artificial_neural_networks.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Google Colab\"/></a>\n",
    "\n",
    "An **Artificial Neural Networs** (ANN) is a ML model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "1. [Biological neurons](#Biological-neurons)\n",
    "2. [Perceptron](#Perceptron)\n",
    "3. [Multilayer Perceptron](#Multilayer-Perceptron)\n",
    "4. [Backpropagation](#Backpropagation)\n",
    "5. [Activation Functions](#Activation-Functions)\n",
    "6. [Regression and Classification](#Regression-and-Classification)\n",
    "7. [Tuning Hyperparameters](#Tuning-Hyperparameters)\n",
    "8. [From Scratch](#From-Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological neurons\n",
    "A neuron is a cell  found in animal brains. It’s composed of a **cell body** containing the **nucleus** and most of the cell’s complex components, many branching extensions called **dendrites**, plus one very long extension called the **axon**. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called **telodendria**, and at the tip of these branches are minuscule structures called **synapses**, which are\n",
    "connected to the dendrites or cell bodies of other neurons. \n",
    "\n",
    "<img src=\"images/biological-neuron.png\" width=\"600\">\n",
    "\n",
    "Biological neurons produce short electrical impulses called **action potentials** which travel along the axons and make the synapses release chemical signals called **neurotransmitters**. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing).\n",
    "\n",
    "Thus, individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons. The architecture of biological neural networks is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, especially in the cerebral cortex.\n",
    "\n",
    "ANN are inspired by this networks of biological neurons, however ANNs have\n",
    "gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "The Perceptron is one of the simplest ANN architectures, invented in 1957\n",
    "by Frank Rosenblatt. It is based on a artificial neuron called **threshold logic unit** (TLU) or **linear threshold unit** (LTU). The inputs ($x$) and output ($y$) are numbers, and each input connection is associated with a weight ($w$. The TLU computes a weighted sum of its inputs: \n",
    "\n",
    "$z = w _1 x_1 + w_2 x_2 + ⋯ + w_n x_n = X^TW$\n",
    "\n",
    "then applies a step function to that sum and outputs the result:\n",
    "\n",
    "$y=f(z)=\\text{step}(z)$\n",
    "\n",
    "<img src=\"images/tlu.png\" width=\"600\">\n",
    "\n",
    "The most common step function used in Perceptrons is the **heaviside step\n",
    "function**, sometimes the **sign function** is used instead.\n",
    "\n",
    "$\\text{heaviside}(z) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if} & z \\lt 0 \\\\\n",
    "    1 & \\text{if} & z \\gt 1\n",
    "\\end{cases}$\n",
    "\n",
    "$\\text{sgn}(z) = \n",
    "\\begin{cases}\n",
    "    -1 & \\text{if} & z \\lt 0 \\\\\n",
    "     0 & \\text{if} & z = 0 \\\\\n",
    "     1 & \\text{if} & z \\gt 1\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise it outputs the negative class (just like a Logistic Regression or linear SVM classifier). Training a TLU means finding the right values for $w_1$, $w_2$, ..., $w_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a **fully connected layer**, or a **dense layer**. The inputs of the\n",
    "Perceptron are fed to special passthrough neurons called **input neurons**: they output whatever input they are fed. All the input neurons form the input\n",
    "layer. Moreover, an extra bias feature is generally added ($x_0=1$): it is\n",
    "typically represented using a special type of neuron called a **bias neuron**, which outputs 1 all the time. A Perceptron with two inputs and three outputs is represented in the following figure. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.\n",
    "\n",
    "<img src=\"images/perceptron.png\" width=\"500\">\n",
    "\n",
    "$h_{W,b}=\\phi(XW+b)$\n",
    "\n",
    "where:\n",
    "- X represents the matrix of input features, it has one row per instance and one column per feature;\n",
    "- W is the weight matrix, it contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer;\n",
    "- b is the bias vector, it contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron;\n",
    "- $\\phi$ is the activation function: when the artificial neurons are TLUs, it is a step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from scratch.linear_algebra import Vector, dot\n",
    "\n",
    "def step(z: float) -> float:\n",
    "    return 1.0 if z >= 0 else 0.0\n",
    "\n",
    "def perceptron(w: Vector, bias: float, x: Vector) -> float:\n",
    "    z = dot(w, x) + bias\n",
    "    return step(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With properly chosen weights, perceptrons can solve a number of simple problems. For example, we can create an AND gate with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "print(perceptron(and_weights, and_bias, [1, 1]))\n",
    "print(perceptron(and_weights, and_bias, [0, 1]))\n",
    "print(perceptron(and_weights, and_bias, [1, 0]))\n",
    "print(perceptron(and_weights, and_bias, [0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using similar reasoning, we could build an OR gate with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "print(perceptron(or_weights, or_bias, [1, 1]))\n",
    "print(perceptron(or_weights, or_bias, [0, 1]))\n",
    "print(perceptron(or_weights, or_bias, [1, 0]))\n",
    "print(perceptron(or_weights, or_bias, [0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some problems that simply can’t be solved by a single perceptron. For example, no matter how hard you try, you cannot use a perceptron to build an XOR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are trained taking into account the error made by the network when it makes a prediction: the learning rule **reinforces** connections that help reduce the error. More specifically, the Perceptron is fed one training\n",
    "instance at a time, and for each instance it makes its predictions. For every\n",
    "output neuron that produced a wrong prediction, it reinforces the connection\n",
    "weights from the inputs that would have contributed to the correct prediction (**delta rule**):\n",
    "\n",
    "$\\boxed{w_{ij}=w_{ij}+\\eta(y_j-t_j)x_i}$\n",
    "\n",
    "where:\n",
    "- $w_{ij}$ is the connection weight between the input neuron $i$ and the output neuron $j$;\n",
    "- $x_i$ is the input value $i$ of the current training instance;\n",
    "- $y_j$ is the output of the output neuron $j$ for the current training instance;\n",
    "- $t_j$ is the target output of the output neuron $j$ for the current training instance;\n",
    "- $\\eta$ is the **learning rate**.\n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are\n",
    "incapable of learning complex patterns (just like Logistic Regression\n",
    "classifiers). However, if the training instances are linearly separable,\n",
    "Rosenblatt demonstrated that this algorithm would converge to a solution.\n",
    "This is called the **Perceptron convergence theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a **Perceptron class** that implements a single-TLU\n",
    "network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(int)\n",
    "\n",
    "perceptron_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "perceptron_clf.fit(X, y)\n",
    "\n",
    "y_pred = perceptron_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization). Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer Logistic Regression over Perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "A **Multilayer Perceptron** (MLP) is composed of one (passthrough) input layer, one or more layers of TLUs, called **hidden layers**, and one final layer of TLUs called the **output layer**. Note the signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a **feedforward neural network** (FNN).\n",
    "\n",
    "<img src=\"images/mlp.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the perceptron, for each neuron we’ll sum up the products of its inputs and its weights. But here, rather than outputting the step function applied to that product, we’ll output a smooth approximation of it, like the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 10.0, -0.2, 1.2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 200).tolist()\n",
    "y_step =  [step(point) for point in x]\n",
    "y_sigmoid =  [sigmoid(point) for point in x]\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.plot(x, y_step, \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(x, y_sigmoid, \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Step and Sigmoid functions\", fontsize=14)\n",
    "plt.axis([-10, 10, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use sigmoid instead of the simpler step function? In order to train a neural network, we need to use calculus, and in order to use calculus, we need smooth functions. Step function isn’t even continuous, and sigmoid is a good smooth approximation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(w: Vector, x: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(w, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this function, we can represent a neuron simply as a vector of weights whose length is one more than the number of inputs to that neuron (because of the bias weight). Then we can represent a neural network as a list of (noninput) layers,\n",
    "where each layer is just a list of the neurons in that layer.\n",
    "\n",
    "That is, we’ll represent a neural network as a list (layers) of lists (neurons) of vectors (weights). Given such a representation, using the neural network is quite\n",
    "simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]], input_vector: Vector) -> List[Vector]:\n",
    "    outputs: List[Vector] = []\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              \n",
    "        output = [neuron(n, input_with_bias) for n in layer]               \n",
    "        outputs.append(output)\n",
    "        input_vector = output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we cab build the XOR gate that we couldn’t build with a single perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38314668300676e-14\n",
      "0.9999999999999059\n",
      "0.9999999999999059\n",
      "9.383146683006828e-14\n"
     ]
    }
   ],
   "source": [
    "xor_network = [# hidden layer\n",
    "               [[20., 20, -30],      \n",
    "                [20., 20, -10]],     \n",
    "               # output layer\n",
    "               [[-60., 60, -30]]]\n",
    "\n",
    "print(feed_forward(xor_network, [0, 0])[-1][0])\n",
    "print(feed_forward(xor_network, [1, 0])[-1][0])\n",
    "print(feed_forward(xor_network, [0, 1])[-1][0])\n",
    "print(feed_forward(xor_network, [1, 1])[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One suggestive way of thinking about this is that the hidden layer is computing features of the input data and the output layer is combining those features in a way that generates the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we don’t build neural networks by hand. This is in part because we use them to solve much bigger problems, and in part because we usually won’t be able to reason out what the neurons should be. Instead, we use data to train neural networks. The typical approach is an algorithm called backpropagation, which uses gradient descent or one of its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Imagine we have a training set that consists of input vectors and corresponding target output vectors. Imagine that our network has some set of weights. Backpropagation, in just two passes through the network (one forward, one backward), is able to compute the gradient of the network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Let’s run through this algorithm in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Backpropagation handles one mini-batch at a time, and it goes through the full training set multiple times. Each pass is called an **epoch**. Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is **the forward pass**: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "<img src=\"images/backpropagation-1.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Next, the algorithm measures the network’s output error (i.e., it uses a **loss function** that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "<img src=\"images/backpropagation-2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) It is impossible to compute error signal for internal neurons directly, becouse output values of these neurons are unknown. The idea is to propagate error signal back to all neurons, working backward until the algorithm reaches the input layer. The weights used to propagate back are equal to these used during the computation of the output value. Only the direction of the data flow is changed. This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "<img src=\"images/backpropagation-3.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "The rule used is actually similar to the conventional delta rule used for the perceptron, it updates the weights in proportion to the learning rate, the input to which the weight is applied, and the error in the output of the unit. The major difference is that the simple error term is replaced by\n",
    "the more complex error term previously calculated. \n",
    "\n",
    "<img src=\"images/backpropagation-4.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to derive the backprogation training rule. Recall the notation:\n",
    "- $x_{ij}$ is the input $i$ to unit $j$\n",
    "- $w_{ij}$ is the weight associated with input $i$ to unit $j$\n",
    "- $z_j$ is the weighted sum of input for unit $j$ $z_j=\\sum\\limits_{i}{w_{ij}x_{ij}}$\n",
    "- $y_j$ is the output computeted by unit $j$ $y_j=f(z_j)$\n",
    "- $t_j$ is the target value of the output unit $j$\n",
    "\n",
    "We start from the **stochastic gradient descent** rule, which involves iterating through the examples in the training set, for each training example descending the gradient of the error function with respect to this example. More specifically, for each example, every weight $w_{ij}$ is updated by adding to it the value $\\Delta w_{ij}$:\n",
    "\n",
    "$\\begin{align}\n",
    "\\Delta w_{ij}=-\\eta \\frac{\\partial J}{\\partial w_{ij}}\n",
    "\\end{align}$      (1)\n",
    "\n",
    "where $J$ is the error on the training example, summed over all output units ($O$) in the output layer of the network:\n",
    "\n",
    "$\\begin{align}\n",
    "J=\\frac{1}{2}\\sum\\limits_{k\\epsilon O}{(t_k-y_k)^2}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now derive the partial derivative term in the gradient descent rule. Notice that weight $w_{ij}$ can influence the network output only through $y_j$. Using the chain rule we can write:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_{ij}}=\\frac{\\partial J}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ij}}=\\frac{\\partial J}{\\partial z_j}x_{ij}=\\delta_jx_{ij}\n",
    "\\end{align}$     (2)\n",
    "\n",
    "we can substitute (2) in (1):\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\Delta w_{ij}=-\\eta\\delta_jx_{ij}}\n",
    "\\end{align}$     \n",
    "\n",
    "the input $x_{ij}$ of an input unit is the input $x_i$ and of an internal or output units is the output $y_i$ of the previous layer.\n",
    "\n",
    "Our objective is now to derive the $\\delta_j$ term. We can consider two different case: the case for an output unit and the case for an internal unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1, output unit**\n",
    "\n",
    "Using the chain rule again we obtain:\n",
    "\n",
    "$\\begin{align}\n",
    "\\delta_j=\\frac{\\partial J}{\\partial z_j}=\\frac{\\partial J}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j}=\\frac{\\partial J}{\\partial y_j}y_j(1-y_j)\n",
    "\\end{align}$     (3)\n",
    "\n",
    "notice that the derivative of $y_j$ is just the derivative of the activation function. In order for this algorithm to work properly, we need to replace the step function with the logistic function( **sigmoid**). This was essential because the step function contains only flat segments, so there is no gradient to work with, while the logistic function has a well-defined nonzero derivative everywhere.\n",
    "\n",
    "$\\begin{align}\n",
    "\\sigma (x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{d\\sigma (x)}{d(x)} = \\sigma (x)\\cdot (1-\\sigma(x))\n",
    "\\end{align}$\n",
    "\n",
    "we now proceed with finding the last derivative:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial J}{\\partial y_j}=\\frac{\\partial}{\\partial y_j}\\frac{1}{2}\\sum\\limits_{k\\epsilon O}{(t_k-y_k)^2}=\\frac{\\partial}{\\partial y_j}\\frac{1}{2}(t_j-y_j)^2=\\frac{1}{2}2(t_j-y_j)\\frac{\\partial}{\\partial y_j}(t_j-y_j)=-(t_j-y_j)\n",
    "\\end{align}$     (4)\n",
    "\n",
    "the summation term over output units is dropped because the derivatives is zero for all output units except for the case when $k=j$. By substituting (4) into (3) we obtain:\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\delta_j=-(t_j-y_j)y_j(1-y_j)}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2, internal unit**\n",
    "\n",
    "When unit $j$ is internal, we must also consider every unit immediately downstream of it (all units whose direct input include the output that unit). This is because a change in $w_{ji}$ (through $z_j$) influences the network outputs through these units. Let $ds(j)$ denote units downstream of unit $j$, then:\n",
    "\n",
    "$\\begin{align}\n",
    "\\delta_j=\\frac{\\partial J}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)} \\frac{\\partial J}{\\partial z_k}\\frac{\\partial z_k}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)}\\delta _k \\frac{\\partial z_k}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j}=\\sum\\limits_{k\\epsilon ds(j)}\\delta_kw_{jk}y_j(1-y_j)  \n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\boxed{\\delta_j=y_j(1-y_j)\\sum\\limits_{k\\epsilon ds(j)}\\delta_kw_{jk}}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to initialize all the connection weights randomly, or else training will fail. For example, if we initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. If instead we randomly initialize the weights, we break the symmetry and allow backpropagation to train a diverse team of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write a function to compute the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]], input_vector: Vector, target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                     for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                     dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can train it using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.gradient_descent import gradient_step\n",
    "import tqdm\n",
    "    \n",
    "def train(network: List[List[Vector]], xs: Vector, ys: Vector, epochs: int, learning_rate: float) -> List[List[Vector]]:\n",
    "    for epoch in tqdm.trange(epochs, desc=\"Neural network for xor\"):\n",
    "        for x, y in zip(xs, ys):\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                        for layer, layer_grad in zip(network, gradients)]\n",
    "    return network;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to learn the XOR network we previously designed by hand. We start by generating the training data and initializing our neural network with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "    \n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "    \n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neural network for xor: 100%|██████████| 2000/2000 [00:00<00:00, 14412.20it/s]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "epochs = 2000;\n",
    "network = train(network, xs, ys, epochs, learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034333804574649227\n",
      "0.9695399969539419\n",
      "0.969456050523935\n",
      "0.03202405432763502\n"
     ]
    }
   ],
   "source": [
    "print(feed_forward(network, [0, 0])[-1][0])\n",
    "print(feed_forward(network, [0, 1])[-1][0])\n",
    "print(feed_forward(network, [1, 0])[-1][0])\n",
    "print(feed_forward(network, [1, 1])[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "The backpropagation algorithm works well with many other activation functions, not just the logistic function. Here are two other popular choices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **hyperbolic tangent function** is a S-shaped, continuous and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "\n",
    "The **Rectified Linear Unit (ReLU) function** is continuous but unfortunately not differentiable at $x=0$ (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for $z<0$. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative(f, x, eps=0.000001):\n",
    "    return (f(x + eps) - f(x - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x, sign(x), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(x, sigmoid(x), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(x, tanh(x), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(x, relu(x), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(x, derivative(sign, x), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(x, derivative(sigmoid, x), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(x, derivative(tanh, x), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(x, derivative(relu, x), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need activation functions? Well, if we chain several linear transformations, all we get is a linear transformation. For example, if $f(x)=2x+3$ and $g(x)=5x–1$, then chaining these two linear functions gives us another linear function: $f(g(x))=2(5x–1)+3=10x+1$. So if we don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single\n",
    "layer, and we can’t solve very complex problems with that. Conversely, a large enough ANN with nonlinear activations can theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. \n",
    "For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around theo bject, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.\n",
    "In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values. If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Finally, if you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, and then scale the labels to the appropriate range.\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the **Huber loss**, which is a combination of both: it is quadratic when the error is smaller than a threshold, but linear when the error is larger than that. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error.\n",
    "The following are the typical hyperparameters of a regression MLP architecture:\n",
    "- number of input neurons: one per input feature (e.g. 28x28 for MNIST)\n",
    "- number of hidden layers: depends on the problem, typically 1 to 5\n",
    "- number of neurons per hidden layer: depends on the problem, typically 10 to 100\n",
    "- number of output neurons: 1 per prediction dimension\n",
    "- hidden activation function: ReLU\n",
    "- output activation function: ReLU, Logistic or Tanh\n",
    "- loss function: MSE ore Huber (if outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number. MLPs can also easily handle multilabel binary classification tasks: you would dedicate one output neuron for each positive class.  For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. \n",
    "Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1 (which is required if the classes are exclusive). This is called multiclass classification.\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss is generally a good choice.\n",
    "The following are the typical hyperparameters of a classification MLP architecture:\n",
    "- number of input neurons: one per input feature (e.g. 28x28 for MNIST)\n",
    "- number of hidden layers: depends on the problem, typically 1 to 5\n",
    "- number of neurons per hidden layer: depends on the problem, typically 10 to 100\n",
    "- number of output neurons: 1 (binary classification), 1 per label (multilabel binary), 1 per class (multiclass)\n",
    "- hidden activation function: ReLU\n",
    "- output activation function: Logistic (binary and multilabel) Softmax (multiclass)\n",
    "- loss function: cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "\n",
    "The flexibility of SNN is also one of their main drawbacks: there are many hyperparameters to tweak. We can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. \n",
    "\n",
    "How do we know what combination of hyperparameters is the best for our task? One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation). \n",
    "\n",
    "However, it  helps to have an idea of what values are reasonable for each hyperparameter so that we can build a quick prototype and restrict the search space. The following sections provide guidelines for choosing the number of hidden layers and neurons in an MLP and for selecting good values for some of the main hyperparameters.\n",
    "\n",
    "For a review of best practices regarding tuning neural network hyperparameters, check out this excellent paper [A Disciplined Approach to Neural Network Hyperparameters](https://arxiv.org/abs/1803.09820)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers\n",
    "For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, **deep networks have a much higher parameter efficiency than shallow ones**: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. The reason is that real-world data is often structured in a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces). This also improves the ability of deeper network to generalize well to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures, it will only have to learn the higher-level structures. This is called **transfer learning**.\n",
    "In summary, for many problems you can start with just one or two hidden\n",
    "layers, for more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set.\n",
    "Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds) and they need a huge amount of training data. We will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons per Hidden Layer\n",
    "The number of neurons in the input and output layers is determined by the type of input and output your task requires.  As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice, it’s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.\n",
    "We can adopt the **stretch pants approach**: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.\n",
    "Anywaym, in general it is better to increase the number of layers instead of the number of neurons per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., $10^-5$ ) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration. If we plot the loss as a function of the learning rate, we should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoo back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it\n",
    "normally using this good learning rate.\n",
    "Anyway, the optimal learning rate depends on the other hyperparameters (especially the batch size) so if you modify any hyperparameter, make sure to update the learning rate as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Choosing a better optimizer than plain old Mini-batch Gradient Descent\n",
    "is also quite important. We will see several advanced optimizers in more advanced topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size\n",
    "The batch size can have a significant impact on our model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators (like GPUs) can process them efficiently. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. However, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. \n",
    "A possible strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "In general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of iterations\n",
    "In most cases, the number of training iterations does not actually need to\n",
    "be tweaked: just use early stopping instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch\n",
    "\n",
    "Previously, we built a simple neural network that allowed us to stack two layers of neurons, each of which computed sigmoid(dot(weights, inputs)). Although that’s perhaps an idealized representation of what an actual neuron does, in practice we’d like to allow a wider variety of things. Perhaps we’d like the neurons to remember something about their previous inputs. Perhaps we’d like to use a different activation function than sigmoid. And frequently we’d like\n",
    "to use more than two layers. \n",
    "\n",
    "Before to introduce Keras (a powerful opensource library), we’ll build from scratch the machinery for implementing such a variety of neural networks. Our fundamental abstraction will be the Layer, something that knows how to apply some function to its inputs and that knows how to backpropagate gradients.\n",
    "\n",
    "One way of thinking about the neural networks we built previously is as a \"linear\" layer, followed by a \"sigmoid\" layer, then another linear layer and another sigmoid layer.\n",
    "\n",
    "We can define a general class to manage a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "from scratch.linear_algebra import Tensor\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"forward\" and \"backward\" methods will have to be implemented in our concrete subclasses. Once we build a neural net, we’ll want to train it using gradient descent, which means we’ll want to update each parameter in the network using its gradient. Accordingly, we insist that each layer be able to tell us its parameters and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers (for example, a layer that applies sigmoid to each of its inputs) have no parameters to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import Tensor, tensor_apply, tensor_combine\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward pass we saved the computed sigmoids so that we could use them later in the backward pass. The sig * (1 - sig) * grad comes from the chain rule from calculus and corresponds to the output * (1 - output) * (output - target) term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In larger networks a popular replacement for the sigmoid is Relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x: max(x, 0), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the \"linear\" layer that represents the dot(weights, inputs) part of the neurons. This layer has parameters, which we initialize with random values. It turns out that the initial parameter values can make a huge difference in how quickly (and sometimes whether) the network trains. If weights are too big, they may produce large outputs in a range where the activation function has near-zero gradients. And parts of the network that have zero gradients necessarily can’t learn anything via gradient descent. Accordingly, we’ll implement two different schemes for randomly generating our weight tensors. The first is to choose each value from the random uniform distribution on (0, 1). The second is to choose each value randomly from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scratch.probability import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int, mean: float = 0.0, variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random()) for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance) for _ in range(dims[0])]\n",
    "\n",
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'normal') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o] for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o] \n",
    "                        for i in range(self.input_dim)]\n",
    "                        for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] \n",
    "                for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real library, the operations written before would be represented as matrix multiplications in an optimized way, probably in C++. Our library is very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are thinking of neural networks as sequences of layers, so let’s come up with a way to combine multiple layers into one. The resulting neural network is itself a layer, and it implements the Layer methods in the obvious ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we wrote individual loss functions and gradient functions for our models. Here we’ll want to experiment with different loss functions, so we introduce a new Loss abstraction that encapsulates both the loss computation and the gradient computation. And we provide a first implementation usgin the sum of squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(Loss):\n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        squared_errors = tensor_combine(lambda predicted, actual: (predicted - actual) ** 2, predicted, actual)\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual), predicted, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we introduce an Optimizer abstraction, of which gradient descent will be a specific instance. This allow su to  implemetn more clever variants of gradient descent, and we don’t want to have to rewrite them each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer (in place) using information\n",
    "    known by either the layer or the optimizer (or by both).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            param[:] = tensor_combine(lambda param, grad: param - grad * self.lr, param, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we’re doing classification problems, we’d like to output a 1 for the correct class and a 0 for all the incorrect classes. Generally, our predictions will not be so perfect, but we’d at least like to predict an actual probability distribution over the classes. In order to accomplish this, we typically forgot the final sigmoid layer and\n",
    "instead use the softmax function, which converts a vector of real numbers to a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import is_1d\n",
    "\n",
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stabilitity.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one remaining issue. Our network will produce a multi-dimensional vector of numbers, but we want a single prediction. We’ll do that by taking the argmax, which is the index of the largest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try our implementation over the MNIST dataset. This time, we use the mnist python library insted of scikitlearn (python -m pip install mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60000, 28, 28]\n",
      "[60000]\n",
      "[10000, 28, 28]\n",
      "[10000]\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import os\n",
    "from scratch.linear_algebra import shape \n",
    "\n",
    "mnist.temporary_dir = lambda: './data/'\n",
    "\n",
    "train_images = mnist.train_images().tolist()\n",
    "train_labels = mnist.train_labels().tolist()\n",
    "\n",
    "test_images = mnist.test_images().tolist()\n",
    "test_labels = mnist.test_labels().tolist()\n",
    "\n",
    "print(shape(train_images))\n",
    "print(shape(train_labels))\n",
    "\n",
    "print(shape(test_images))\n",
    "print(shape(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28x28 pixels, but our linear layer can only deal with one-dimensional\n",
    "inputs, so we’ll just flatten them (and also divide by 256 to get them between 0 and 1). In addition, our neural net will train better if our inputs are 0 on average, so we’ll subtract out the average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60000, 784]\n",
      "[10000, 784]\n",
      "1.0862287069812737e-08\n"
     ]
    }
   ],
   "source": [
    "from scratch.linear_algebra import tensor_sum \n",
    "\n",
    "# Compute the average pixel value\n",
    "avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "\n",
    "# Recenter, rescale, and flatten\n",
    "train_images = [[(pixel - avg) / 256 for row in image for pixel in row] for image in train_images]\n",
    "test_images = [[(pixel - avg) / 256 for row in image for pixel in row] for image in test_images]\n",
    "\n",
    "print(shape(train_images))\n",
    "print(shape(test_images))\n",
    "print(tensor_sum(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to one-hot-encode the targets, since we have 10 outputs. First let’s\n",
    "write a one_hot_encode function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60000, 10]\n",
      "[10000, 10]\n"
     ]
    }
   ],
   "source": [
    "train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "\n",
    "print(shape(train_labels))\n",
    "print(shape(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of our abstractions is that we can use the same training/evaluation loop with a variety of models. So let’s write that first. We’ll\n",
    "pass it our model, the data, a loss function, and an optimizer. It will make a pass through our data, track performance, and update our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "    \n",
    "def loop(model: Layer, \n",
    "         images: List[Tensor], labels: List[Tensor], \n",
    "         loss: Loss, optimizer: Optimizer = None) -> None:\n",
    "        correct = 0         # Track number of correct predictions.\n",
    "        total_loss = 0.0    # Track total loss.\n",
    "        \n",
    "        with tqdm.trange(len(images)) as t:\n",
    "            for i in t:\n",
    "                predicted = model.forward(images[i])             # Predict.\n",
    "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                    correct += 1                                 # correctness.\n",
    "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "    \n",
    "                # If we're training, backpropagate gradient and update weights.\n",
    "                if optimizer is not None:\n",
    "                    gradient = loss.gradient(predicted, labels[i])\n",
    "                    model.backward(gradient)\n",
    "                    optimizer.step(model)\n",
    "    \n",
    "                # And update our metrics in the progress bar.\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                acc = correct / (i + 1)\n",
    "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we can use our library to train a logistic regression model, which is just a single linear layer followed by a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mnist loss: 11.150 acc: 0.483: 100%|██████████| 60000/60000 [05:15<00:00, 190.42it/s]\n",
      "mnist loss: 243.831 acc: 0.140: 100%|██████████| 10000/10000 [00:12<00:00, 796.39it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "model = Linear(784, 10)\n",
    "loss = SSE()\n",
    "optimizer = GradientDescent(learning_rate=0.01)\n",
    "\n",
    "# Train on the training data\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Test on the test data (no optimizer means just evaluate)\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see if we can do better with a more complex neural network. We’ll use two hidden layers, the first with 30 neurons, and the second with 10 neurons. And we can just use the same training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mnist loss: 0.189 acc: 0.903: 100%|██████████| 60000/60000 [14:36<00:00, 68.46it/s]\n",
      "mnist loss: 0.133 acc: 0.937: 100%|██████████| 10000/10000 [00:23<00:00, 433.06it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "model = Sequential([\n",
    "            Linear(784, 30),  # Hidden layer 1: size 30\n",
    "            Relu(),\n",
    "            Linear(30, 10),   # Hidden layer 2: size 10\n",
    "            Relu(),\n",
    "            Linear(10, 10)    # Output layer: size 10\n",
    "        ])\n",
    "    \n",
    "optimizer = GradientDescent(learning_rate=0.01)\n",
    "loss = SSE()\n",
    "\n",
    "# Train on the training data\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Test on the test data (no optimizer means just evaluate)\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MNIST website](http://yann.lecun.com/exdb/mnist/) describes a variety of models that outperform these. Many of them could be implemented using the machinery we’ve developed so far, but would take an extremely long time to train in our naive framework. We can start using some more optimized solution, like Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
