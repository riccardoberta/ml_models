{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement  Learning\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/riccardoberta/machine-learning/blob/master/XX_reinforcement_learning/reinforcement_learning.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Google Colab\"/></a>\n",
    "\n",
    "What if we need to tackle complex problems, such as detecting hundreds of types of objects in high-resolution images? We need to train a **deeper** ANN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections. However, training a **Deep Deep Neural Network (DNN)** rise some problems:\n",
    "- the gradients can grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training, making lower layers very hard to train (***vanishing/explodin gradients***);\n",
    "- we might not have enough training data for such a large network, or it might be too costly to label;\n",
    "- training may be extremely slow;\n",
    "- a model with millions of parameters would severely risk overfitting the training set.\n",
    "\n",
    "In the following we will go through each of these problems and present techniques to solve them. \n",
    "\n",
    "1. [The Vanishing Exploding Gradients Problems](#The-Vanishing-Exploding-Gradients-Problems)\n",
    "    - [Initialization strategies](#Initialization-strategies)\n",
    "    - [Nonsaturating Activation Functions](#Nonsaturating-Activation-Functions)\n",
    "    - [Batch Normalization](#Batch-Normalization)\n",
    "    - [Gradient Clipping](#Gradient-Clipping)\n",
    "2. [Transfer Learning](#Transfer-Learning)\n",
    "3. [Faster Optimization](#Faster-Optimization)\n",
    "    - [Momentum Optimization](#Momentum-Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing Exploding Gradients Problems\n",
    "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the **vanishing\n",
    "gradients problem**. \n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight update and the algorithm diverges. This is the **exploding gradients problem**, which\n",
    "surfaces in recurrent neural networks. \n",
    "\n",
    "More generally, DNN suffer from **unstable gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is due to  the combination of the logistic sigmoid activation function and the weight initialization technique (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). \n",
    "\n",
    "It can be showed that with this combination, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0.\n",
    "\n",
    "For a detailed matematical description, see the paper [Xavier Glorot and Yoshua Bengio, **Understanding the Difficulty of Training Deep Feedforward Neural Networks**, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010)](http://proceedings.mlr.press/v9/glorot10a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the logistic activation function, we can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(x, logit(x), \"b-\", linewidth=2)\n",
    "\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(\"Logistic activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization strategies\n",
    "In order to alleviate the unstable gradients problem, we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. \n",
    "\n",
    "We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. \n",
    "\n",
    "It is not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the **fan-in** and **fan-out** of the layer). A good compromise that has proven to work well in practice: the connection weights of each layer must be initialized randomly as described in the following equations\n",
    "\n",
    "- $ \\text{Normal distribution with mean 0 and variance } \\sigma^2=\\frac{1}{\\text{fan}_\\text{avg}}$  \n",
    "- $ \\text{Uniform distribution between } -r \\text{ and } +r \\text{ with } r=\\sqrt{\\frac{3}{\\text{fan}_\\text{avg}} } $\n",
    "- $ \\text{where } \\text{fan}_\\text{avg}=(\\text{fan}_\\text{in}+\\text{fan}_\\text{out})/2$\n",
    "\n",
    "This initialization strategy is called **Xavier initialization** or **Glorot initialization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some papers have provided similar strategies for different activation functions. \n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution, however it support a number of other strategies. When creating a layer, we can change the initialization stratergy by setting **kernel_initializer** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "The problem with unstable gradients is in part due to a poor choice of activation function. The sigmoid activation function (used in biological neurons) is not the best choice. The **ReLU activation function** is fast to compute and does not saturate for positive values. Unfortunately, it is not perfect. It suffers from a problem known as the **dying ReLUs**: during training, some neurons “die” (they stop outputting anything other than 0). This happens when the neuron weights get tweaked in a way that the weighted sum of inputs are negative for all instances in the training set. In these conditions, the neuron just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "\n",
    "To solve this problem, we can use a variant of the ReLU function: the **leaky ReLU**. \n",
    "\n",
    "$\\text{LeakyReLU}(x)=\\text{max}(\\alpha x, x)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "plt.plot(x, leaky_relu(x, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for z < 0. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
    "\n",
    "Other variants are: the **randomized leaky ReLU (RReLU)**, where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing, and the **parametric leaky ReLU (PReLU)**, where $\\alpha$ is authorized to be learned during training. For a comparison of several variants of the ReLU activation function, we can refer to the paper:[Bing Xu et al. **Empirical Evaluation of Rectified Activations in Convolutional Network** arXiv preprint arXiv:1505.00853 (2015)](https://arxiv.org/pdf/1505.00853.pdf)\n",
    "\n",
    "In Keras, to use the leaky ReLU activation function, we can just create a LeakyReLU layer and add it to our model just after the layer we want to apply it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use the **exponential linear unit (ELU)** activation function, that outperform ReLU variants: \n",
    "\n",
    "$\\text{ELU}_\\alpha(x) = \n",
    "\\begin{cases}\n",
    "    \\alpha(\\text{exp}(x)-1 & \\text{if} & x \\lt 0 \\\\\n",
    "     x & \\text{if} & x \\gt 0\n",
    "\\end{cases}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu(x, alpha=1):\n",
    "    return np.where(x < 0, alpha * (np.exp(x) - 1), x)\n",
    "\n",
    "plt.plot(x, elu(x), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU functions (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **Scaled ELU (SELU)** activation function is a variant of the ELU activation function. For a neural network composed exclusively of a stack of dense layers, the SELU activation function allows a self-normalization:\n",
    "the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEMCAYAAAA70CbBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV1f3/8dcHwo4SAY11A1v3KqUlavVb21T0W/UndcMdW6qVCJWKQhUVKq0bLrRYERUKoqAigrjw1e/Dir22rl+DolYriwrFBQUkSEKAkJzfH+eGhJuwhMzNuXfu+/l4zCOTmeHOJ4fhzeTMcsw5h4iIxEOL0AWIiEh0FOoiIjGiUBcRiRGFuohIjCjURURiRKEuIhIjCnXJamY2xczmNMN+iszMmVnXZtjXADP7j5lVm9modO9vO7X0N7OykDVI4yjUY8TMdjez8Wa2xMw2mNmXZjbXzE6ss00iGU6p0/Q62zgz69vA53dPritsYF3CzMal8WfbWqheAfSLeF9LzGxYyuJXgW8Bq6LcVwP73g24B7gD2Bu4M537S9l3Q3/vjwHfbq4apOnyQhcgkZoFtAcuARYDewA/AbqkbPcAcF3Ksoq0V5cGzrk1zbSfjcDyZthVN/y/yznOuS+aYX/b5JyrIEuPjVylM/WYMLN84DhguHNurnNuqXPuTefcnc656Smbr3POLU+Z0hqOZvYdM3vKzJabWbmZvWVmp6Zs09rMbjGzpcnfND42s9+aWXfg78nNViTPKKck/8zm7hczK07+dpKX8rmPmNlTO1KHmSXwwXpHzW8xyeX1flMwszPN7L1krcvM7Hozszrrl5jZCDO738y+MbNPzex322ij/sDbyW8/Tu6vu5mNMrN/pW5bt1ukZhszO8/MPjKztWb2ZOpvNmb2yzo1f1mnHZckN3k8ud8lDe2nTjsvNrONya+Xpqx3yS6kx5Nt/LGZRfrblGydQj0+ypLTz82sbehiGtAReA44Efge/reKJ8zskDrbPAj8ArgKOBT/G0cpsAw4K7nNd/HdIFc0sI8ZQD5wQs0CM+sAnAZM28E6zgQ+Bf6Y3M+3GvphzKwX8DjwBHAEMBy4Frg8ZdMrgfeAHwC3Abeb2TENfSa+q+Ok5PxRyX0v28q2DekOnAucAfw38H3g5jo1FwP3439T6wGcAryfXH1k8uulyf3WfL8FMzsDGAeMBQ4H7gLGm1mflE1/DzyFb+PHgMlm1q0RP4vsLOecpphM+OD7GlgPvIbvjz06ZZsEsJHa/wRqpkF1tnFA3wY+v3tyXWED6xLAuEbW+zowIjl/YPKzT9rKtkXJ9V1Tlk/Bd1XUfD8bmFrn+37AGqDtjtSR/H4JMGxb+wceBl5M2WYU8GnK5zyass2iuvtqoJbC5H66p3zuv1K26w+UpWyzHuhUZ9n1wOI6338KjN7Gvuv9vTewn1eAyQ38Hbyc8jm31vk+D1gH9Av9byQXJp2px4hzbhawF9AHfzZ6LPC6maX2nz8G9EyZHk5nbWbWwcxuN7MPzGx18lf6QmC/5CbfB6qp7WbZWdOA082sffL7C4GZzrn1O1jHjjoUH3B1vQzsbWa71ln2bso2n+OvdaTDUrdlN9rmfZnZHvgLr3ObuI+t/dyHpSzb/HM75zYBK0jfzy116EJpzCTD62/J6Y9m9ldglJnd6fzFPoA1zrnFO/HxNYHRqYF1+XXWN+ROfNfCMPzZ6jrgIaB1cr1t5c811hxgE3Camc3Fd8X8dyPq2FGGPyNtSN3llQ2sa+zJVDX126dVA9tta19RtW/N525vWRQ/t+wENXL8fYD/z7vJ/ezOudXASqBX3eXJM9MDgAXb+OM/Ah5yzs1yzr2L7wr4Tp31b+GPx59u5c/X/IfUcjs1bgBm4s/Qz8XfsfJSI+qo2dc294Nv1x+lLPsRvvtl7Xb+bGOtAArqXoTF/3a1w5xzXwKfAb23sVkl2/+5/03DP/cHjalH0kdn6jFhZl3wF+4m43/1XYvvVrgamOuc+6bO5u3NbM+Uj9jonPu6zvfdzSw1OD4G/gQMN7PP8f32XYCR+LB/fBslLgTOSN6FUgncQJ3/aJxzi8xsBvBXM7sCH/L74PuWpwJL8Wd7/8/MngEqnHNbeyhmGvACsD/wiHOuekfrSFoCHGdm04ANzrmVDexjDPCm+YeDHsFfWBxK/VtFo5AAOgPXmX+eoAio9xzBDrgZ+LOZfQn8D/72197OuTHJ9UuA3mb2Ev7nXt3AZ9yBv0NmHvA8/reeC/EXmCUThO7U1xTNBLQBbgHeBFbjuxUW4UO4c53tEvhwTJ1SL3Q1NJ2KP5MbjP+Powx/pjudOhf2tlJfN3zQlif/zDB8V8mUlJ/hdvwZ5QbgI+DyOutHAl/guyOmJJdNoc6F0uQywweUA47YiTp+CLyDv/DoksuKSLlQiw+y9/Bn9svwFyatzvol1L/gmmAbF5Rp4EJpcnkx/j+28mR7X0H9C6XbvJiaXHYJ/qy65r77yXXW9UkeM5XAkm18xmX45yAqk18vTVnf0AXXem2hKT2TJRtcRERiQH3qIiIxolAXEYkRhbqISIwo1EVEYiT4LY1du3Z13bt3D1pDeXk5HTp0CFpDplBbeAsWLKCqqorDDkt9UDI3ZcJxUV4OCxaAc7D//tC5c6g6wrcFwLx581Y653ZPXR481Lt3705JSUnQGhKJBEVFRUFryBRqC6+oqIjS0tLgx2amCH1cfPEF9OrlA/2KK2Ds2GClBG+LGma2tKHl6n4RkYy2cSOcfbYP9h//GO64I3RFmU2hLiIZbehQeOUV2HtvmDEDWjX01hvZTKEuIhnroYdg3Dho3RpmzYKCgtAVZb5IQ93MppnZF8lRXhaa2a+j/HwRyR1vvQXFxX7+7rvh6KPD1pMtoj5TvxX/zopdgZ8DNyVHiBER2WErV8KZZ8L69fDrX8OAAaEryh6Rhrpz7n3nX30KtS+BSn2tqYjIVlVVwfnnw9KlcNRRvvtFdlzktzSa2Xj8m93a4QfRfbaBbQYAAwAKCgpIJBJRl9EoZWVlwWvIFGoLr7S0lKqqKrVFUnMeFxMmfJsXXtiP/PyNDB06j9de27D9P9SMMv3fSFre0mhmLYFj8K8rvc05lzoKymaFhYUu9L3AmXLfaSZQW3g196nPnz8/dCkZobmOi1mzoG9faNkSXngBMvFQzJR/I2Y2zzlXmLo8LXe/OOeqnHMv4wc5GJiOfYhIvHzwAfTv7+fvuCMzAz0bpPuWxjzUpy4i27FmDZxxBpSV+f70IUNCV5S9Igt1M9vDzM4zs45m1tLMfgacD7wY1T5EJH6qq+EXv4CFC6FHD5g4ESzKYbJzTJQXSh2+q+U+/H8WS4EhzrmnItyHiMTMzTfD009Dfj488QRkwLuyslpkoe6cWwH8JKrPE5H4e/ZZuOEGf2b+yCPwHXXWNlnwtzSKSG5avBguvNC/efHGG+Hkk0NXFA9694uINLvycv/EaGkpnHYaXHdd6IriQ6EuIs3KOf/o/3vvwUEHwYMPQgslUWTUlCLSrMaOhenToWNHePJJ6NQpdEXxolAXkWbz97/D737n5x98EA49NGw9caRQF5FmsWwZnHuuf2HX8OG+T12ip1AXkbRbvx7OOgtWrIATT4SbbgpdUXwp1EUkrZyDyy+HN9+E7t3h0Uf9C7skPRTqIpJWEyfCpEnQtq1/YrRLl9AVxZtCXUTS5vXX/Vk6wIQJ8P3vh60nFyjURSQtli/3/eiVlTB4MFx0UeiKcoNCXUQiV1kJ55wDn38Oxx0HY8aErih3KNRFJHLDhsE//wl77QUzZkCrVqEryh0KdRGJ1LRp8Je/+CCfORP23DN0RblFoS4ikZk/HwYM8PN/+Qscc0zYenKRQl1EIvH1135IuooKuPhiKC4OXVFuUqiLSJNVVfmxRZcsgcJCuOceDUkXikJdRJps5Eh4/nno2hVmzfIPGkkYCnURaZInnoBbb/XvRJ8xA/bbL3RFuU2hLiI77d//hl/+0s/ffjv89Kdh6xGFuojspG++8RdGy8r8K3Wvuip0RQIKdRHZCdXV/gx9wQI4/HD/wi5dGM0MCnURabTRo/1QdPn5MHs2dOgQuiKpoVAXkUb53/+FESP8mfnDD8MBB4SuSOrKC12AiGSPjz+GCy7wA1/84Q9wyimhK5JUOlMXkR2ybp2/MLp6NfTp48/WJfMo1EVku5yDSy+Fd9+FAw+EqVP9femSefTXIiLbNWvW3jzyiL8gOns2dOoUuiLZGoW6iGzTSy/Bvff6q6EPPADf/W7ggmSbFOoislWffupHMKquNq6+Gs4+O3RFsj0KdRFp0IYNfozRr76CXr2+5uabQ1ckOyKyUDezNmY2ycyWmtlaM3vbzE6O6vNFpHkNHgz/93/QrRuMHPlv8nQDdFaI8kw9D1gG/AToBIwEZphZ9wj3ISLNYOJEP7Vt69/C2KlTZeiSZAdFFurOuXLn3Cjn3BLnXLVzbg7wCdArqn2ISPq98QZcfrmfv+8++MEPwtYjjZO2X6jMrAA4CHi/gXUDgAEABQUFJBKJdJWxQ8rKyoLXkCnUFl5paSlVVVU51xZff92K4uJCNm5sw+mnf0a3botIJHRc1JXpbWHOueg/1KwV8BzwkXNumyMVFhYWupKSkshraIxEIkFRUVHQGjKF2sIrKiqitLSU+fPnhy6l2VRWwokn+lsY/+u/4MUXoXVrv07HRa1MaQszm+ecK0xdHvndL2bWApgKbAQuj/rzRSQ9rr7aB/q3vgWPP14b6JJdIu1+MTMDJgEFwCnOOV1dEckCjzwCY8dCq1Ywc6YPdslOUfep3wscCpzgnKuI+LNFJA3eeQd+/Ws/P3YsHHts2HqkaaK8T70bUAz0BJabWVlyujCqfYhItL7+2r95saIC+veHgQNDVyRNFdmZunNuKaABrUSyRFUVXHghfPKJv21x/HgNSRcHek2ASI4aNcqPYtS1q3/AqF270BVJFBTqIjnoySfhppv8O9GnT/evApB4UKiL5JgPP4Rf/MLPjx4NvXuHrUeipVAXySFr1/oLo2vX+tfoDhsWuiKJmkJdJEc45+9w+fBDP9DF5Mm6MBpHCnWRHHHbbTVvXPRD0nXsGLoiSQeFukgOeP55uP56Pz9tmh88WuJJoS4Sc598AuefD9XVcMMNcOqpoSuSdFKoi8TYunVw5pn+ydFTT4Xf/z50RZJuCnWRmHIOioth/nw44ACYOtXfly7xpr9ikZgaN873n7dv7y+M5ueHrkiag0JdJIb++U+46io//8ADcPjhYeuR5qNQF4mZzz7zDxZt2uQfLjrnnNAVSXNSqIvEyIYN0LcvfPklHH883Hpr6IqkuSnURWLkiivg9ddhv/38i7ry0ja0vGQqhbpITEyaBPffD23awKxZsPvuoSuSEBTqIjHw5pswaJCfv/deKKw3xrzkCoW6SJb76iv/gNHGjX44ul/9KnRFEpJCXSSLbdoE554Ln34KxxzjB46W3KZQF8liw4dDIgF77gkzZ0Lr1qErktAU6iJZavp0GDPG3+Hy+OOw116hK5JMoFAXyULvvguXXOLn//xn+NGPwtYjmUOhLpJlVq/2F0bXrfNjjf7mN6ErkkyiUBfJItXV0K8ffPQRfP/7cN99GpJOtqRQF8kif/gDPPssdOnih6Zr1y50RZJpFOoiWeLpp+GPf/TvRH/0UejePXRFkokU6iJZYOFCuOgiP3/LLXDiiWHrkcylUBfJcGvXwhlnwDffwFlnwdVXh65IMplCXSSDOQcXXwwffACHHeYHvNCFUdkWhbpIBrvjDv+k6K67+guju+wSuiLJdJGGupldbmYlZrbBzKZE+dkiueZvf4Nrr/XzU6fCwQeHrUeyQ9Sv0P8cuAn4GaCbrUR20pIlcP75/r70kSPh5z8PXZFki0hD3Tn3BICZFQL7RPnZIrmiosI/MbpqFZxyCowaFboiySZBBrsyswHAAICCggISiUSIMjYrKysLXkOmUFt4paWlVFVVNXtbOAejRx/C22/vyV57VXDZZfP4xz82NWsNDdFxUSvT2yJIqDvnJgATAAoLC11RUVGIMjZLJBKEriFTqC28/Px8SktLm70t7rkHnn8e2reH555rR48emfGmLh0XtTK9LXT3i0iGePllGDLEz0+aBD16hK1HspNCXSQDfP45nH22H8noqqvgvPNCVyTZKtLuFzPLS35mS6ClmbUFNjnnwncKimSojRt9oC9fDkVFcNttoSuSbBb1mfoIoAIYDvRLzo+IeB8isXLllfDqq7DPPvDYY34kI5GdFfUtjaOAUVF+pkicTZkC48f7sUWfeAL22CN0RZLt1KcuEkhJCVx2mZ8fPx6OPDJsPRIPCnWRAFas8A8YbdgAxcW1442KNJVCXaSZbdrk725Ztgx++EO4667QFUmcKNRFmtl118GLL0JBgX8DY5s2oSuSOFGoizSjGTP863Tz8uDxx2HvvUNXJHGjUBdpJv/6lx/wAmDMGDjuuLD1SDwp1EWaQWmpH5KuvBz69YPBg0NXJHGlUBdJs+pqP2j04sXQsyfcf7+GpJP0UaiLpNmNN8KcObDbbv4Bo/btQ1ckcaZQF0mjOXP8IBdm8OijsP/+oSuSuFOoi6TJokW+/xzg5pvhZz8LW4/kBoW6SBqUlfkLo2vW+K/Dh4euSHKFQl0kYs75x/7ffx8OOcS/tEsXRqW5KNRFIjZmjH/IaJddYPZs2HXX0BVJLlGoi0Ro7ly45ho//9BD/kxdpDkp1EUisnQpnHuuvy/9+uvh9NNDVyS5SKEuEoGKCjjrLFi1Ck46Cf7wh9AVSa5SqIs0kXMwaBDMmwff/jY8/DC0bBm6KslVCnWRJrrvPn+HS7t2/onRzp1DVyS5TKEu0gSvvgpXXOHn//pX+N73wtYjolAX2UlffAF9+0JlJQwZAhdcELoiEYW6yE7ZuBHOPtsH+09+ArffHroiEU+hLrIThg6FV17xIxc99hi0ahW6IhFPoS7SSA89BOPGQevWMGuWH2tUJFMo1EUa4a23oLjYz48bB0cfHbYekVQKdZEdtHIlnHkmrF8Pl17qJ5FMo1AX2QGbNsH55/tXARx1FNx9d+iKRBqmUBfZASNGwAsvwB57+H70Nm1CVyTSMIW6yHbMnAm33eYf/Z8xA/bZJ3RFIlunUBfZhg8+gP79/fydd/p70kUyWaShbmadzWy2mZWb2VIz0zN2krWqqozTT4fycv+0aM3rAEQyWV7En3cPsBEoAHoC/2Nm7zjn3o94PyJpt2xZe9asgR49YOJEDUkn2cGcc9F8kFkHYDVwuHNuYXLZVOAz59xWh93dZZddXK9evSKpYWeVlpaSn58ftIZMobbwSkrmU14OLVv2pLAQ2rYNXVFYOi5qZUpbvPTSS/Occ4Wpy6M8Uz8IqKoJ9KR3gHq9kGY2ABgA0KpVK0pLSyMso/GqqqqC15Ap1BZeRYUDjN13X8/69etZvz50RWHpuKiV6W0RZah3BNakLFsD7JK6oXNuAjABoLCw0JWUlERYRuMlEgmKioqC1pAp1Bbw+ONwzjlF5OVVs3jxP+jQIXRF4em4qJUpbWFb6Q+M8kJpGZA6bvquwNoI9yGSVpWVfnxRgD333KBAl6wTZagvBPLM7MA6y74H6CKpZI3Jk2HRIj+KUefOG0KXI9JokYW6c64ceAL4o5l1MLP/Ak4Dpka1D5F0Ki+vHTB6//11t4tkp6gfPhoEtAO+Ah4FBup2RskWd93lB70oLITddw9djcjOiTTUnXNfO+dOd851cM7t55x7JMrPF0mXVav8qwAARo8OW4tIU+g1ASLArbfCN9/AiSdC796hqxHZeQp1yXlLlvgBL0Bn6ZL9FOqS84YPhw0b/PtdfvCD0NWINI1CXXLaa6/5gaPbtvVdMCLZTqEuOau6Gq680s8PGwb77Re2HpEoKNQlZz32GLzxBuy5J1xzTehqRKKhUJecVFHh+9IBbroJOnYMW49IVBTqkpNGj4b//Me/K71mZCOROFCoS8758MPaWxfvvtuPPSoSFwp1ySnOwWWXwcaNcMkl8OMfh65IJFoKdckpDz0EL70EXbvWvhZAJE4U6pIzVq6EoUP9/J/+BF26hK1HJB0U6pIzhg71L+46/njo1y90NSLpoVCXnDB7tu96adsW7r1X70qX+FKoS+x9+SUMGODnb78dDjoobD0i6aRQl1hzDi691Pen9+4Nv/lN6IpE0kuhLrH2wAPwzDPQqZOfb6EjXmJOh7jE1sKFcMUVfn7cONh337D1iDQHhbrE0rp10LcvlJXBOefAhReGrkikeSjUJZYuvxzeew8OPBAmTtTdLpI7FOoSOw884Ke2bWHmTNh119AViTQfhbrEyjvvwKBBfn78eP8WRpFcolCX2Fi+HPr0gfXr4Ve/8pNIrlGoSyxUVMDpp8OyZXDMMf4sXSQXKdQl6znnz8rfeAO6dYMnn/T96SK5SKEuWe+GG/x4o7vsAnPmwB57hK5IJByFumS1u++GG2/0T4pOnw6HHx66IpGwFOqStaZOhd/+1s9PnAinnBK2HpFMoFCXrPT007V3t9x5J1x8cdh6RDKFQl2yznPP+Uf/q6rg+utrRzMSkYhC3cwuN7MSM9tgZlOi+EyRhjz1FJx2GmzYAIMH+/50EakV1Zn658BNwOSIPk+knhkz/Eu6Kivhyivhrrv0TheRVJGEunPuCefck8CqKD5PJNXkyXD++bBpE1x7LYwZo0AXaUheiJ2a2QBgAEBBQQGJRCJEGZuVlZUFryFTZFpbOAcPPtidBx/sDkD//p9w4olLeeml9O63tLSUqqqqjGqLkDLtuAgp09siSKg75yYAEwAKCwtdUVFRiDI2SyQShK4hU2RSW1RWQnExPPigvw993DgYOHB/YP+07zs/P5/S0tKMaYvQMum4CC3T22K73S9mljAzt5Xp5eYoUnLPqlX+vvMHHoD27f2j/wMHhq5KJPNt90zdOVfUDHWIbPb223DmmbBkiX/kf84cOPLI0FWJZIeobmnMM7O2QEugpZm1NbMgXTuS3aZNg2OP9YF+5JFQUqJAF2mMqG5pHAFUAMOBfsn5ERF9tuSAsjK45BK46CL/PvSLL4Z//EODRYs0ViRn0865UcCoKD5Lcs+8ef52xUWLoE0bf//5gAG6ZVFkZ+g1ARJMZSXcfLMf1GLRIv+GxZISf8eLAl1k56jfW4J46y3f3TJ/vv9+8GC47TZo1y5sXSLZTmfq0qzKyuCaa+Coo3yg778//O1v8Je/KNBFoqBQl2bhHDzyCBx8MNx+O1RX+/e3vPcenHBC6OpE4kPdL5J2r78Ov/sdvJx8VO3II/3ToUcdFbYukTjSmbqkzfvvwxln+AuhL7/sHySaPNmHvAJdJD10pi6Re/ddf9Fz+nTfzdK+PQwZAldfDZ06ha5OJN4U6hKZf/4TRo+GZ5/13+flwWWXwYgR8K1vha1NJFco1KVJKivhmWfgT3+CV17xy9q1g0svhauugm7dwtYnkmsU6rJTli6FiRNh0iRYvtwv2203f7/54MHQtWvY+kRylUJddlhFhe9amTzZD/7snF9+6KG+m+Xii6Fjx7A1iuQ6hbpsU2UlzJ0Ljz4Ks2fD2rV+eevWfrzQ4mI47jg91i+SKRTqUk95uQ/yZ57xg1OsXFm7rrAQLrjAv01RXSwimUehLgB88onvUnnooSOYPx82bKhdd+ih/i2K550HBx4YrkYR2T6Feo767DP4+9/99OKLflAKrwtmcPTR0KePn444Qt0rItlCoZ4DNmzwL896443a6aOPttxmt93g+OPhgAM+5MorD6GgIEytItI0CvWYqaiADz7wL8p6+20f4G+/DRs3brldx47w4x/7ID/+eOjRA1q2hERiOQUFh4QpXkSaTKGepdat82fbCxfCv/7lQ/y992DxYv9ofqpDD/VdKj/8of96+OH+iU8RiRf9s85QzsGKFfCf/8CyZT6sFy/2IwQtWgSfftrwn2vZEg47zPeD9+jhX5x15JF654pIrlCoB1BRAV9+WTstX+6DuybAa6a6d6CkysvzA0wceCB897s+wI84Ag45xI/zKSK5SaHeBM75h3FWr4avv97ya935FSvgq69qQ7zmAZ7t2W032G8/2Hff2gCvmbp1U/eJiNQX+1iorIT16/1UUbHl15r5kpKufPGF76cuK/OhW/O17nzq1zVroKqq8TW1agUFBf794gUFfqoJ75qv++6rR+5FpPGCh/oXX8Dvf+/Dt6nTxo1+qhvcOxa6h+90/R06QOfO/qy6Zqr7fefO0KVLbXgXFEB+vu77FpH0CB7qn3++gBtvLEpZeg4wCFgHnNLAn+qfnFYCfRtYPxA4F1gGXESLFmwxFRQMZY89+lBdvYCPPy6mqqqSNm1a0aKF79I47rgR9OhxAmvWzOfJJ4fQsqW/AJmX579ec80tFBUdy/vvv8oNN1y3xZ5Xr4YbbhhLz549eeGFF7jpppvqVXf//fdz8MEH88wzzzBmzJh666dOncq+++7LY489xr333ltv/cyZM+natStTpkxhypQp9dY/++yztG/fnvHjxzNjxox66xOJBAB33nknc+bM2WJdRUUFb7zxBgA33ngjc+fO3WJ9ly5dmDVrFgDXXnstr7322hbr99lnH6ZNmwbAkCFDmD9//hbrDzroICZMmADAgAEDWLhw4Rbre/bsydixYwHo168fn6ZcET7mmGO49dZbATjrrLNYtWrVFut79+7NyJEjATj55JOpqKjYYv2pp57KsGHDACgqKiLVOeecw6BBg6iurmbx4sX1tunfvz/9+/dn5cqV9O1b/9gbOHAg5557LsuWLeOiiy6qt37o0KH06dOHBQsWUFxcXG/9iBEjOOGEE5g/fz5Dhgypt/6WW27h2GOP5dVXX+W6666rt37s2PQce6WlpeTn56f12GvXrh3PPfcckNvH3rp16zjllPq5t71jr0bwUG/d2g+g0KKFP3s1g169oHdvf2veXXf5ZXXXn3QSnHqqf0fJiBFbrm/Rwr8t8LzzfF/2xRfX3+fQof5JyQUL/AupSkvLyc/P37z+kkv8YMjz5/uh11LttZd/70mrVmlsGMtOTpsAAAPISURBVBGRnWCu5v2pgRQWFrqSkpKgNSQSiQb/58xFaguvqKiI0tLSemd7uUrHRa1MaQszm+ecK0xdroGnRURiRKEuIhIjCnURkRhRqIuIxIhCXUQkRpoc6mbWxswmmdlSM1trZm+b2clRFCciIo0TxZl6Hv4pn58AnYCRwAwz6x7BZ4uISCM0+eEj51w5MKrOojlm9gnQC1jS1M8XEZEdF/kTpWZWABwEvL+NbQYAAwAKCgo2PzocSllZWfAaMoXawistLaWqqkptkaTjolamt0WkT5SaWSvgOeAj51z9F1s0QE+UZha1hacnSrek46JWprTFTj9RamYJM3NbmV6us10LYCqwEbg80upFRGSHbLf7xTlXtL1tzMyASUABcIpzrrLppYmISGNF1ad+L3AocIJzrmJ7G4uISHpEcZ96N6AY6AksN7Oy5HRhk6sTEZFGieKWxqWAxvEREckAek2AiEiMBB8kw8xWAEuDFgFd8WPjidqiLrVFLbVFrUxpi27Oud1TFwYP9UxgZiUN3e+Zi9QWtdQWtdQWtTK9LdT9IiISIwp1EZEYUah7E0IXkEHUFrXUFrXUFrUyui3Upy4iEiM6UxcRiRGFuohIjCjURURiRKHeADM70MzWm9m00LWEkOvjzppZZzObbWblyTa4IHRNIeT6cbA1mZ4PCvWG3QO8GbqIgHJ93Nl78OMCFAAXAvea2XfDlhRErh8HW5PR+aBQT2Fm5wGlwNzQtYTinCt3zo1yzi1xzlU75+YANePOxpqZdQDOAkY658qccy8DTwMXha2s+eXycbA12ZAPCvU6zGxX4I/A0NC1ZJIdGXc2Rg4CqpxzC+ssewfIxTP1LeTYcVBPtuSDQn1LNwKTnHPLQheSKZLjzj4MPOic+zB0Pc2gI7AmZdkaYJcAtWSMHDwOGpIV+ZAzob69sVbNrCdwAvDn0LWmm8ad3aYyYNeUZbsCawPUkhFy9DjYQjblQ1TD2WW87Y21amZDgO7Af/yQq3QEWprZYc65H6S9wGakcWe3aSGQZ2YHOucWJZd9j9ztcsjV4yBVEVmSD3pNQJKZtWfLM7Rh+L/Egc65FUGKCsjM7sMPUXiCc64sdD3NycymAw74Nb4NngWOdc7lXLDn8nFQVzblQ86cqW+Pc24dsK7mezMrA9Zn2l9Yc6gz7uwG/LizNauKnXMPByus+QwCJgNfAavw/3BzMdBz/TjYLJvyQWfqIiIxkjMXSkVEcoFCXUQkRhTqIiIxolAXEYkRhbqISIwo1EVEYkShLiISIwp1EZEY+f+uZFQM9V7p7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
    "\n",
    "def selu(x, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(x, alpha)\n",
    "\n",
    "plt.plot(x, selu(x), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which activation function should we use? In general SELU -> ELU -> leaky ReLU (and its variants) -> ReLU -> tanh > logistic. However, because ReLU is the most used activation function, many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is a priority, ReLU might  be the best choice.\n",
    "\n",
    "An update list of activation functions with refereces can be accessed [here](https://paperswithcode.com/methods/category/activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Leaky ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.2934 - accuracy: 0.5906 - val_loss: 0.8794 - val_accuracy: 0.7226\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7985 - accuracy: 0.7396 - val_loss: 0.7106 - val_accuracy: 0.7720\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6858 - accuracy: 0.7737 - val_loss: 0.6366 - val_accuracy: 0.7956\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6263 - accuracy: 0.7918 - val_loss: 0.5925 - val_accuracy: 0.8048\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5877 - accuracy: 0.8037 - val_loss: 0.5637 - val_accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5605 - accuracy: 0.8126 - val_loss: 0.5383 - val_accuracy: 0.8220\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5395 - accuracy: 0.8186 - val_loss: 0.5243 - val_accuracy: 0.8262\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5236 - accuracy: 0.8233 - val_loss: 0.5072 - val_accuracy: 0.8320\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5099 - accuracy: 0.8260 - val_loss: 0.4969 - val_accuracy: 0.8360\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4988 - accuracy: 0.8289 - val_loss: 0.4852 - val_accuracy: 0.8362\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Leaky PReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.3676 - accuracy: 0.5957 - val_loss: 0.9001 - val_accuracy: 0.7254\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8055 - accuracy: 0.7416 - val_loss: 0.7121 - val_accuracy: 0.7720\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6886 - accuracy: 0.7728 - val_loss: 0.6371 - val_accuracy: 0.7898\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6294 - accuracy: 0.7904 - val_loss: 0.5925 - val_accuracy: 0.8070\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5906 - accuracy: 0.8035 - val_loss: 0.5596 - val_accuracy: 0.8144\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5625 - accuracy: 0.8117 - val_loss: 0.5358 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5409 - accuracy: 0.8180 - val_loss: 0.5185 - val_accuracy: 0.8248\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5236 - accuracy: 0.8220 - val_loss: 0.5017 - val_accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5090 - accuracy: 0.8264 - val_loss: 0.4926 - val_accuracy: 0.8328\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4971 - accuracy: 0.8293 - val_loss: 0.4810 - val_accuracy: 0.8386\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) SELU (with 100 hidden layers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.2919 - accuracy: 0.5059 - val_loss: 0.9045 - val_accuracy: 0.6812\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.9088 - accuracy: 0.6622 - val_loss: 0.7168 - val_accuracy: 0.7502\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.6925 - accuracy: 0.7489 - val_loss: 0.7484 - val_accuracy: 0.7152\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.6030 - accuracy: 0.7850 - val_loss: 0.5741 - val_accuracy: 0.8050\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.5305 - accuracy: 0.8142 - val_loss: 0.5149 - val_accuracy: 0.8294\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what happens if we try to use the ReLU activation function with 100 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 2.1582 - accuracy: 0.1884 - val_loss: 1.8396 - val_accuracy: 0.2968\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 1.6136 - accuracy: 0.3405 - val_loss: 1.2495 - val_accuracy: 0.4862\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 1.1963 - accuracy: 0.4847 - val_loss: 1.0074 - val_accuracy: 0.5718\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.9312 - accuracy: 0.6046 - val_loss: 0.9630 - val_accuracy: 0.6234\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.8274 - accuracy: 0.6680 - val_loss: 0.8217 - val_accuracy: 0.6752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization techniques andELU (or variant of ReLU) can significantly reduce the the vanishing/exploding gradients problems at the beginning of training, however it doesn’t guarantee that they won’t come back during training.\n",
    "**Batch Normalization** (BN) technique consists of adding an operation in the model before or after the activation function of each hidden layer. This operation **zerocenters and normalizes** each input, then scales and shifts the result using\n",
    "two new parameters (to be learned during backpropagation). Often, if we add a BN layer as first layer, we do not need to standardize the training set; the BN layer will do it (approximately, since it only looks at one batch at a time).\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”):\n",
    "\n",
    "$\\begin{align}\n",
    "\\mu_{B}=\\frac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{x^{(i)}}\n",
    "\\end{align}$ \n",
    "\n",
    "$\\begin{align}\n",
    "\\sigma _B^2 =\\frac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(x^{(i)}-\\mu _B)^2}\n",
    "\\end{align}$ \n",
    "\n",
    "$\\begin{align}\n",
    "\\hat{x}^{(i)} =\\frac{x^{(i)}-\\mu _B}{\\sqrt{\\sigma _B^2+\\epsilon}} \n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "z^{(i)}=\\gamma \\otimes \\hat{x}^{(i)}+\\beta \n",
    "\\end{align}$\n",
    "\n",
    "where:\n",
    "- $B$ is the mini-batch\n",
    "- $m_B$ is the number of instances in the mini-batch\n",
    "- $\\mu_{B}$ is the vector of input means, evaluated over the mini-batch\n",
    "- $\\sigma_B$ is the vector of input standard deviations, evaluated over the minibatch\n",
    "- $\\hat{x}^{(i)}$ is the vector of zero-centered and normalized inputs for instance i\n",
    "- $\\gamma$ is the scale parameter vector for the layer\n",
    "- $\\beta$ is the shift parameter vector for the layer\n",
    "- $\\epsilon$ is a tiny number (smoothing term) to avoids division by zero\n",
    "- $z^{(i)}$ is the output of the BN operation: a rescaled and shifted version of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets\n",
    "them. But what about at prediction time? We may need to make predictions for individual instances or over small batch of instances, so computing statistics would be unreliable. \n",
    "\n",
    "One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch ones when making predictions. \n",
    "\n",
    "Most implementations of BN (also Keras) estimate these \"final\" statistics during training by using a moving average of the layer’s input means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [Sergey Ioffe and Christian Szegedy **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift** Proceedings of the 32nd International Conference on Machine Learning (2015)](https://arxiv.org/abs/1502.03167) shows tha BN considerably improve DNN, in particular leading to a huge improvement in the [ImageNet](http://www.image-net.org/) classification task. \n",
    "\n",
    "BN, however, adds some complexity to the model and there is a runtime penalty: the NN makes slower predictions due to the extra computations required at each layer. Fortunately, it’s  possible to fuse the BN layer with the previous layer, after training, in order to avoid the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. The previous laryer computes: \n",
    "\n",
    "$\\begin{align}\n",
    "XW+b\n",
    "\\end{align}$\n",
    "\n",
    "the BN layers computes (ignoring the smoothing term): \n",
    "\n",
    "$\\begin{align}\n",
    "\\gamma \\otimes \\frac{(XW+b-\\mu)}{\\sigma} + \\beta\n",
    "\\end{align}$\n",
    "\n",
    "If we define:\n",
    "\n",
    "$\\begin{align}\n",
    "W' = \\gamma \\otimes \\frac{W}{\\sigma}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "b'= \\gamma \\otimes \\frac{(b-\\mu)}{\\sigma} +\\beta\n",
    "\\end{align}$\n",
    "\n",
    "the equation simplifies to\n",
    "\n",
    "$\\begin{align}\n",
    "XW'+b'\n",
    "\\end{align}$\n",
    "\n",
    "So, if we replace the previous layer’s weights and biases ($W$ and $b$) with the updated weights and biases ($W'$ and $b'$), we can get rid of the BN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, we can use BN just adding a **BatchNormalization** layer before or after each hidden layer activation function. As an example, the following model applies BN after every hidden layer and as the first layer (after flattening the input images): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tiny example with just two hidden layers, it’s unlikely that BN will have a positive impact, but for deeper networks it can make a tremendous difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each BN layer adds four parameters per input ($\\gamma$, $\\beta$, $\\mu$, and $\\sigma$), the first BN layer adds 3.136 parameters (which is 4x784). The last two parameters ($\\mu$, and $\\sigma$) are the moving averages and they are not affected by backpropagation, so Keras calls them **non-trainable** (3.136 + 1.200 + 400)/2=2.368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8672 - accuracy: 0.7114 - val_loss: 0.5619 - val_accuracy: 0.8130\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5810 - accuracy: 0.8026 - val_loss: 0.4835 - val_accuracy: 0.8356\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5197 - accuracy: 0.8202 - val_loss: 0.4453 - val_accuracy: 0.8460\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4811 - accuracy: 0.8328 - val_loss: 0.4223 - val_accuracy: 0.8506\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4590 - accuracy: 0.8411 - val_loss: 0.4074 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4412 - accuracy: 0.8439 - val_loss: 0.3960 - val_accuracy: 0.8606\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4247 - accuracy: 0.8501 - val_loss: 0.3860 - val_accuracy: 0.8654\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4178 - accuracy: 0.8535 - val_loss: 0.3803 - val_accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4061 - accuracy: 0.8561 - val_loss: 0.3745 - val_accuracy: 0.8674\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3936 - accuracy: 0.8615 - val_loss: 0.3686 - val_accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better, there's a debate on this topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 1.0439 - accuracy: 0.6797 - val_loss: 0.6768 - val_accuracy: 0.7884\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6844 - accuracy: 0.7812 - val_loss: 0.5584 - val_accuracy: 0.8158\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5949 - accuracy: 0.8039 - val_loss: 0.5024 - val_accuracy: 0.8320\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5458 - accuracy: 0.8173 - val_loss: 0.4678 - val_accuracy: 0.8424\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5151 - accuracy: 0.8262 - val_loss: 0.4441 - val_accuracy: 0.8496\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4920 - accuracy: 0.8335 - val_loss: 0.4271 - val_accuracy: 0.8554\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4720 - accuracy: 0.8397 - val_loss: 0.4136 - val_accuracy: 0.8590\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8454 - val_loss: 0.4038 - val_accuracy: 0.8628\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4458 - accuracy: 0.8468 - val_loss: 0.3948 - val_accuracy: 0.8642\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4316 - accuracy: 0.8509 - val_loss: 0.3879 - val_accuracy: 0.8652\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN has become one of the most-used layers in DNN, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. However, a recent paper [Hongyi Zhang et al. **Fixup Initialization: Residual Learning Without Normalization** arXiv preprint arXiv:1901.09321 (2019)](https://arxiv.org/abs/1901.09321) may change this assumption: by using a novel weight initialization technique, the authors managed to train a very deep NN (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This technique, called **Gradient Clipping**, was presented in the paper [Razvan Pascanu et al. **On the Difficulty of Training Recurrent Neural Networks** Proceedings of the 30th International Conference on Machine Learning (2013)](https://arxiv.org/abs/1211.5063) and it is most often used in recurrent neural networks, for other types of networks, BN is usually sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, the use of Gradient Clipping is done by setting **clipvalue** or **clipnorm** arguments when creating an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. Note that this may change the orientation of the gradient vector. For instance, if the\n",
    "original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If we want to ensure that Gradient Clipping does not change the direction of the gradient vector, we can clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its norm is greater than the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "It is generally not a good idea to train a large DNN from scratch: instead, we can try to find an existing neural network that accomplishes a similar task to the one we are trying to tackle, then reuse the lower layers of this network. This technique is called **transfer learning**. It will not only speedcup training considerably, but also require significantly less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have access to a DNN that was trained to classify pictures into 100 different categories (e.g. animals, plants, vehicles, and everyday objects) and we want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so we can try to reuse parts of the first network.\n",
    "\n",
    "<img src=\"transfer-learning.png\" width=\"500\">\n",
    "\n",
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task. Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. We have  to find the right number of layers to reuse: the more similar the tasks are, the more layers we can reuse (starting with the lower layers). For very similar tasks, we can try to keep all the hidden layers and just replace the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example in order to show how Transfer Learning can be implemented in Keras. Suppose someone has built a model on a simpler Fashion MNIST dataset (one with only eight classes, all the classes except for sandal and shirt). Let’s call this model A. We now want to tackle a different task: we have images of sandals and shirts, and we want to train a binary classifier (positive=shirt, negative=sandal). That task is quite similar to the first one, so we can try transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "- X_train_A: all images of all items except for sandals and shirts (classes 5 and 6)\n",
    "- X_train_B: a much smaller training set of just the first 200 images of sandals or shirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 1s 980us/step - loss: 0.5599 - accuracy: 0.8162 - val_loss: 0.3792 - val_accuracy: 0.8647\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 1s 899us/step - loss: 0.3522 - accuracy: 0.8772 - val_loss: 0.3220 - val_accuracy: 0.8869\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 1s 904us/step - loss: 0.3161 - accuracy: 0.8896 - val_loss: 0.3056 - val_accuracy: 0.8924\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 1s 886us/step - loss: 0.2967 - accuracy: 0.8967 - val_loss: 0.2859 - val_accuracy: 0.8996\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 1s 892us/step - loss: 0.2841 - accuracy: 0.9012 - val_loss: 0.2817 - val_accuracy: 0.9028\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 1s 923us/step - loss: 0.2740 - accuracy: 0.9057 - val_loss: 0.2692 - val_accuracy: 0.9058\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 1s 890us/step - loss: 0.2665 - accuracy: 0.9090 - val_loss: 0.2656 - val_accuracy: 0.9076\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 1s 889us/step - loss: 0.2597 - accuracy: 0.9108 - val_loss: 0.2589 - val_accuracy: 0.9143\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 1s 910us/step - loss: 0.2541 - accuracy: 0.9124 - val_loss: 0.2502 - val_accuracy: 0.9131\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 1s 905us/step - loss: 0.2487 - accuracy: 0.9154 - val_loss: 0.2496 - val_accuracy: 0.9121\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 1s 917us/step - loss: 0.2446 - accuracy: 0.9177 - val_loss: 0.2444 - val_accuracy: 0.9168\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 1s 916us/step - loss: 0.2408 - accuracy: 0.9186 - val_loss: 0.2439 - val_accuracy: 0.9168\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 1s 895us/step - loss: 0.2374 - accuracy: 0.9198 - val_loss: 0.2453 - val_accuracy: 0.9141\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 1s 915us/step - loss: 0.2339 - accuracy: 0.9215 - val_loss: 0.2387 - val_accuracy: 0.9188\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 1s 912us/step - loss: 0.2306 - accuracy: 0.9218 - val_loss: 0.2462 - val_accuracy: 0.9193\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 1s 896us/step - loss: 0.2278 - accuracy: 0.9222 - val_loss: 0.2347 - val_accuracy: 0.9220\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 1s 900us/step - loss: 0.2251 - accuracy: 0.9230 - val_loss: 0.2332 - val_accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 1s 895us/step - loss: 0.2230 - accuracy: 0.9249 - val_loss: 0.2338 - val_accuracy: 0.9198\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 1s 903us/step - loss: 0.2205 - accuracy: 0.9245 - val_loss: 0.2312 - val_accuracy: 0.9218\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 1s 897us/step - loss: 0.2179 - accuracy: 0.9256 - val_loss: 0.2337 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new model (model B) based on that model A layers, let’s reuse all the layers except for the output layer. We can train model B for task B, but since the new output layer was initialized randomly, it will make large errors, so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, we have to set every layer’s trainable attribute to false, compile the model and train it for a few epochs; then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2335 - accuracy: 0.2200 - val_loss: 1.1227 - val_accuracy: 0.2617\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.1146 - accuracy: 0.2450 - val_loss: 1.0182 - val_accuracy: 0.3103\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.0087 - accuracy: 0.3000 - val_loss: 0.9248 - val_accuracy: 0.3580\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.9144 - accuracy: 0.3750 - val_loss: 0.8416 - val_accuracy: 0.4199\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.7006 - accuracy: 0.5600 - val_loss: 0.5144 - val_accuracy: 0.7323\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4434 - accuracy: 0.8300 - val_loss: 0.3640 - val_accuracy: 0.8773\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8950 - val_loss: 0.2824 - val_accuracy: 0.9249\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2461 - accuracy: 0.9200 - val_loss: 0.2318 - val_accuracy: 0.9462\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2012 - accuracy: 0.9500 - val_loss: 0.1973 - val_accuracy: 0.9584\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1694 - accuracy: 0.9700 - val_loss: 0.1698 - val_accuracy: 0.9665\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9800 - val_loss: 0.1514 - val_accuracy: 0.9716\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1272 - accuracy: 0.9900 - val_loss: 0.1370 - val_accuracy: 0.9746\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9900 - val_loss: 0.1258 - val_accuracy: 0.9767\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9900 - val_loss: 0.1156 - val_accuracy: 0.9777\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9900 - val_loss: 0.1079 - val_accuracy: 0.9787\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9807\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9817\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9817\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9817\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=4,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=16,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what’s the final verdict? We can compare with a network learned from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.8604 - accuracy: 0.4950 - val_loss: 0.7212 - val_accuracy: 0.5781\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.6300 - val_loss: 0.5620 - val_accuracy: 0.6937\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4898 - accuracy: 0.7600 - val_loss: 0.4558 - val_accuracy: 0.8022\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3944 - accuracy: 0.8650 - val_loss: 0.3824 - val_accuracy: 0.8621\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8900 - val_loss: 0.3303 - val_accuracy: 0.8895\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.9200 - val_loss: 0.2899 - val_accuracy: 0.9189\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2422 - accuracy: 0.9350 - val_loss: 0.2607 - val_accuracy: 0.9290\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9500 - val_loss: 0.2361 - val_accuracy: 0.9402\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9600 - val_loss: 0.2149 - val_accuracy: 0.9462\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1735 - accuracy: 0.9800 - val_loss: 0.1968 - val_accuracy: 0.9564\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9850 - val_loss: 0.1836 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9900 - val_loss: 0.1715 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9950 - val_loss: 0.1615 - val_accuracy: 0.9645\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9950 - val_loss: 0.1525 - val_accuracy: 0.9716\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9950 - val_loss: 0.1444 - val_accuracy: 0.9716\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1068 - accuracy: 0.9950 - val_loss: 0.1383 - val_accuracy: 0.9726\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9950 - val_loss: 0.1317 - val_accuracy: 0.9757\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9950 - val_loss: 0.1261 - val_accuracy: 0.9777\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9950 - val_loss: 0.1212 - val_accuracy: 0.9777\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9950 - val_loss: 0.1171 - val_accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses.\n",
    "\n",
    "It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimization\n",
    "\n",
    "Training large DNNs can be painfully slow. A huge speed boost comes from using a\n",
    "faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity. In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
    "\n",
    "Recall that Gradient Descent updates the weights by subtracting the gradient of the cost function with regard to the weight multiplied by the learning rate:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\nabla J(\\theta)$\n",
    "\n",
    "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "\n",
    "Momentum optimization ([Boris T. Polyak **Some Methods of Speeding Up the Convergence of Iteration Methods** USSR Computational Mathematics and Mathematical Physics 4, no. 5 (1964)](https://vsokolov.org/courses/750/files/polyak64.pdf)) cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector $m$ and it updates the weights by adding this momentum:\n",
    "\n",
    "$m = \\beta m - \\eta \\nabla J(\\theta)$\n",
    "\n",
    "$\\theta = \\theta + m$\n",
    "\n",
    "In other words, the gradient is **used for acceleration, not for speed**. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, called the **momentum**, which must be set between 0 (high friction) and 1 (no friction). A typicalvmomentum value is 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use momentum optimization in Keras, we have to use the SGD optimizer and set its **momentum hyperparameter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "The **Nesterov Accelerated Gradient (NAG**) method ([Yurii Nesterov **A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence** Doklady AN USSR 269 (1983)](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)), is a small variant to momentum optimization which measures the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum, at:\n",
    "\n",
    "$m = \\beta m - \\eta \\nabla J(\\theta + \\beta m)$\n",
    "\n",
    "$\\theta = \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position:  \n",
    "\n",
    "<img src=\"nesterov.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, simply set **nesterov=True** when creating the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The **AdaGrad algorithm** [John Duchi et al. **Adaptive Subgradient Methods for Online Learning and Stochastic Optimization** Journal of Machine Learning Research 12 (2011)](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) achieves this correction by scaling down the gradient vector along the\n",
    "steepest dimensions:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
