{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "What if we need to tackle complex problems, such as detecting hundreds of types of objects in high-resolution images? We need to train a **deeper** ANN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections. However, training a **Deep Deep Neural Network (DNN)** rise some problems:\n",
    "- the gradients can grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training, making lower layers very hard to train (***vanishing/explodin gradients***);\n",
    "- we might not have enough training data for such a large network, or it might be too costly to label;\n",
    "- training may be extremely slow;\n",
    "- a model with millions of parameters would severely risk overfitting the training set.\n",
    "\n",
    "In the following we will go through each of these problems and present techniques to solve them. \n",
    "\n",
    "1. [The Vanishing Exploding Gradients Problems](#The-Vanishing-Exploding-Gradients-Problems)\n",
    "    - [Initialization strategies](#Initialization-strategies)\n",
    "    - [Nonsaturating Activation Functions](#Nonsaturating-Activation-Functions)\n",
    "    - [Batch Normalization](#Batch-Normalization)\n",
    "    - [Gradient Clipping](#Gradient-Clipping)\n",
    "2. [Transfer Learning](#Transfer-Learning)\n",
    "3. [Faster Optimization](#Faster-Optimization)\n",
    "    - [Momentum Optimization](#Momentum-Optimization)\n",
    "    - [Nesterov Accelerated Gradient](#Nesterov-Accelerated-Gradient)\n",
    "    - [AdaGrad](#AdaGrad)\n",
    "    - [RMSProp](#RMSProp)\n",
    "    - [Adam](#Adam)\n",
    "    - [Higher orders derivatives](#Higher-orders-derivatives)\n",
    "    - [Remarks](#Remarks)\n",
    "4. [Learning Rate Scheduling](#Learning-Rate-Scheduling)\n",
    "    - [Power scheduling](#Power-scheduling)\n",
    "    - [Exponential scheduling](#Exponential-scheduling)\n",
    "    - [Piecewise constant scheduling](#Piecewise-constant-scheduling)\n",
    "    - [Performance scheduling](#Performance-scheduling)\n",
    "    - [Comparison](#Comparison)\n",
    "5. [Regularization](#Regularization)\n",
    "    - [Early stopping](#Early-stopping)\n",
    "    - [L1 and L2 regularization](#L1-and-L2-regularization)\n",
    "    - [Dropout](#Dropout)\n",
    "    - [Monte Carlo Dropout](#Monte-Carlo-Dropout)\n",
    "    - [MaxNorm Regularization](#MaxNorm-Regularization)\n",
    "6. [Summary and Practical Guidelines](#Summary-and-Practical-Guidelines)\n",
    "5. [Exercise](#Exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing Exploding Gradients Problems\n",
    "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the **vanishing\n",
    "gradients problem**. \n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight update and the algorithm diverges. This is the **exploding gradients problem**, which\n",
    "surfaces in recurrent neural networks. \n",
    "\n",
    "More generally, DNN suffer from **unstable gradients**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is due to  the combination of the logistic sigmoid activation function and the weight initialization technique (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). \n",
    "\n",
    "It can be showed that with this combination, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0.\n",
    "\n",
    "For a detailed matematical description, see the paper [Xavier Glorot and Yoshua Bengio, **Understanding the Difficulty of Training Deep Feedforward Neural Networks**, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010)](http://proceedings.mlr.press/v9/glorot10a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the logistic activation function, we can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(x, logit(x), \"b-\", linewidth=2)\n",
    "\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(\"Logistic activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization strategies\n",
    "In order to alleviate the unstable gradients problem, we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. \n",
    "\n",
    "We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. \n",
    "\n",
    "It is not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the **fan-in** and **fan-out** of the layer). A good compromise that has proven to work well in practice: the connection weights of each layer must be initialized randomly as described in the following equations\n",
    "\n",
    "- $ \\text{Normal distribution with mean 0 and variance } \\sigma^2=\\frac{1}{\\text{fan}_\\text{avg}}$  \n",
    "- $ \\text{Uniform distribution between } -r \\text{ and } +r \\text{ with } r=\\sqrt{\\frac{3}{\\text{fan}_\\text{avg}} } $\n",
    "- $ \\text{where } \\text{fan}_\\text{avg}=(\\text{fan}_\\text{in}+\\text{fan}_\\text{out})/2$\n",
    "\n",
    "This initialization strategy is called **Xavier initialization** or **Glorot initialization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some papers have provided similar strategies for different activation functions. \n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution, however it support a number of other strategies. When creating a layer, we can change the initialization stratergy by setting **kernel_initializer** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "The problem with unstable gradients is in part due to a poor choice of activation function. The sigmoid activation function (used in biological neurons) is not the best choice. The **ReLU activation function** is fast to compute and does not saturate for positive values. Unfortunately, it is not perfect. It suffers from a problem known as the **dying ReLUs**: during training, some neurons “die” (they stop outputting anything other than 0). This happens when the neuron weights get tweaked in a way that the weighted sum of inputs are negative for all instances in the training set. In these conditions, the neuron just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "\n",
    "To solve this problem, we can use a variant of the ReLU function: the **leaky ReLU**. \n",
    "\n",
    "$\\text{LeakyReLU}(x)=\\text{max}(\\alpha x, x)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEMCAYAAAACt5eaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5b3H8c+PBDBhRy5xo2At1q2CGqm7cQMvqLXFBRCUWkStqPWFC9dC4aJcRVGhiKgIIqu4lFfdSiu1odJSJCJWsYqKICiyCAECYcnkuX88EzKEhEyWmTPL9/16zYuTMyfnfOfMmR9PnnnOOeacQ0REEluDoAOIiEj1VKxFRJKAirWISBJQsRYRSQIq1iIiSUDFWkQkCahYJxEzc2Z2VdA5kpmZ9TezojhtKy7vl5mdbWb/NrM9ZpYf6+1Vk6VD+HXnBpkjFalY1xMzm2pmbwSdoybMbET4g+XMrNTMvjWzmWbWrobryTezJ6t4bpWZ3V3Ftj+ubfYoc1VWLOcAP6zn7VT13h8OvF6f26rCOOBD4BjgF3HYHlDl+74G/7qXxStHulCxls/wH66jgGuBnwAvBZoohpxzxc65DXHa1nfOud1x2NSPgHecc2ucc5vjsL0qOedC4dddEmSOVKRiHSdmdoKZvWlm281sg5nNNrPDIp4/3cz+YmabzGybmS00szOrWed94eXPD//OVRWev8TM9ppZzkFWUxL+cH3rnHsXmAScYWbNI9ZzuZm9b2a7zOwrMxtlZo1quSuiYmYZZjY5vL1iM/vczO41swYVlrvBzD4ys91mtt7Mpobnrwov8nK4hb0qPH9fN4iZHRt+7icV1jkwvF8bVpfDzEYANwA9Iv5KyQs/t1/L3sx+Ymbzw+vZHG6Rt4h4fqqZvWFmd5rZN2a2xcyeN7PsKvZRBzNzQAtgSnh7/c0sLzzdpuKyZd0TEctcZGaLzWynmRWY2akVtnGGmb1jZjvMbKuZ/dXMjgjv5/OB2yJed4fKukHM7LzwNnaF36MnIo+fcAv9KTP7v/B+32BmYyq+1+lOOyMOzOxw4O/Ax0AX4GKgKfBaxAHZDJgOnBteZhnwVuQHLmJ9ZmZjgNuB851zC4DZwI0VFr0ReMM5tz7KnIfh/4wOhR+YWTdgJvAkcGJ4nVcB/xfVi6+9BsA3wDXA8cBvgfuBX0bkvRl4BngeOBnoDiwPP316+N+b8H85lP28j3NuBVAAXFfhqeuAOc65vVHkGIP/S2R+eDuHA/+suK1wwZ0HFOHf358DZwFTKix6LnAS/hi5NrzcnRXXF1bW5bAT+E14ek4Vy1blIWAIcCrwPTDTzCycuRPwN+AL4GzgjPBrzQxnWoTf92Wve00lr/tI4E/AB8ApwK+A3uHtRroOKMHvk0Hh13NtDV9LanPO6VEPD2AqvjBW9txI4K8V5rUCHNClit8xYB3QN2Kewx/AzwMrgA4Rz+XiD/YjI9ZfDFx2kMwj8EW5CP+Bd+HHuIhl/g4Mq/B7V4Z/x8I/5wNPVrGNVcDdVWz74xru44eB+RE/rwUePsjyDriqwrz+QFHEz3cCqyNeSzugFDizBjkqfe8jt4//T2Mr0Czi+bzwMj+KWM8aIDNimUmR26oiTxHQv5L1tomY1yE8L7fCMt0iljk7PO+o8M8zgX8dZLsHvO+VbGcUvtg3qPAe7AayI9azqMJ63gaeq8tnMtUealnHx2nAeWZWVPagvBVyDICZtTWzZ8xshZltBbYDbYEfVFjXGPwH7Rzn3Kqymc65AuAj/J/kAH2ALfhWzcF8CXTGtzx/CyzFtxwjs/+2QvZZQBPgsIorq09mdkv4T/ON4e3eRXh/mFlb4Ejgr3XczGzgCHyLFvx+W+mcWxRNjho4Hvi3c257xLx/4v9jOCFi3idu//7eb/HHQaz8u8K2iNjeKdR9/x6PL8SlEfMWAo3wfe2V5SjLEsvXnXRUrOOjAfAmvihGPjoCZaMIXsAXzLvwfwp2xrccK/YNv40vkt0r2c5zlP95fiMw1TkXqibbHufcF8655c65/8N/aCZUyP6/FXKfHM6+sZp1A2zD96lW1BLf0qyUmV0LjMW3NruFt/sU5fvDoth2tZz/snE+5V0h1+FblNHmiJbhW5yVxoiY3lvJczX9nJYVxsh91LCKZSO3V5ajbHv1sY/j+bpTWmbQAdLEUnyf52rn+0Ercw5wh3PuTQDzXwoeXslybwF/IPzFmXPuhYjnZgCPmtkgfB9kr1pkfQD4zMzGO+feD2c/zjn3RS3WBX60yWmVzD81/FxVzgEWO+f2DQ0zs2PKpp1z683sG+Ai/H9gldkLZESRcQYw3syexY+G6RltjrA9UWznE+BGM2sW0bo+C1+Q/hNFxpoo+0/08IjpzrVYz1LgwoM8H+3rvsbMGkS0rs8J/+6XtciUtvQ/V/1qbmadKzw64FuqLYA5ZvZTM/uhmV1sZs+aWbPw764A+pofNXI68CL+gD6Ac+4N4GrgaTO7PmL+VuBl4DHg7865z2v6ApxzK4HX8EUbfH97HzMbaWYnmdlxZnaVmT1S4VfbVPLajwCeALqZ2bDwazvRzEYBZ+JbrFVZAZxqZv9tZh3NbBh+9EGkUcBvzOwu8yM7OpvZ4IjnVwEXmdlhZtbqINuai295Tgbeq7DfosmxCjjJzH5sZm3MrLJW7ExgBzDN/KiQ8/Bfjv6hDv8RVuULfDfbiPB+6QoMrcV6HgVOCR+nncKvb4CZlXUBrQK6hEeAtKli9MZT+G6mp8zseDPrge/zf9I5t7MWmdJX0J3mqfLA/5nsKnm8En6+I/AKvh+5GN+qHA80Cj/fCVgcfu5LoB9+9MiIiG3s94UZcHl4+esj5p0XXu76KDKPoJIv+fAtPgecFf65K/Au/kvIbfgRFIMils+v4rWPqfD7m/EjDvKB86rJ1ghfPLcAheHp3wGrKiz3K3zrbQ/wHTClwv75HN/CXhWe15+ILxgjlp0Wznx7TXMA/wX8Bf89gwPyqni/foLvAy4Or28q0KLCMfRGhe1X+h5VWGa/Lxgj3sNl4W0tAnpQ+ReMVX4JGZ53Dv5L5uLw658PHB5+7tjwusu+nO5QxTrOwx/bu4H1+P/AG1c4fip+UXnAvkj3R9k34JIiwn2szwBHOLVcRFKG+qxTRHgcbwf8SI5JKtQiqUV91qnjXvz1ITZT3t8sIilC3SAiIklALWsRkSQQsz7rNm3auA4dOsRq9VHZsWMHTZo0CTRDotC+8D777DNCoRAnnHBC9QunAR0X5SrbF99+C+vWQcOGcMIJkBmHb/nef//9Tc65/6o4P2ab7tChAwUFBbFafVTy8/PJy8sLNEOi0L7w8vLyKCwsDPzYTBQ6LspV3Bfvvgt5eWAG8+bBhQc7PagemdnqyuarG0REpIItW+C666C0FO67L36F+mBUrEVEIjgHN90Ea9ZAly4wcmTQiTwVaxGRCM89B6++Cs2awezZvr86EahYi4iE/ec/cGf4Vg8TJ8IP6/VunXVTo2IdvpDNLjObEatAIiJB2LOnAb17Q3Ex9Ovn+6wTSU1b1hOAJbEIIiISpGef/SEffgg/+hFMmFD98vEWdbE2s174q27V9c4RIiIJ5c034dVXjyIzE2bN8v3ViSaqYm3+TtcjgcHVLSsikkzWrYP+/f30qFFw+gG3Vk4M0Z4U8wAw2Tm3Jnzj40qZ2UBgIEBOTg75+fl1DlgXRUVFgWdIFNoXXmFhIaFQSPsiLN2Pi9JSuPfek9m0qTWdO28kN3c5ibo7qi3WZtYZuBh/88yDcs49CzwLkJub64I+M0pnZ5XTvvBatmxJYWGh9kVYuh8Xjz4K778PbdrA0KGfc+GFeQEnqlo0Les8/HWSvw63qpsCGWZ2gnPu1NhFExGJnSVL4P77/fTUqdCkSaV30UsY0fRZPwscQ/mdrZ/G36m7WwxziYjEzPbt0Ls3lJTAHXdAjx5BJ6petS3r8B1H9t11xMyKgF3OuY1V/5aISOIaNAi+/BI6dYLRo4NOE50aX3XPOTciBjlEROJi5kyYNg2ysvzp5IccEnSi6Oh0cxFJGytXwq23+ulx4+D444PNUxMq1iKSFvbu9f3U27dDz54wYEDQiWpGxVpE0sLw4fDee9CuHUya5G8qkExUrEUk5b3zDjz8MDRo4PusW7UKOlHNqViLSErbtAn69vU3FRg2DM49N+hEtaNiLSIpyzm48UZ//Y+zz4ahQ4NOVHsq1iKSsp56Cl5/HVq29N0f8bg7eayoWItISvroIxgcvk7opEnQvn2weepKxVpEUs7OndCrF+ze7YfoXXVV0InqTsVaRFLO4MHwySdw3HEwdmzQaeqHirWIpJS5c+Hpp6FRI386eZMmQSeqHyrWIpIy1qyBX/3KTz/yCHTuHGye+qRiLSIpIRTydyXfsgW6d/eXPk0lKtYikhIeeggWLICcHHj++eQ7nbw6KtYikvT++U8YMcJPT58ObdsGGicmVKxFJKkVFkKfPr4b5J574JJLgk4UGyrWIpK0nINbboHVqyE3Fx58MOhEsaNiLSJJa+pUmDPHD8+bNcsP10tVKtYikpQ++wxuv91PP/UUdOwYbJ5YU7EWkaSze7e/68uOHb6/ul+/oBPFnoq1iCSd+++HDz6Ao4+GiRNTb5heZVSsRSSpzJsHjz8OGRm+n7p586ATxYeKtYgkjfXr4YYb/PQDD8AZZwSbJ55UrEUkKZSW+kK9YQNccAHce2/QieJLxVpEksLYsfDnP8Ohh/qzFDMygk4UXyrWIpLwli6FIUP89OTJcOSRweYJgoq1iCS0oiI/TG/vXrjtNvjZz4JOFAwVaxFJaHfcAStWwEknwaOPBp0mOCrWIpKw5szxlzs95BB48UXIygo6UXBUrEUkIa1aBQMH+uknnoATTww0TuBUrEUk4ZSU+NPIt22DK6+Em28OOlHwVKxFJOH87//CokV+1Mdzz6XH6eTVUbEWkYSyYAGMGuUL9IwZfly1qFiLSALZvBn69vU3FfjtbyEvL+hEiUPFWkQSgnMwYACsXQtnngnDhwedKLGoWItIQnjmGZg7119Fb9YsyMwMOlFiUbEWkcAtXw533eWnn3kGOnQINE5CiqpYm9kMM1tnZtvMbIWZDYh1MBFJD8XF/nTyXbvgl7+EXr2CTpSYom1ZPwR0cM41B64AHjSz02IXS0TSxT33wEcfwbHHwu9/H3SaxBVVsXbOLXfO7S77Mfw4JmapRCQtvPYaTJgADRvC7NnQtGnQiRJX1F34ZvYU0B/IAj4A3qpkmYHAQICcnBzy8/PrJWRtFRUVBZ4hUWhfeIWFhYRCIe2LsCCPi40bGzFgwOlAQwYM+IJt29YS5NuS6J8Rc85Fv7BZBnAmkAeMds7trWrZ3NxcV1BQUOeAdZGfn0+eBmoC2hdl8vLyKCwsZNmyZUFHSQhBHRehEFxyCfztb9CtG7z1FjQIeLhDonxGzOx951xuxfk12j3OuZBzbiFwFHBrfYUTkfTyyCO+ULdtCy+8EHyhTga13UWZqM9aRGph8WIYNsxPv/AC5OQEmydZVFuszaytmfUys6ZmlmFm3YDewDuxjyciqWTbNj9MLxTy46ovvTToRMkjmi8YHb7L42l8cV8N/MY598dYBhOR1OIc3HorfPUVnHIKPPRQ0ImSS7XF2jm3ETg/DllEJIVNn+5PI8/O9sP0GjcOOlFyUbe+iMTcF1/4m90CjB8PP/5xsHmSkYq1iMTUnj2+n7qoCK65xp9SLjWnYi0iMTVsGBQUQPv2/iJNuutL7ahYi0jMvP22H1OdkeH7q1u2DDpR8lKxFpGY2LgRrr/eTw8fDmedFWyeZKdiLSL1zjnfN/3dd3DeeXD//UEnSn4q1iJS78aPhzffhFat/E1vMzKCTpT8VKxFpF4tW+avUQ0weTK0axdsnlShYi0i9WbHDj9Mb88euPlm+PnPg06UOlSsRaTe3HUXfPopnHACPP540GlSi4q1iNSLV16BSZP8aeQvvuhPK5f6o2ItInX29ddw001+eswY+MlPgs2TilSsRaROSkrguuugsBAuv7z8GiBSv1SsRaRORo2ChQvh8MNhyhSdTh4rKtYiUmsLF8LIkb5Az5gBbdoEnSh1qViLSK1s2QJ9+kBpKdx3H1x4YdCJUpuKtYjUmHMwcCCsWQNduvjWtcSWirWI1NjkyX6oXrNm/mp6DRsGnSj1qViLSI385z9wxx1+euJEOOaYYPOkCxVrEYnarl3+dPLiYujXzw/Zk/hQsRaRqA0ZAh9+6FvTEyYEnSa9qFiLSFTefBPGjYPMTH938mbNgk6UXlSsRaRa69ZB//5+etQoOP30QOOkJRVrETmo0lJ/e65Nm+Dii+Huu4NOlJ5UrEXkoB57DObP92cnTpsGDVQ1AqHdLiJVWrKk/P6JU6f6639IMFSsRaRS27f7YXolJX5cdY8eQSdKbyrWIlKpQYPgyy+hUycYPTroNKJiLSIHmDXL909nZflheoccEnQiUbEWkf2sXAm33OKnx42D448PNo94KtYiss/evb6fevt26NkTBgwIOpGUUbEWkX2GD4f33oN27fzNb3XXl8ShYi0iALzzDjz8sB9HPXMmtGoVdCKJpGItImza5K+i5xwMGwbnnht0IqlIxVokzTkHN94I334LZ58NQ4cGnUgqo2Itkuaeegpefx1atPDdH5mZQSeSylRbrM2ssZlNNrPVZrbdzD4ws/+ORzgRia2VK5sweLCfnjQJ2rcPNo9ULZqWdSawBjgfaAEMA14ysw6xiyUisbZzJzzwwAns3u2H6F19ddCJ5GCq/YPHObcDGBEx6w0z+wo4DVgVm1giEmuDB8OqVU047jgYOzboNFKdGvdOmVkOcCywvJLnBgIDAXJycsjPz69rvjopKioKPEOi0L7wCgsLCYVCab8v3n23DU8/fRKZmSEGD/6AJUuKgo4UuET/jJhzLvqFzRoCfwK+dM7dfLBlc3NzXUFBQR3j1U1+fj55eXmBZkgU2hdeXl4ehYWFLFu2LOgogVm71l+cafNmuO22z3nyyY5BR0oIifIZMbP3nXO5FedHPRrEzBoA04E9wKB6zCYicRIKQd++vlB37w49e34TdCSJUlTF2swMmAzkAD2dc3tjmkpEYuKhh2DBAsjJgeef1+nkySTalvVE4HjgcudccQzziEiMLFoEI0b46WnToG3bQONIDUUzzro9cDPQGfjOzIrCj+tink5E6kVhob+aXigE99wDXbsGnUhqKpqhe6sB/bEkkqSc89enXr0acnPhwQeDTiS1odPNRVLc1KkwZw40aeLvANOoUdCJpDZUrEVS2IoVcPvtfnrCBOioUXpJS8VaJEXt3g29esGOHdCnD1x/fdCJpC5UrEVS1P33wwcfwNFHw8SJGqaX7FSsRVLQvHnw+OOQkeH7qZs3DzqR1JWKtUiKWb8ebrjBTz/wAJxxRrB5pH6oWIukkNJS6N8fNmyACy6Ae+8NOpHUFxVrkRQydqzvAjn0UJg+3XeDSGpQsRZJEUuXwpAhfnryZDjyyGDzSP1SsRZJAUVF/nTyvXvhttvgZz8LOpHUNxVrkRRw553+BJiTToJHHw06jcSCirVIkpszB6ZMgUMOgRdfhKysoBNJLKhYiySxVatg4EA//fjjcOKJgcaRGFKxFklSJSX+NPJt2+DKK/2V9SR1qViLJKmRI/0NBY48Ep57TqeTpzoVa5EktGCBvy61GcyY4cdVS2pTsRZJMps3+5veOucv1pQAN+SWOFCxFkkizsGAAbB2LZx5JgwfHnQiiRcVa5Ek8uyzMHeuv4rerFnQsGHQiSReVKxFksTy5fCb3/jpZ56BDh0CjSNxpmItkgR27fKnk+/a5a+q16tX0Ikk3lSsRZLAPffARx/5eyiOHx90GgmCirVIgnv9dXjySd8//eKL0LRp0IkkCCrWIgnsm2/gl7/00w89BKeeGmweCY6KtUiCCoX8Hcm//x66doW77go6kQRJxVokQT36KLzzDrRtCy+8AA30aU1revtFEtDixTB0qJ9+4QU47LBg80jwVKxFEsy2bX6YXijkuz4uvTToRJIIVKxFEsyvfw1ffQWnnOK/VBQBFWuRhDJ9OsycCdnZMHs2NG4cdCJJFCrWIgniiy98qxr8iS8//nGweSSxqFiLJIA9e3w/dVERXHNN+dhqkTIq1iIJYNgwKCiA9u39RZp01xepSMVaJGBvvw2PPAIZGf6ypy1bBp1IEpGKtUiANm70ZymCv5HAWWcFm0cSl4q1SECc833T330H553nb9ElUpWoirWZDTKzAjPbbWZTY5xJJC2MHw9vvgmtWvmb3mZkBJ1IEllmlMt9CzwIdAOyYhdHJD18+KG/RjXA5MnQrl2weSTxRVWsnXN/ADCzXOComCYSSXE7dvg7vezZAzffDD//edCJJBlE27KOipkNBAYC5OTkkJ+fX5+rr7GioqLAMyQK7QuvsLCQUCgU6L4YM+ZYPv30CNq338GVV75Pfn5pYFl0XJRL9H1Rr8XaOfcs8CxAbm6uy8vLq8/V11h+fj5BZ0gU2hdey5YtKSwsDGxfvPKK76du3Bhee60JJ598XiA5yui4KJfo+0KjQUTi5Ouv4aab/PSYMXDyycHmkeSiYi0SByUlcN11UFgIl18Ot90WdCJJNlF1g5hZZnjZDCDDzA4BSpxzJbEMJ5IqRo2ChQvh8MNhyhSdTi41F23LeihQDAwB+oanh8YqlEgqWbgQRo70BXr6dGjTJuhEkoyiHbo3AhgR0yQiKWjLFt/9UVoKQ4bARRcFnUiSlfqsRWLEORg40H+x2KWLb12L1JaKtUiMTJ7sh+o1a+avptewYdCJJJmpWIvEwKefwp13+umJE+GYY4LNI8lPxVqknu3a5U8n37kT+vXzfdYidaViLVLPhgzxF2o65hiYMCHoNJIqVKxF6tFbb8G4cZCZ6e9O3qxZ0IkkVahY1wMz45VXXgk6hgRs3Tro399PjxoFp58eaBxJMWlRrPv3789ll10WdAxJYaWl/vZcGzfCxRfD3XcHnUhSTVoUa5FYe+wxmD/fn504bRo00CdL6lnaH1KffPIJPXr0oFmzZrRt25bevXvz3Xff7Xt+yZIldO3alTZt2tC8eXPOOeccFi1adNB1jh49mjZt2rB48eJYx5cEUFBQfv/EqVP99T9E6ltaF+t169Zx3nnncdJJJ/Hee+8xf/58ioqKuOKKKygt9ReE3759O/369ePdd9/lvffeo3PnznTv3p1NmzYdsD7nHHfffTfjx49nwYIF/PSnP433S5I4274devf2V9W74w7o0SPoRJKq6vXmA8lm4sSJdOrUidGjR++bN23aNFq3bk1BQQFdunThwgsv3O93xo8fz6uvvsq8efPo27fvvvmhUIgbb7yRf/zjHyxcuJAOHTrE62VIgAYNgi++gE6dIOIwEql3aV2s33//ff7+97/TtGnTA5778ssv6dKlCxs2bGDYsGH87W9/Y/369YRCIYqLi/n666/3W/7uu+8mMzOTxYsX07Zt23i9BAnQrFm+fzoryw/TO+SQoBNJKkvrYl1aWkqPHj0YM2bMAc/l5OQAcMMNN7B+/XqeeOIJOnToQOPGjbnooovYs2fPfstfcsklzJ49m7feeov+ZeO3JGWtXAm33OKnx42D448PNo+kvrQu1qeeeiovvfQS7du3p2EVV9lZuHAhv//97+kR7oxcv34969atO2C57t2784tf/IKrr74aM+OGG26IaXYJzt690KeP76/u2RMGDAg6kaSDtPmCcdu2bSxbtmy/R48ePdi6dSvXXnstixcvZuXKlcyfP5+BAweyfft2AI499lhmzJjBJ598wpIlS+jVqxeNGjWqdBuXXXYZL7/8MrfccgvTpk2L58uTOBo+HBYvhnbtYNIk3fVF4iNtWtbvvvsup5xyyn7zevbsyT/+8Q/+53/+h0svvZRdu3bxgx/8gK5du9K4cWMApkyZwsCBAznttNM44ogjGDFiBBs3bqxyO5dddhkvvfQS11xzDQDXX3997F6UxN0778DDD/tx1DNnQqtWQSeSdJEWxXrq1KlMnTq1yucPdqp4p06dDhgv3a9fv/1+ds7t9/Pll19OcXFxzYNKQtu0yV9Fzzn43e/g3HODTiTpJG26QUTqwjn41a/g22/h7LNhqO5AKnGmYi0ShaeegtdegxYtfPdHZlr8TSqJRMVapBoffQSDB/vpSZOgfftg80h6UrEWOYjiYn86+e7dfoje1VcHnUjSVVIX65KSEm699Vaee+65oKNIiho8GJYvh+OOg7Fjg04j6Sxpe962b99Ojx49KCgo4IUXXuDwww/fd+KKSH2YO9ff7LZRI386eZMmQSeSdJaULetvvvmGU089lffee4/i4mKKi4u59tprWbp0adDRJEWsXVt+ZuLo0dC5c7B5RJKuWP/73/+mU6dOfPXVV+zevXvf/B07dtCtW7d9lzYVqa1QCPr2hc2boXt3uPPOoBOJJFmx/vOf/8xZZ53F999/TygU2u+5rKwshg4dSgPdokPq6OGHYcECyMmB55/X6eSSGJKmz3rSpEnceeedlZ4ZmJ2dzezZs7niiisCSCapZNEif+0P8Jc/1dVuJVEkfLF2zjFkyBCefPLJAwp1gwYNaN68OW+//Ta5ubkBJZRUsXWrv5peKAT33ANduwadSKRcQhfrPXv20KdPH/70pz+xc+fO/Z5r1KgRhx12GAsWLNBdWaTOnIObb4ZVqyA3Fx58MOhEIvtL2GK9ZcsWunXrxscff3xAizorK4sTTzyRv/zlL7TSZc+kHkydCnPm+OF5s2b54XoiiSQhv41btWoVnTt35sMPPzygUGdnZ9O9e3cWLlyoQi31YsUKuP12Pz1hAnTsGGwekcokXLEuKCjglFNOYe3atQfcOis7O5tBgwbx8ssv77vetEhd7N7tTyffscP3V+vy45KoAinWH3/8MWeccQbff//9fvP/+Mc/cv7551NYWHjAeOmsrCyeeOIJRo8ejWksldST3/4Wli6Fo4/2Zyvq0JJEFUixHj16NEuWLOGSSy5h165dAIwdO5bevXsf8EUiQJMmTZg7dy4DBw6Md6SSLzYAAAfbSURBVFRJYfPmwWOPQUaG76du3jzoRCJVi/sXjFu3buWVV16htLSUTz/9lF69enHUUUfx/PPPH9A/nZGRQcuWLXnnnXc4+eST4x1VUtj69VB2T+ORI+GMM4LNI1KduBfrqVOn7jvLsLi4mLfffhvggBZ148aNadeuHfn5+Rx55JHxjikprn9/2LABLrgA7rsv6DQi1YuqG8TMWpvZXDPbYWarzaxPbTbmnGPMmDH7FeadO3ceUKizsrLo0qULS5cuVaGWerdhQ2PmzYNDD4Xp0303iEiii7ZlPQHYA+QAnYE3zexD59zymmwsPz+fwsLCgy6TnZ1Nz549mTJlCpm6d5LUUUkJFBb6izJt2eKH6a1blwXA5MmgtoAkC6t4Z+4DFjBrAmwBTnLOrQjPmw5845wbUtXvNWvWzJ122mn7zfvoo4/YvHlzldtq0KABOTk5dOzYsV5GfBQWFtKyZcs6rycVJPu+CIV84S0pgb17K/+3snkVrvcFLAOgY8fOHHFE3F9Gwkn246I+Jcq+WLBgwfvOuQOunxFN0/VYIFRWqMM+BM6vuKCZDQQGAjRs2HC/VvTevXvZsmXLQTdUWlrKhg0baNmyJY3q4RSyUChUbUs+XSTCvnAOQiGLeDSgpKT856qmQ6EGVNOmOKiMDLfvsWePo2HDENnZhejQSIzjIlEk+r6Iplg3BbZWmLcVaFZxQefcs8CzALm5ua6goGDfc8OHD2f06NH7XYO6Krt372bRokW0aNEiinhVy8/PJy8vr07rSBX1tS+c8/clLOtW2Lw5+umtFY+iGsjKgtatoVUr/2+0082bQ+RVc/Py8igsLGTZsmV13hepQJ+RcomyL6rqVYimWBcBFUegNge2R7vxkpISxo8fH1WhDoVCrF69mt/97neMGzcu2k1IDYVCvnjWpNiW/RzF21gpM2jZsmbFtuznQw6p39cvkmyiKdYrgEwz6+ic+zw8rxMQ9ZeLr7/+OiUlJVU+37RpU/bu3cuhhx5Kt27d6N69O5dcckm0q09rlbVyKyu4X355Ms6Vz9+6lVp3LTRu7EdS1LSV26LF/q1cEYletcXaObfDzP4AjDSzAfjRID8Dzop2I4888gjbt5c3xLOzs3HOkZ2dzUUXXcTll1/OBRdckLbD9MpauTXtVtiyBcIngEah9X4/1aWVm5VV77tARKoR7di4XwNTgA3A98Ct0Q7bW7lyJf/617/IysqiUaNGnH/++VxxxRVccMEFHH300Sl1nY9du2pebDdv9kPLatvKbdTo4K3csp/XrPmQCy7otO+5Fi00vlgkmURVrJ1zm4Era7OBVq1aMWnSJM4++2yOO+64hC/OpaXRt3Ir/hx9K/dALVocvNhWNZ2VFd3Fh/Lzt3D66bXPJyLBivlZJ61atWLAgAGx3swBdu2C779vxPLl1ffnRk5v2VK3Vm5Ni23r1r47Qq1cETmYhD5FsLQUtm2r3TAxf02oqLvV99O8ec2Kbdl0drYusSkisRGXYr17d+2+PNuyxRfs2mjYEJo23cNhhzWq0aiFli1BZ7mLSKKJWVn65BNo184X3kouUR215s1rPkSsdWvfyl2w4J8JMchdRKSuYlasi4th7drwRjJrN0SsZUvfQhYRSXcxK9bHH+/vxNG6tb9jtPpyRURqL2bFOjsbfvCDWK1dRCS96ORfEZEkoGItIpIEVKxFRJKAirWISBJQsRYRSQIq1iIiSUDFWkQkCahYi4gkARVrEZEkYK62F2+ubsVmG4HVMVl59NoAmwLOkCi0L8ppX5TTviiXKPuivXPuvyrOjFmxTgRmVuCcyw06RyLQviinfVFO+6Jcou8LdYOIiCQBFWsRkSSQ6sX62aADJBDti3LaF+W0L8ol9L5I6T5rEZFUkeotaxGRlKBiLSKSBFSsRUSSQFoVazPraGa7zGxG0FmCYGaNzWyyma02s+1m9oGZ/XfQueLFzFqb2Vwz2xHeB32CzhSEdD8OqpLo9SGtijUwAVgSdIgAZQJrgPOBFsAw4CUz6xBgpniaAOwBcoDrgIlmdmKwkQKR7sdBVRK6PqRNsTazXkAh8NegswTFObfDOTfCObfKOVfqnHsD+Ao4LehssWZmTYCewDDnXJFzbiHwGtAv2GTxl87HQVWSoT6kRbE2s+bASGBw0FkSiZnlAMcCy4POEgfHAiHn3IqIeR8C6diy3k+aHQcHSJb6kBbFGngAmOycWxN0kERhZg2BmcALzrlPg84TB02BrRXmbQWaBZAlYaThcVCZpKgPSV+szSzfzFwVj4Vm1hm4GHgi6KyxVt2+iFiuATAd3387KLDA8VUENK8wrzmwPYAsCSFNj4P9JFN9yAw6QF055/IO9ryZ/QboAHxtZuBbWBlmdoJz7tSYB4yj6vYFgPmdMBn/JVt359zeWOdKECuATDPr6Jz7PDyvE+n7p3+6HgcV5ZEk9SHlTzc3s2z2b1HdjX9zbnXObQwkVIDM7GmgM3Cxc64o6DzxZGYvAg4YgN8HbwFnOefSrmCn83EQKZnqQ9K3rKvjnNsJ7Cz72cyKgF2J9kbEg5m1B24GdgPfhVsSADc752YGFix+fg1MATYA3+M/kOlYqNP9ONgnmepDyresRURSQdJ/wSgikg5UrEVEkoCKtYhIElCxFhFJAirWIiJJQMVaRCQJqFiLiCQBFWsRkSTw/ydvq1XbtJ2MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "plt.plot(x, leaky_relu(x, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for z < 0. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
    "\n",
    "Other variants are: the **randomized leaky ReLU (RReLU)**, where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing, and the **parametric leaky ReLU (PReLU)**, where $\\alpha$ is authorized to be learned during training. For a comparison of several variants of the ReLU activation function, we can refer to the paper:[Bing Xu et al. **Empirical Evaluation of Rectified Activations in Convolutional Network** arXiv preprint arXiv:1505.00853 (2015)](https://arxiv.org/pdf/1505.00853.pdf)\n",
    "\n",
    "In Keras, to use the leaky ReLU activation function, we can just create a LeakyReLU layer and add it to our model just after the layer we want to apply it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use the **exponential linear unit (ELU)** activation function, that outperform ReLU variants: \n",
    "\n",
    "$\\text{ELU}_\\alpha(x) = \n",
    "\\begin{cases}\n",
    "    \\alpha(\\text{exp}(x)-1 & \\text{if} & x \\lt 0 \\\\\n",
    "     x & \\text{if} & x \\gt 0\n",
    "\\end{cases}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu(x, alpha=1):\n",
    "    return np.where(x < 0, alpha * (np.exp(x) - 1), x)\n",
    "\n",
    "plt.plot(x, elu(x), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU functions (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **Scaled ELU (SELU)** activation function is a variant of the ELU activation function. For a neural network composed exclusively of a stack of dense layers, the SELU activation function allows a self-normalization:\n",
    "the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
    "\n",
    "def selu(x, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(x, alpha)\n",
    "\n",
    "plt.plot(x, selu(x), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which activation function should we use? In general SELU -> ELU -> leaky ReLU (and its variants) -> ReLU -> tanh > logistic. However, because ReLU is the most used activation function, many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is a priority, ReLU might  be the best choice.\n",
    "\n",
    "An update list of activation functions with refereces can be accessed [here](https://paperswithcode.com/methods/category/activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Leaky ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 878us/step - loss: 1.2743 - accuracy: 0.6130 - val_loss: 0.8752 - val_accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 1s 812us/step - loss: 0.7895 - accuracy: 0.7429 - val_loss: 0.7044 - val_accuracy: 0.7762\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 1s 814us/step - loss: 0.6791 - accuracy: 0.7745 - val_loss: 0.6314 - val_accuracy: 0.7960\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 820us/step - loss: 0.6212 - accuracy: 0.7928 - val_loss: 0.5904 - val_accuracy: 0.8096\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 820us/step - loss: 0.5843 - accuracy: 0.8050 - val_loss: 0.5607 - val_accuracy: 0.8150\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 834us/step - loss: 0.5575 - accuracy: 0.8130 - val_loss: 0.5371 - val_accuracy: 0.8244\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 1s 814us/step - loss: 0.5377 - accuracy: 0.8205 - val_loss: 0.5188 - val_accuracy: 0.8316\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 1s 814us/step - loss: 0.5213 - accuracy: 0.8239 - val_loss: 0.5044 - val_accuracy: 0.8340\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 1s 807us/step - loss: 0.5078 - accuracy: 0.8280 - val_loss: 0.4939 - val_accuracy: 0.8370\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 1s 830us/step - loss: 0.4963 - accuracy: 0.8314 - val_loss: 0.4828 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Leaky PReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 953us/step - loss: 1.4294 - accuracy: 0.5748 - val_loss: 0.9422 - val_accuracy: 0.7078\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 925us/step - loss: 0.8367 - accuracy: 0.7343 - val_loss: 0.7381 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 912us/step - loss: 0.7064 - accuracy: 0.7716 - val_loss: 0.6550 - val_accuracy: 0.7862\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 903us/step - loss: 0.6386 - accuracy: 0.7916 - val_loss: 0.5994 - val_accuracy: 0.8034\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 932us/step - loss: 0.5955 - accuracy: 0.8040 - val_loss: 0.5640 - val_accuracy: 0.8208\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 913us/step - loss: 0.5651 - accuracy: 0.8128 - val_loss: 0.5382 - val_accuracy: 0.8260\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 920us/step - loss: 0.5429 - accuracy: 0.8176 - val_loss: 0.5195 - val_accuracy: 0.8308\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 903us/step - loss: 0.5255 - accuracy: 0.8223 - val_loss: 0.5079 - val_accuracy: 0.8316\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.5112 - accuracy: 0.8264 - val_loss: 0.4899 - val_accuracy: 0.8402\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.4990 - accuracy: 0.8297 - val_loss: 0.4803 - val_accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) SELU (with 100 hidden layers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.9661 - accuracy: 0.6285 - val_loss: 0.7183 - val_accuracy: 0.7312\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.6572 - accuracy: 0.7659 - val_loss: 0.6298 - val_accuracy: 0.7804\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.6931 - accuracy: 0.7527 - val_loss: 1.1169 - val_accuracy: 0.7006\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.7993 - accuracy: 0.7246 - val_loss: 0.7850 - val_accuracy: 0.7318\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.8747 - accuracy: 0.6790 - val_loss: 0.8143 - val_accuracy: 0.6766\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what happens if we try to use the ReLU activation function with 100 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 1.8988 - accuracy: 0.2553 - val_loss: 1.3451 - val_accuracy: 0.4562\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 1.2564 - accuracy: 0.4710 - val_loss: 1.1376 - val_accuracy: 0.4636\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.9766 - accuracy: 0.5870 - val_loss: 0.8601 - val_accuracy: 0.6506\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.8390 - accuracy: 0.6529 - val_loss: 0.7783 - val_accuracy: 0.6848\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.7797 - accuracy: 0.6988 - val_loss: 0.8253 - val_accuracy: 0.6862\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization techniques andELU (or variant of ReLU) can significantly reduce the the vanishing/exploding gradients problems at the beginning of training, however it doesn’t guarantee that they won’t come back during training.\n",
    "**Batch Normalization** (BN) technique consists of adding an operation in the model before or after the activation function of each hidden layer. This operation **zerocenters and normalizes** each input, then scales and shifts the result using\n",
    "two new parameters (to be learned during backpropagation). Often, if we add a BN layer as first layer, we do not need to standardize the training set; the BN layer will do it (approximately, since it only looks at one batch at a time).\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”):\n",
    "\n",
    "$\\begin{align}\n",
    "\\mu_{B}=\\frac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{x^{(i)}}\n",
    "\\end{align}$ \n",
    "\n",
    "$\\begin{align}\n",
    "\\sigma _B^2 =\\frac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(x^{(i)}-\\mu _B)^2}\n",
    "\\end{align}$ \n",
    "\n",
    "$\\begin{align}\n",
    "\\hat{x}^{(i)} =\\frac{x^{(i)}-\\mu _B}{\\sqrt{\\sigma _B^2+\\epsilon}} \n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "z^{(i)}=\\gamma \\otimes \\hat{x}^{(i)}+\\beta \n",
    "\\end{align}$\n",
    "\n",
    "where:\n",
    "- $B$ is the mini-batch\n",
    "- $m_B$ is the number of instances in the mini-batch\n",
    "- $\\mu_{B}$ is the vector of input means, evaluated over the mini-batch\n",
    "- $\\sigma_B$ is the vector of input standard deviations, evaluated over the minibatch\n",
    "- $\\hat{x}^{(i)}$ is the vector of zero-centered and normalized inputs for instance i\n",
    "- $\\gamma$ is the scale parameter vector for the layer\n",
    "- $\\beta$ is the shift parameter vector for the layer\n",
    "- $\\epsilon$ is a tiny number (smoothing term) to avoids division by zero\n",
    "- $z^{(i)}$ is the output of the BN operation: a rescaled and shifted version of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets\n",
    "them. But what about at prediction time? We may need to make predictions for individual instances or over small batch of instances, so computing statistics would be unreliable. \n",
    "\n",
    "One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch ones when making predictions. \n",
    "\n",
    "Most implementations of BN (also Keras) estimate these \"final\" statistics during training by using a moving average of the layer’s input means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [Sergey Ioffe and Christian Szegedy **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift** Proceedings of the 32nd International Conference on Machine Learning (2015)](https://arxiv.org/abs/1502.03167) shows tha BN considerably improve DNN, in particular leading to a huge improvement in the [ImageNet](http://www.image-net.org/) classification task. \n",
    "\n",
    "BN, however, adds some complexity to the model and there is a runtime penalty: the NN makes slower predictions due to the extra computations required at each layer. Fortunately, it’s  possible to fuse the BN layer with the previous layer, after training, in order to avoid the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. The previous laryer computes: \n",
    "\n",
    "$\\begin{align}\n",
    "XW+b\n",
    "\\end{align}$\n",
    "\n",
    "the BN layers computes (ignoring the smoothing term): \n",
    "\n",
    "$\\begin{align}\n",
    "\\gamma \\otimes \\frac{(XW+b-\\mu)}{\\sigma} + \\beta\n",
    "\\end{align}$\n",
    "\n",
    "If we define:\n",
    "\n",
    "$\\begin{align}\n",
    "W' = \\gamma \\otimes \\frac{W}{\\sigma}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "b'= \\gamma \\otimes \\frac{(b-\\mu)}{\\sigma} +\\beta\n",
    "\\end{align}$\n",
    "\n",
    "the equation simplifies to\n",
    "\n",
    "$\\begin{align}\n",
    "XW'+b'\n",
    "\\end{align}$\n",
    "\n",
    "So, if we replace the previous layer’s weights and biases ($W$ and $b$) with the updated weights and biases ($W'$ and $b'$), we can get rid of the BN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, we can use BN just adding a **BatchNormalization** layer before or after each hidden layer activation function. As an example, the following model applies BN after every hidden layer and as the first layer (after flattening the input images): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tiny example with just two hidden layers, it’s unlikely that BN will have a positive impact, but for deeper networks it can make a tremendous difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each BN layer adds four parameters per input ($\\gamma$, $\\beta$, $\\mu$, and $\\sigma$), the first BN layer adds 3.136 parameters (which is 4x784). The last two parameters ($\\mu$, and $\\sigma$) are the moving averages and they are not affected by backpropagation, so Keras calls them **non-trainable** (3.136 + 1.200 + 400)/2=2.368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.8207 - accuracy: 0.7272 - val_loss: 0.5385 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5607 - accuracy: 0.8067 - val_loss: 0.4672 - val_accuracy: 0.8418\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5059 - accuracy: 0.8235 - val_loss: 0.4333 - val_accuracy: 0.8524\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8345 - val_loss: 0.4137 - val_accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4525 - accuracy: 0.8414 - val_loss: 0.4014 - val_accuracy: 0.8630\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4342 - accuracy: 0.8474 - val_loss: 0.3884 - val_accuracy: 0.8656\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4220 - accuracy: 0.8533 - val_loss: 0.3819 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4092 - accuracy: 0.8558 - val_loss: 0.3713 - val_accuracy: 0.8718\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3947 - accuracy: 0.8597 - val_loss: 0.3671 - val_accuracy: 0.8718\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3876 - accuracy: 0.8639 - val_loss: 0.3606 - val_accuracy: 0.8744\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better, there's a debate on this topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 1.0846 - accuracy: 0.6596 - val_loss: 0.6806 - val_accuracy: 0.7862\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6804 - accuracy: 0.7793 - val_loss: 0.5559 - val_accuracy: 0.8176\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5916 - accuracy: 0.8027 - val_loss: 0.5020 - val_accuracy: 0.8314\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5435 - accuracy: 0.8162 - val_loss: 0.4686 - val_accuracy: 0.8432\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5118 - accuracy: 0.8251 - val_loss: 0.4459 - val_accuracy: 0.8502\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4906 - accuracy: 0.8320 - val_loss: 0.4294 - val_accuracy: 0.8568\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4699 - accuracy: 0.8387 - val_loss: 0.4171 - val_accuracy: 0.8578\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4584 - accuracy: 0.8406 - val_loss: 0.4053 - val_accuracy: 0.8616\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4441 - accuracy: 0.8451 - val_loss: 0.3970 - val_accuracy: 0.8642\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4324 - accuracy: 0.8498 - val_loss: 0.3897 - val_accuracy: 0.8666\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN has become one of the most-used layers in DNN, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. However, a recent paper [Hongyi Zhang et al. **Fixup Initialization: Residual Learning Without Normalization** arXiv preprint arXiv:1901.09321 (2019)](https://arxiv.org/abs/1901.09321) may change this assumption: by using a novel weight initialization technique, the authors managed to train a very deep NN (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This technique, called **Gradient Clipping**, was presented in the paper [Razvan Pascanu et al. **On the Difficulty of Training Recurrent Neural Networks** Proceedings of the 30th International Conference on Machine Learning (2013)](https://arxiv.org/abs/1211.5063) and it is most often used in recurrent neural networks, for other types of networks, BN is usually sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, the use of Gradient Clipping is done by setting **clipvalue** or **clipnorm** arguments when creating an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. Note that this may change the orientation of the gradient vector. For instance, if the\n",
    "original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If we want to ensure that Gradient Clipping does not change the direction of the gradient vector, we can clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its norm is greater than the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "It is generally not a good idea to train a large DNN from scratch: instead, we can try to find an existing neural network that accomplishes a similar task to the one we are trying to tackle, then reuse the lower layers of this network. This technique is called **transfer learning**. It will not only speedcup training considerably, but also require significantly less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have access to a DNN that was trained to classify pictures into 100 different categories (e.g. animals, plants, vehicles, and everyday objects) and we want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so we can try to reuse parts of the first network.\n",
    "\n",
    "<img src=\"transfer-learning.png\" width=\"500\">\n",
    "\n",
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task. Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. We have  to find the right number of layers to reuse: the more similar the tasks are, the more layers we can reuse (starting with the lower layers). For very similar tasks, we can try to keep all the hidden layers and just replace the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example in order to show how Transfer Learning can be implemented in Keras. Suppose someone has built a model on a simpler Fashion MNIST dataset (one with only eight classes, all the classes except for sandal and shirt). Let’s call this model A. We now want to tackle a different task: we have images of sandals and shirts, and we want to train a binary classifier (positive=shirt, negative=sandal). That task is quite similar to the first one, so we can try transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "- X_train_A: all images of all items except for sandals and shirts (classes 5 and 6)\n",
    "- X_train_B: a much smaller training set of just the first 200 images of sandals or shirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 1s 973us/step - loss: 0.6087 - accuracy: 0.8035 - val_loss: 0.3900 - val_accuracy: 0.8625\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 1s 892us/step - loss: 0.3626 - accuracy: 0.8756 - val_loss: 0.3363 - val_accuracy: 0.8764\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 1s 885us/step - loss: 0.3232 - accuracy: 0.8887 - val_loss: 0.3071 - val_accuracy: 0.8916\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 1s 890us/step - loss: 0.3031 - accuracy: 0.8950 - val_loss: 0.2925 - val_accuracy: 0.8991\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 1s 886us/step - loss: 0.2890 - accuracy: 0.9002 - val_loss: 0.2872 - val_accuracy: 0.8986\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 1s 889us/step - loss: 0.2791 - accuracy: 0.9041 - val_loss: 0.2795 - val_accuracy: 0.9028\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 1s 884us/step - loss: 0.2707 - accuracy: 0.9078 - val_loss: 0.2681 - val_accuracy: 0.9091\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 1s 886us/step - loss: 0.2640 - accuracy: 0.9094 - val_loss: 0.2632 - val_accuracy: 0.9108\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 1s 887us/step - loss: 0.2585 - accuracy: 0.9118 - val_loss: 0.2592 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 1s 890us/step - loss: 0.2538 - accuracy: 0.9128 - val_loss: 0.2519 - val_accuracy: 0.9133\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 1s 886us/step - loss: 0.2492 - accuracy: 0.9153 - val_loss: 0.2579 - val_accuracy: 0.9131\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 1s 886us/step - loss: 0.2450 - accuracy: 0.9161 - val_loss: 0.2477 - val_accuracy: 0.9148\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 1s 904us/step - loss: 0.2419 - accuracy: 0.9170 - val_loss: 0.2473 - val_accuracy: 0.9145\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 1s 889us/step - loss: 0.2381 - accuracy: 0.9192 - val_loss: 0.2452 - val_accuracy: 0.9165\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 1s 887us/step - loss: 0.2351 - accuracy: 0.9190 - val_loss: 0.2417 - val_accuracy: 0.9143\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 1s 887us/step - loss: 0.2324 - accuracy: 0.9211 - val_loss: 0.2492 - val_accuracy: 0.9160\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 1s 889us/step - loss: 0.2294 - accuracy: 0.9217 - val_loss: 0.2435 - val_accuracy: 0.9138\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 1s 889us/step - loss: 0.2268 - accuracy: 0.9227 - val_loss: 0.2395 - val_accuracy: 0.9175\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 1s 897us/step - loss: 0.2244 - accuracy: 0.9235 - val_loss: 0.2377 - val_accuracy: 0.9170\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 1s 892us/step - loss: 0.2218 - accuracy: 0.9243 - val_loss: 0.2354 - val_accuracy: 0.9175\n"
     ]
    }
   ],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new model (model B) based on that model A layers, let’s reuse all the layers except for the output layer. We can train model B for task B, but since the new output layer was initialized randomly, it will make large errors, so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, we have to set every layer’s trainable attribute to false, compile the model and train it for a few epochs; then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.6857 - accuracy: 0.5950 - val_loss: 0.6873 - val_accuracy: 0.6024\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6125 - accuracy: 0.6700 - val_loss: 0.6227 - val_accuracy: 0.6744\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5522 - accuracy: 0.7200 - val_loss: 0.5673 - val_accuracy: 0.7323\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5007 - accuracy: 0.8050 - val_loss: 0.5203 - val_accuracy: 0.7698\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4000 - accuracy: 0.8800 - val_loss: 0.3507 - val_accuracy: 0.8955\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.9500 - val_loss: 0.2683 - val_accuracy: 0.9452\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2175 - accuracy: 0.9600 - val_loss: 0.2175 - val_accuracy: 0.9574\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1778 - accuracy: 0.9800 - val_loss: 0.1847 - val_accuracy: 0.9675\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1508 - accuracy: 0.9850 - val_loss: 0.1605 - val_accuracy: 0.9726\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9900 - val_loss: 0.1422 - val_accuracy: 0.9797\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9950 - val_loss: 0.1282 - val_accuracy: 0.9807\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9950 - val_loss: 0.1173 - val_accuracy: 0.9828\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9950 - val_loss: 0.1085 - val_accuracy: 0.9828\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9950 - val_loss: 0.1009 - val_accuracy: 0.9828\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9950 - val_loss: 0.0942 - val_accuracy: 0.9858\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9950 - val_loss: 0.0889 - val_accuracy: 0.9868\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9950 - val_loss: 0.0843 - val_accuracy: 0.9878\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0666 - accuracy: 0.9950 - val_loss: 0.0802 - val_accuracy: 0.9888\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9950 - val_loss: 0.0767 - val_accuracy: 0.9888\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0599 - accuracy: 0.9950 - val_loss: 0.0733 - val_accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=4,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=16,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what’s the final verdict? We can compare with a network learned from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.9043 - accuracy: 0.4700 - val_loss: 0.7915 - val_accuracy: 0.5213\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.7158 - accuracy: 0.6150 - val_loss: 0.6530 - val_accuracy: 0.6268\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5870 - accuracy: 0.6850 - val_loss: 0.5427 - val_accuracy: 0.7170\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.7650 - val_loss: 0.4640 - val_accuracy: 0.7982\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4124 - accuracy: 0.8200 - val_loss: 0.4019 - val_accuracy: 0.8550\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.8850 - val_loss: 0.3522 - val_accuracy: 0.9026\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3055 - accuracy: 0.9250 - val_loss: 0.3134 - val_accuracy: 0.9300\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.9550 - val_loss: 0.2839 - val_accuracy: 0.9412\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9650 - val_loss: 0.2595 - val_accuracy: 0.9442\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2167 - accuracy: 0.9750 - val_loss: 0.2382 - val_accuracy: 0.9533\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9750 - val_loss: 0.2203 - val_accuracy: 0.9594\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1789 - accuracy: 0.9750 - val_loss: 0.2067 - val_accuracy: 0.9625\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1657 - accuracy: 0.9750 - val_loss: 0.1938 - val_accuracy: 0.9655\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1533 - accuracy: 0.9750 - val_loss: 0.1826 - val_accuracy: 0.9675\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9850 - val_loss: 0.1728 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9900 - val_loss: 0.1638 - val_accuracy: 0.9675\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9900 - val_loss: 0.1563 - val_accuracy: 0.9686\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9900 - val_loss: 0.1494 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9900 - val_loss: 0.1435 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1042 - accuracy: 0.9900 - val_loss: 0.1381 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses.\n",
    "\n",
    "It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimization\n",
    "\n",
    "Training large DNNs can be painfully slow. A huge speed boost comes from using a\n",
    "faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity. In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
    "\n",
    "Recall that Gradient Descent updates the weights by subtracting the gradient of the cost function with regard to the weight multiplied by the learning rate:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\nabla J(\\theta)$\n",
    "\n",
    "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "\n",
    "Momentum optimization ([Boris T. Polyak **Some Methods of Speeding Up the Convergence of Iteration Methods** USSR Computational Mathematics and Mathematical Physics 4, no. 5 (1964)](https://vsokolov.org/courses/750/files/polyak64.pdf)) cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector $m$ and it updates the weights by adding this momentum:\n",
    "\n",
    "$1.\\quad m \\longleftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$2.\\quad \\theta \\longleftarrow \\theta + m$\n",
    "\n",
    "In other words, the gradient is **used for acceleration, not for speed**. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, called the **momentum**, which must be set between 0 (high friction) and 1 (no friction). A typicalvmomentum value is 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use momentum optimization in Keras, we have to use the SGD optimizer and set its **momentum hyperparameter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "The **Nesterov Accelerated Gradient (NAG**) method ([Yurii Nesterov **A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence** Doklady AN USSR 269 (1983)](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)), is a small variant to momentum optimization which measures the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum, at:\n",
    "\n",
    "$1.\\quad m \\longleftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta + \\beta m)$\n",
    "\n",
    "$2.\\quad \\theta \\longleftarrow \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position:  \n",
    "\n",
    "<img src=\"nesterov.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, simply set **nesterov=True** when creating the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The **AdaGrad algorithm** ([John Duchi et al. **Adaptive Subgradient Methods for Online Learning and Stochastic Optimization** Journal of Machine Learning Research 12 (2011)](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)) achieves this correction by scaling down the gradient vector along the\n",
    "steepest dimensions:\n",
    "\n",
    "$1.\\quad s \\longleftarrow s + \\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$2.\\quad  \\theta \\longleftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{s + \\epsilon}$\n",
    "\n",
    "The first step accumulates the square of the gradients into the vector $s$, the second step is almost identical to Gradient Descent, but the gradient vector is scaled down by a factor of $\\sqrt{s + \\epsilon}$ ($\\epsilon$ is a smoothing term to avoid division by zero. \n",
    "\n",
    "The vectorized form is equivalent to simultaneously computing:\n",
    "\n",
    "$\\begin{align}\n",
    "s_i \\longleftarrow s_i + (\\frac{\\partial J(\\theta}{\\partial \\theta_i})^2\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta_i \\longleftarrow \\theta_i - \\frac{\\eta}{\\sqrt{s_i + \\epsilon}} \\frac{\\partial J(\\theta)}{\\partial \\theta_i}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an **adaptive learning rate**. It helps point the resulting updates more directly toward the global optimum.\n",
    "\n",
    "<img src=\"adagrad.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. You  should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers.\n",
    "\n",
    "Keras has an Adagrad optimizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "The **RMSProp algorithm** ([Geoffrey Hinton, Coursera class on neural networks] (https://homl.info/57)) fixes the AdaGrad risk of slowing down a bit too fast by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step:\n",
    "\n",
    "\n",
    "$1.\\quad s \\longleftarrow \\beta s + (1-\\beta)\\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$2.\\quad \\theta \\longleftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\oslash \\sqrt{s + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decay rate $β$ is typically set to 0.9, and it is a new hyperparameter, but this default value often works well, so we may not need to tune it at all. Except on very simple problems, this optimizer almost always performs much better than AdaGrad. \n",
    "\n",
    "Keras has an RMSprop optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "The Adaptive Moment Estimation **Adam algorithm** ([Diederik P. Kingma and Jimmy Ba. **Adam: A Method for Stochastic Optimization** arXiv preprint arXiv:1412.6980 (2014)](https://arxiv.org/abs/1412.6980)) combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "$1.\\quad m \\longleftarrow \\beta_1 m - (1-\\beta_1) \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$2.\\quad  s \\longleftarrow \\beta_2 s + (1-\\beta_2)\\nabla_{\\theta} J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$\\begin{align}\n",
    "3.\\quad \n",
    "\\hat{m} \\longleftarrow \\frac{m}{1-\\beta_1^T} \\quad T:\\text{interation number}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "4.\\quad \n",
    "\\hat{s} \\longleftarrow \\frac{s}{1-\\beta_2^T}\n",
    "\\end{align}$\n",
    "\n",
    "$5.\\quad  \\theta \\longleftarrow \\theta + \\eta \\hat{m} \\oslash \\sqrt{\\hat{s} + \\epsilon}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has an Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mention a variants of Adam, called **Nadam algorithm** ([Timothy Dozat **Incorporating Nesterov Momentum into Adam** (2016)](http://cs229.stanford.edu/proj2015/054_report.pdf)) which include the the Nesterov trick in Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher orders derivatives\n",
    "\n",
    "All the optimization techniques discussed so far only rely on the first-order partial derivatives (**Jacobians**). The optimization literature also contains amazing algorithms based on the second-order partial derivatives (the **Hessians**, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are $n^2$ Hessians per output (where $n$ is the number of parameters), as opposed to just $n$ Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "All these adaptive optimization methods  are often great, converging fast to a good solution. However, this paper [Ashia C. Wilson et al. **The Marginal Value of Adaptive Gradient Methods in Machine Learning** Advances in Neural Information Processing Systems 30 (2017): 4148–4158.](https://arxiv.org/abs/1705.08292) showed that they can lead to solutions that generalize poorly on some datasets. \n",
    "\n",
    "So when we are disappointed by our model’s performance, we can try using plain Nesterov Accelerated Gradient instead: our  dataset may just be allergic to adaptive\n",
    "gradients. Also check out the latest research, because it’s moving fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "Finding a good learning rate is very important: \n",
    "- if we set it much too high, training may diverge; \n",
    "- if we set it too low, training will eventually converge to the optimum, but it will take a very long time; \n",
    "- if we set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down\n",
    "\n",
    "We can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. we can then\n",
    "reinitialize our model and train it with that learning rate.\n",
    "\n",
    "But we can do better than a constant learning rate: if we start with a large learning rate and then reduce it once training stops making fast progress, we can reach a good solution faster than with the optimal constant learning rate. \n",
    "\n",
    "<img src=\"learning-rate.png\" width=\"500\">\n",
    "\n",
    "There are many different strategies, called **learning schedule** to reduce the learning rate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power scheduling\n",
    "\n",
    "Set the learning rate to a function of the iteration number $t$: \n",
    "\n",
    "$\\begin{align}\n",
    "\\eta(t) = \\frac{\\eta_0}{(1+t/s)^c}\n",
    "\\end{align}$\n",
    "\n",
    "The initial learning rate $\\eta_0$ , the power $c$ (typically set to 1), and the steps $s$ are hyperparameters. The learning rate drops at each step. After $s$ steps, it is down to $\\eta_0/2$. After $s$ more steps, it is down to $\\eta_0/3$,\n",
    "then it goes down to $\\eta_0/4$, then $\\eta_0/5$, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning $\\eta$ and $s$ and $c$.\n",
    "\n",
    "To implement power scheduling in Keras, you have to set the **decay hyperparameter** when creating an optimizer (the decay is the inverse of $s$ and Keras assumes $c=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 2s 957us/step - loss: 0.6873 - accuracy: 0.7570 - val_loss: 0.5159 - val_accuracy: 0.8256\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4948 - accuracy: 0.8244 - val_loss: 0.4653 - val_accuracy: 0.8414\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 930us/step - loss: 0.4543 - accuracy: 0.8396 - val_loss: 0.4412 - val_accuracy: 0.8498\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 899us/step - loss: 0.4312 - accuracy: 0.8483 - val_loss: 0.4240 - val_accuracy: 0.8570\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 894us/step - loss: 0.4153 - accuracy: 0.8540 - val_loss: 0.4146 - val_accuracy: 0.8620\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 902us/step - loss: 0.4025 - accuracy: 0.8592 - val_loss: 0.4054 - val_accuracy: 0.8626\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 1s 856us/step - loss: 0.3927 - accuracy: 0.8625 - val_loss: 0.3985 - val_accuracy: 0.8646\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 1s 853us/step - loss: 0.3840 - accuracy: 0.8657 - val_loss: 0.3936 - val_accuracy: 0.8666\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 1s 868us/step - loss: 0.3768 - accuracy: 0.8675 - val_loss: 0.3894 - val_accuracy: 0.8666\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 873us/step - loss: 0.3700 - accuracy: 0.8705 - val_loss: 0.3848 - val_accuracy: 0.8694\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 902us/step - loss: 0.3645 - accuracy: 0.8722 - val_loss: 0.3812 - val_accuracy: 0.8682\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 882us/step - loss: 0.3591 - accuracy: 0.8740 - val_loss: 0.3796 - val_accuracy: 0.8688\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 0.3544 - accuracy: 0.8760 - val_loss: 0.3763 - val_accuracy: 0.8694\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 949us/step - loss: 0.3498 - accuracy: 0.8777 - val_loss: 0.3727 - val_accuracy: 0.8696\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.3457 - accuracy: 0.8788 - val_loss: 0.3699 - val_accuracy: 0.8710\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 929us/step - loss: 0.3418 - accuracy: 0.8806 - val_loss: 0.3687 - val_accuracy: 0.8726\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 916us/step - loss: 0.3381 - accuracy: 0.8819 - val_loss: 0.3665 - val_accuracy: 0.8726\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3347 - accuracy: 0.8831 - val_loss: 0.3639 - val_accuracy: 0.8742\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.3314 - accuracy: 0.8842 - val_loss: 0.3613 - val_accuracy: 0.8754\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.3280 - accuracy: 0.8855 - val_loss: 0.3627 - val_accuracy: 0.8732\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 889us/step - loss: 0.3254 - accuracy: 0.8860 - val_loss: 0.3590 - val_accuracy: 0.8752\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3224 - accuracy: 0.8876 - val_loss: 0.3583 - val_accuracy: 0.8752\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 929us/step - loss: 0.3196 - accuracy: 0.8889 - val_loss: 0.3570 - val_accuracy: 0.8768\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 898us/step - loss: 0.3171 - accuracy: 0.8893 - val_loss: 0.3553 - val_accuracy: 0.8752\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3145 - accuracy: 0.8901 - val_loss: 0.3550 - val_accuracy: 0.8756\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential scheduling\n",
    "\n",
    "Set the learning rate to \n",
    "\n",
    "$\\eta(t) = \\eta_0 0.1^{t/s}$ \n",
    "\n",
    "The learning rate will gradually drop by a factor of 10 every $s$ steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every $s$ steps.\n",
    "\n",
    "To implement exponential scheduling in Keras, you need to define a function that takes the current epoch and returns the learning rate, then create a **LearningRateScheduler callback**, giving it the schedule function, and pass this callback to the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.8541 - accuracy: 0.7548 - val_loss: 0.7697 - val_accuracy: 0.7716\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.7129 - accuracy: 0.7873 - val_loss: 1.1835 - val_accuracy: 0.6618\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6578 - accuracy: 0.7993 - val_loss: 0.5880 - val_accuracy: 0.8334\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.5435 - accuracy: 0.8340 - val_loss: 0.5926 - val_accuracy: 0.8274\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4891 - accuracy: 0.8438 - val_loss: 0.5841 - val_accuracy: 0.8170\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4812 - accuracy: 0.8494 - val_loss: 0.4889 - val_accuracy: 0.8560\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4182 - accuracy: 0.8646 - val_loss: 0.4837 - val_accuracy: 0.8654\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3840 - accuracy: 0.8745 - val_loss: 0.4728 - val_accuracy: 0.8658\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3626 - accuracy: 0.8822 - val_loss: 0.4836 - val_accuracy: 0.8742\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3273 - accuracy: 0.8905 - val_loss: 0.4337 - val_accuracy: 0.8738\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3142 - accuracy: 0.8949 - val_loss: 0.4525 - val_accuracy: 0.8638\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2893 - accuracy: 0.9040 - val_loss: 0.4710 - val_accuracy: 0.8772\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2665 - accuracy: 0.9092 - val_loss: 0.4323 - val_accuracy: 0.8816\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2502 - accuracy: 0.9145 - val_loss: 0.4570 - val_accuracy: 0.8834\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2330 - accuracy: 0.9203 - val_loss: 0.4377 - val_accuracy: 0.8824\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2209 - accuracy: 0.9265 - val_loss: 0.4666 - val_accuracy: 0.8858\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2052 - accuracy: 0.9294 - val_loss: 0.4199 - val_accuracy: 0.8868\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1923 - accuracy: 0.9343 - val_loss: 0.4927 - val_accuracy: 0.8866\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1763 - accuracy: 0.9404 - val_loss: 0.5069 - val_accuracy: 0.8878\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1661 - accuracy: 0.9434 - val_loss: 0.5272 - val_accuracy: 0.8862\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1570 - accuracy: 0.9467 - val_loss: 0.5088 - val_accuracy: 0.8920\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1469 - accuracy: 0.9511 - val_loss: 0.5229 - val_accuracy: 0.8892\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1372 - accuracy: 0.9542 - val_loss: 0.5335 - val_accuracy: 0.8886\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1281 - accuracy: 0.9579 - val_loss: 0.5765 - val_accuracy: 0.8856\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1236 - accuracy: 0.9596 - val_loss: 0.5630 - val_accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise constant scheduling\n",
    "\n",
    "Use a constant learning rate for a number of epochs (e.g., $\\eta=0.1$ for 5 epochs), then a smaller learning rate for another number of epochs (e.g., $\\eta=0.001$ for 50 epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them.\n",
    "\n",
    "For using Piecewise constant scheduling with Keras, we can do similar to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8676 - accuracy: 0.7496 - val_loss: 0.8298 - val_accuracy: 0.7782\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.8535 - accuracy: 0.7474 - val_loss: 1.0883 - val_accuracy: 0.6594\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 1.0013 - accuracy: 0.6868 - val_loss: 1.0355 - val_accuracy: 0.7278\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.9643 - accuracy: 0.7057 - val_loss: 1.0247 - val_accuracy: 0.6698\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 1.0945 - accuracy: 0.6435 - val_loss: 1.1318 - val_accuracy: 0.5930\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.9385 - accuracy: 0.6257 - val_loss: 0.8873 - val_accuracy: 0.6412\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.8517 - accuracy: 0.6602 - val_loss: 0.9160 - val_accuracy: 0.6496\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8295 - accuracy: 0.6679 - val_loss: 0.8091 - val_accuracy: 0.7030\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7049 - accuracy: 0.7602 - val_loss: 0.6572 - val_accuracy: 0.8192\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5871 - accuracy: 0.8232 - val_loss: 0.7158 - val_accuracy: 0.8380\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5744 - accuracy: 0.8345 - val_loss: 0.6101 - val_accuracy: 0.8282\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5300 - accuracy: 0.8432 - val_loss: 0.5889 - val_accuracy: 0.8372\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.5334 - accuracy: 0.8434 - val_loss: 0.6056 - val_accuracy: 0.8510\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.5054 - accuracy: 0.8494 - val_loss: 0.7671 - val_accuracy: 0.8168\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5136 - accuracy: 0.8498 - val_loss: 0.6985 - val_accuracy: 0.8350\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3663 - accuracy: 0.8809 - val_loss: 0.5467 - val_accuracy: 0.8632\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3373 - accuracy: 0.8889 - val_loss: 0.5355 - val_accuracy: 0.8700\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3242 - accuracy: 0.8919 - val_loss: 0.6232 - val_accuracy: 0.8584\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3148 - accuracy: 0.8945 - val_loss: 0.5708 - val_accuracy: 0.8664\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3024 - accuracy: 0.8973 - val_loss: 0.5706 - val_accuracy: 0.8732\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2910 - accuracy: 0.9017 - val_loss: 0.5730 - val_accuracy: 0.8704\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2857 - accuracy: 0.9039 - val_loss: 0.5890 - val_accuracy: 0.8648\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2755 - accuracy: 0.9072 - val_loss: 0.5462 - val_accuracy: 0.8692\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2689 - accuracy: 0.9094 - val_loss: 0.5600 - val_accuracy: 0.8692\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2632 - accuracy: 0.9124 - val_loss: 0.5515 - val_accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance scheduling\n",
    "\n",
    "Measure the validation error every $N$ steps (just like for early stopping), and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.\n",
    "\n",
    "To use Performance Scheduling in Keras, we can use the **ReduceLROnPlateau callback**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5920 - accuracy: 0.8073 - val_loss: 0.5493 - val_accuracy: 0.8152\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5100 - accuracy: 0.8365 - val_loss: 0.4757 - val_accuracy: 0.8534\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5033 - accuracy: 0.8439 - val_loss: 0.5757 - val_accuracy: 0.8384\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 955us/step - loss: 0.5065 - accuracy: 0.8465 - val_loss: 0.4976 - val_accuracy: 0.8404\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 923us/step - loss: 0.5214 - accuracy: 0.8486 - val_loss: 0.7465 - val_accuracy: 0.8086\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5009 - accuracy: 0.8541 - val_loss: 0.6929 - val_accuracy: 0.8278\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 2s 920us/step - loss: 0.5439 - accuracy: 0.8510 - val_loss: 0.6673 - val_accuracy: 0.8404\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.3019 - accuracy: 0.8941 - val_loss: 0.3960 - val_accuracy: 0.8800\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.2574 - accuracy: 0.9069 - val_loss: 0.3981 - val_accuracy: 0.8820\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.2300 - accuracy: 0.9145 - val_loss: 0.4517 - val_accuracy: 0.8816\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.2137 - accuracy: 0.9211 - val_loss: 0.4204 - val_accuracy: 0.8774\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.2048 - accuracy: 0.9245 - val_loss: 0.3887 - val_accuracy: 0.8874\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 997us/step - loss: 0.1925 - accuracy: 0.9284 - val_loss: 0.4410 - val_accuracy: 0.8862\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 957us/step - loss: 0.1826 - accuracy: 0.9323 - val_loss: 0.4281 - val_accuracy: 0.8872\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 916us/step - loss: 0.1731 - accuracy: 0.9354 - val_loss: 0.4172 - val_accuracy: 0.8896\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.1685 - accuracy: 0.9378 - val_loss: 0.4180 - val_accuracy: 0.8904\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 0.1539 - accuracy: 0.9417 - val_loss: 0.4658 - val_accuracy: 0.8884\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.1029 - accuracy: 0.9595 - val_loss: 0.4636 - val_accuracy: 0.8910\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 933us/step - loss: 0.0875 - accuracy: 0.9663 - val_loss: 0.4644 - val_accuracy: 0.8852\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0812 - accuracy: 0.9685 - val_loss: 0.4763 - val_accuracy: 0.8962\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 918us/step - loss: 0.0742 - accuracy: 0.9718 - val_loss: 0.4928 - val_accuracy: 0.8902\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.0705 - accuracy: 0.9731 - val_loss: 0.5056 - val_accuracy: 0.8994\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 993us/step - loss: 0.0518 - accuracy: 0.9813 - val_loss: 0.5208 - val_accuracy: 0.8992\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0461 - accuracy: 0.9834 - val_loss: 0.5200 - val_accuracy: 0.8992\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0438 - accuracy: 0.9843 - val_loss: 0.5475 - val_accuracy: 0.8968\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison \n",
    "\n",
    "The paper [Andrew Senior et al. **An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition** Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013): 6724–6728.](https://ieeexplore.ieee.org/document/6638963) compared the performance of some of the most popular learning schedules when using momentum optimization to train deep neural networks for speech recognition. The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well. They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution (they also mentioned that it was easier to implement than performance scheduling, but in Keras both options are easy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "DNNs typically have tens of thousands of parameters (even millions). This gives them an incredible amount of freedom and means they can fit a huge variety of complex datasets. But this great flexibility also makes the network **prone to overfitting** the training set. We need regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. \n",
    "\n",
    "In Keras we can use the **EarlyStopping callback**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1719/1719 [==============================] - 2s 887us/step - loss: 27.6178 - val_loss: 27.5936\n",
      "Epoch 2/250\n",
      "1719/1719 [==============================] - 1s 813us/step - loss: 27.6156 - val_loss: 27.5925\n",
      "Epoch 3/250\n",
      "1719/1719 [==============================] - 1s 817us/step - loss: 27.6149 - val_loss: 27.5920\n",
      "Epoch 4/250\n",
      "1719/1719 [==============================] - 1s 815us/step - loss: 27.6144 - val_loss: 27.5916\n",
      "Epoch 5/250\n",
      "1719/1719 [==============================] - 1s 865us/step - loss: 27.6141 - val_loss: 27.5913\n",
      "Epoch 6/250\n",
      "1719/1719 [==============================] - 1s 844us/step - loss: 27.6139 - val_loss: 27.5911\n",
      "Epoch 7/250\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 27.6137 - val_loss: 27.5910\n",
      "Epoch 8/250\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 27.6136 - val_loss: 27.5909\n",
      "Epoch 9/250\n",
      "1719/1719 [==============================] - 2s 883us/step - loss: 27.6134 - val_loss: 27.5907\n",
      "Epoch 10/250\n",
      "1719/1719 [==============================] - 2s 964us/step - loss: 27.6133 - val_loss: 27.5907\n",
      "Epoch 11/250\n",
      "1719/1719 [==============================] - 1s 860us/step - loss: 27.6132 - val_loss: 27.5906\n",
      "Epoch 12/250\n",
      "1719/1719 [==============================] - 1s 828us/step - loss: 27.6131 - val_loss: 27.5905\n",
      "Epoch 13/250\n",
      "1719/1719 [==============================] - 1s 814us/step - loss: 27.6131 - val_loss: 27.5904\n",
      "Epoch 14/250\n",
      "1719/1719 [==============================] - 1s 823us/step - loss: 27.6130 - val_loss: 27.5904\n",
      "Epoch 15/250\n",
      "1719/1719 [==============================] - 1s 837us/step - loss: 27.6130 - val_loss: 27.5903\n",
      "Epoch 16/250\n",
      "1719/1719 [==============================] - 1s 809us/step - loss: 27.6129 - val_loss: 27.5903\n",
      "Epoch 17/250\n",
      "1719/1719 [==============================] - 1s 833us/step - loss: 27.6129 - val_loss: 27.5902\n",
      "Epoch 18/250\n",
      "1719/1719 [==============================] - 2s 880us/step - loss: 27.6129 - val_loss: 27.5902\n",
      "Epoch 19/250\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 27.6128 - val_loss: 27.5902\n",
      "Epoch 20/250\n",
      "1719/1719 [==============================] - 1s 824us/step - loss: 27.6129 - val_loss: 27.5902\n",
      "Epoch 21/250\n",
      "1719/1719 [==============================] - 1s 812us/step - loss: 27.6128 - val_loss: 27.5901\n",
      "Epoch 22/250\n",
      "1719/1719 [==============================] - 1s 828us/step - loss: 27.6128 - val_loss: 27.5901\n",
      "Epoch 23/250\n",
      "1719/1719 [==============================] - 1s 816us/step - loss: 27.6128 - val_loss: 27.5901\n",
      "Epoch 24/250\n",
      "1719/1719 [==============================] - 1s 812us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 25/250\n",
      "1719/1719 [==============================] - 2s 900us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 26/250\n",
      "1719/1719 [==============================] - 2s 901us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 27/250\n",
      "1719/1719 [==============================] - 2s 937us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 28/250\n",
      "1719/1719 [==============================] - 1s 833us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 29/250\n",
      "1719/1719 [==============================] - 1s 799us/step - loss: 27.6127 - val_loss: 27.5900\n",
      "Epoch 30/250\n",
      "1719/1719 [==============================] - 1s 808us/step - loss: 27.6126 - val_loss: 27.5899\n",
      "Epoch 31/250\n",
      "1719/1719 [==============================] - 1s 814us/step - loss: 27.6126 - val_loss: 27.5899\n",
      "Epoch 32/250\n",
      "1719/1719 [==============================] - 1s 809us/step - loss: 27.6126 - val_loss: 27.5899\n",
      "Epoch 33/250\n",
      "1719/1719 [==============================] - 1s 831us/step - loss: 27.6126 - val_loss: 27.5899\n",
      "Epoch 34/250\n",
      "1719/1719 [==============================] - 1s 811us/step - loss: 27.6126 - val_loss: 27.5899\n",
      "Epoch 35/250\n",
      "1719/1719 [==============================] - 1s 825us/step - loss: 27.6125 - val_loss: 27.5899\n",
      "Epoch 36/250\n",
      "1719/1719 [==============================] - 1s 806us/step - loss: 27.6125 - val_loss: 27.5899\n",
      "Epoch 37/250\n",
      "1719/1719 [==============================] - 1s 804us/step - loss: 27.6125 - val_loss: 27.5898\n",
      "Epoch 38/250\n",
      "1719/1719 [==============================] - 1s 817us/step - loss: 27.6125 - val_loss: 27.5898\n",
      "Epoch 39/250\n",
      "1719/1719 [==============================] - 1s 804us/step - loss: 27.6125 - val_loss: 27.5898\n",
      "Epoch 40/250\n",
      "1719/1719 [==============================] - 1s 802us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 41/250\n",
      "1719/1719 [==============================] - 1s 811us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 42/250\n",
      "1719/1719 [==============================] - 1s 819us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 43/250\n",
      "1719/1719 [==============================] - 1s 816us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 44/250\n",
      "1719/1719 [==============================] - 1s 818us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 45/250\n",
      "1719/1719 [==============================] - 1s 822us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 46/250\n",
      "1719/1719 [==============================] - 1s 818us/step - loss: 27.6124 - val_loss: 27.5898\n",
      "Epoch 47/250\n",
      "1719/1719 [==============================] - 1s 840us/step - loss: 27.6124 - val_loss: 27.5897\n",
      "Epoch 48/250\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 27.6123 - val_loss: 27.5897\n",
      "Epoch 49/250\n",
      "1719/1719 [==============================] - 1s 853us/step - loss: 27.6124 - val_loss: 27.5897\n",
      "Epoch 50/250\n",
      "1719/1719 [==============================] - 1s 831us/step - loss: 27.6123 - val_loss: 27.5897\n",
      "Epoch 51/250\n",
      "1719/1719 [==============================] - 1s 823us/step - loss: 27.6123 - val_loss: 27.5897\n",
      "Epoch 52/250\n",
      "1719/1719 [==============================] - 1s 843us/step - loss: 27.6123 - val_loss: 27.5897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(keras.optimizers.SGD(), loss='mse')\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[callback])\n",
    "\n",
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 regularization\n",
    "\n",
    "Just like for simple linear models, you can use $\\ell_1$ regularization to constrain a neural network’s connection weights, and/or $\\ell_2$ regularization if we want a sparse model (with many weights equal to 0). In these methods a regularization term is added to the cost function to forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. \n",
    "\n",
    "In Keras we can specify the regularization in that way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **l2()** function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. We can use **keras.regularizers.l1()** if we want $\\ell_1$ regularization, if we want both regularization, we can use **keras.regularizers.l1_l2()** function.\n",
    "\n",
    "Since typically we want to apply the same regularizer to all layers in the network, as well as using the same activation function and the same initialization strategy in all hidden layers, we may find ourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, we can use Python’s **functools.partial()** function, which lets we create a thin wrapper for any callable, with some default argument values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.5936 - accuracy: 0.8121 - val_loss: 0.7835 - val_accuracy: 0.8058\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.7172 - accuracy: 0.8270 - val_loss: 0.6900 - val_accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "            keras.layers.Flatten(input_shape=[28, 28]),\n",
    "            RegularizedDense(300),\n",
    "            RegularizedDense(100),\n",
    "            RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout [Nitish Srivastava et al. **Dropout: A Simple Way to Prevent Neural Networks from Overfitting** Journal of Machine Learning Research (2014)](https://dl.acm.org/doi/10.5555/2627435.2670313) is one of the most popular regularization techniques for deep neural networks.  It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $p$ of being temporarily “dropped out”, meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter $p$ is called the **dropout rate** and it is typically set between 10% and 50%. After training, neurons don’t get dropped anymore.\n",
    "\n",
    "<img src=\"dropout.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons trained with dropout cannot coadapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, we get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there are a total of $2^N$ possible networks (where N is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller\n",
    "neural networks.\n",
    "\n",
    "There is one small but important technical detail. Suppose $p=50%$, in which case a durign test neuron would be connected to twice as many input neurons as it would be (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability $(1 – p)$ after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
    "\n",
    "Since dropout is only active during training, comparing the training loss and the\n",
    "validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).\n",
    "\n",
    "To implement dropout using Keras, you can use the **keras.layers.Dropout** layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5754 - accuracy: 0.8031 - val_loss: 0.3748 - val_accuracy: 0.8654\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4227 - accuracy: 0.8452 - val_loss: 0.3450 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe that the model is overfitting, we can increase the dropout rate. Conversely, we should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so we may want to try this if full dropout is too strong. Dropout does tend to significantly slow down convergence, but it usually\n",
    "results in a much better model when tuned properly. So, it is generally well worth the extra time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to regularize a self-normalizing network based on the SELU activation\n",
    "function, we should use **alpha dropout**, a variant of dropout that preserves the mean and standard deviation of its inputs (as regular dropout would break self-normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6646 - accuracy: 0.7583 - val_loss: 0.5628 - val_accuracy: 0.8476\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5583 - accuracy: 0.7928 - val_loss: 0.5272 - val_accuracy: 0.8488\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5259 - accuracy: 0.8043 - val_loss: 0.5402 - val_accuracy: 0.8468\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5068 - accuracy: 0.8124 - val_loss: 0.5033 - val_accuracy: 0.8480\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4935 - accuracy: 0.8161 - val_loss: 0.4873 - val_accuracy: 0.8498\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4798 - accuracy: 0.8230 - val_loss: 0.4815 - val_accuracy: 0.8594\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4692 - accuracy: 0.8274 - val_loss: 0.4325 - val_accuracy: 0.8646\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4679 - accuracy: 0.8271 - val_loss: 0.4266 - val_accuracy: 0.8694\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4570 - accuracy: 0.8298 - val_loss: 0.4139 - val_accuracy: 0.8746\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4584 - accuracy: 0.8309 - val_loss: 0.4198 - val_accuracy: 0.8724\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4523 - accuracy: 0.8322 - val_loss: 0.4384 - val_accuracy: 0.8752\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4438 - accuracy: 0.8357 - val_loss: 0.4875 - val_accuracy: 0.8590\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4374 - accuracy: 0.8369 - val_loss: 0.4203 - val_accuracy: 0.8740\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4365 - accuracy: 0.8379 - val_loss: 0.4564 - val_accuracy: 0.8766\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4344 - accuracy: 0.8385 - val_loss: 0.4193 - val_accuracy: 0.8762\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4293 - accuracy: 0.8401 - val_loss: 0.4112 - val_accuracy: 0.8816\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4248 - accuracy: 0.8418 - val_loss: 0.4010 - val_accuracy: 0.8792\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4261 - accuracy: 0.8423 - val_loss: 0.4023 - val_accuracy: 0.8780\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4179 - accuracy: 0.8439 - val_loss: 0.4284 - val_accuracy: 0.8740\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4218 - accuracy: 0.8420 - val_loss: 0.5023 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Dropout\n",
    "\n",
    "Monte Carlo Dropout ([Yarin Gal and Zoubin Ghahramani **Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning** Proceedings of the 33rd International Conference on Machine Learning (2016)](https://arxiv.org/abs/1506.02142)) is a clever realization that the use of the regular dropout can be interpreted as a Bayesian approximation of a well known probabilistic model: the Gaussian process. \n",
    "\n",
    "We can treat the many different networks (with different neurons dropped out) as Monte Carlo samples from the space of all available models. This provides mathematical grounds to reason about the model’s uncertainty and, as it turns out, often improves its performance.\n",
    "\n",
    "How does it work? We simply apply dropout at test time, that's all! Then, instead of one prediction, we get many, one by each model. We can then average them or analyze their distributions. And the best part: it does not require any changes in the model’s architecture. We can even use this trick on a model that has already been trained! To see it working in practice, let’s train a simple network to recognize digits from the MNIST dataset.\n",
    "\n",
    "We can try to boost the dropout model we trained earlier without retraining it. We just make 100 predictions over the test set, setting training=True to ensure that the Dropout layer is active, and stack the predictions. Since dropout is active, all the predictions will be different. Then we average and we get a prediction. Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout\n",
    "off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let’s look at the model’s prediction for the first instance in\n",
    "the test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the predictions made when dropout is activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.25, 0.  , 0.58]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still thinks this image belongs to class 9, but only with a 58% confidence, which seems much more reasonable than 98%. Plus it’s useful to know exactly which other classes it thinks are likely. And you can also take a look at the standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.2 , 0.01, 0.28]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there’s quite a lot of variance in the probability estimates: if we were building a risk-sensitive system (e.g., a medical or financial system), we should probably treat such an uncertain prediction with extreme caution.\n",
    "\n",
    "Moreover, the model’s accuracy got a small boost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples we use (100 in this example) is a hyperparameter we can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates will be. However, if we double it, inference time will also be doubled. Moreover, above a certain number of samples, we will notice little improvement. So our job is to find the right trade-off between latency and accuracy, depending on our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxNorm Regularization\n",
    "\n",
    "Max-norm regularization* constrains the incoming connection weights $w$ of each neuron $\\left\\|w\\right\\|_2 < r$, where $r$ is the **maxnorm hyperparameter**. It does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing $\\left\\|w\\right\\|_2$ after each training step and rescaling w if needed.\n",
    "\n",
    "Reducing $r$ increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems (if you are not using Batch Normalization).\n",
    "\n",
    "In Keras, we can set the **kernel_constraint argument** of each hidden layer to a **max_norm()** constraint with the appropriate max value, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fe7308b9b20>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines\n",
    "\n",
    "We have covered a wide range of techniques, and you may be wondering which ones you should use. This depends on the task, and there is no clear consensus yet, but the configuration in the following table work fine in most cases, without requiring much hyperparameter tuning:\n",
    "\n",
    "<img src=\"guidelines.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t forget to normalize the input features! \n",
    "\n",
    "We should also try to reuse parts of a pretrained neural network if we can find one that solves a similar problem.\n",
    "\n",
    "If we need a sparse model, we can use $\\ell_1$ regularization (and optionally zero out the tiny weights after training). If we need an even sparser model, we can use the TensorFlow Model Optimization Toolkit. This will break self-normalization, so we should use the default configuration in this case.\n",
    "\n",
    "If we need a low-latency model (one that performs lightning-fast predictions), we may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU.\n",
    "\n",
    "If we are building a risk-sensitive application, or inference latency is not very important in our application, we can use Monte Carlo Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.\n",
    "\n",
    "Finally, we may want to reduce the float precision from 32 bits to 16 or even 8 bits (e.g. to deploying a model to an embedded Device).\n",
    "\n",
    "It is possible to create really powerful DNS with Keras, however, we can need even more control (for example, to write a custom loss function or to tweak the training algorithm). For such cases we will need to use TensorFlow’s lower-level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Practice training a deep neural network on the CIFAR10 image dataset. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes. You can load it with keras.datasets.cifar10.load_ data()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 3.9728 - accuracy: 0.1676 - val_loss: 2.2328 - val_accuracy: 0.2040\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.0702 - accuracy: 0.2427 - val_loss: 2.0485 - val_accuracy: 0.2402\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.9416 - accuracy: 0.2920 - val_loss: 1.9681 - val_accuracy: 0.2964\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.8585 - accuracy: 0.3220 - val_loss: 1.9178 - val_accuracy: 0.3254\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7985 - accuracy: 0.3451 - val_loss: 1.8256 - val_accuracy: 0.3384\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7495 - accuracy: 0.3665 - val_loss: 1.7430 - val_accuracy: 0.3692\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.7072 - accuracy: 0.3825 - val_loss: 1.7199 - val_accuracy: 0.3824\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6735 - accuracy: 0.3959 - val_loss: 1.6746 - val_accuracy: 0.3972\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6407 - accuracy: 0.4087 - val_loss: 1.6622 - val_accuracy: 0.4004\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.6126 - accuracy: 0.4184 - val_loss: 1.7065 - val_accuracy: 0.3840\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5885 - accuracy: 0.4298 - val_loss: 1.6736 - val_accuracy: 0.3914\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5626 - accuracy: 0.4385 - val_loss: 1.6220 - val_accuracy: 0.4224\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5452 - accuracy: 0.4425 - val_loss: 1.6332 - val_accuracy: 0.4144\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5282 - accuracy: 0.4515 - val_loss: 1.5785 - val_accuracy: 0.4326\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5095 - accuracy: 0.4586 - val_loss: 1.6267 - val_accuracy: 0.4164\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4898 - accuracy: 0.4644 - val_loss: 1.5846 - val_accuracy: 0.4316\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4836 - accuracy: 0.4662 - val_loss: 1.5549 - val_accuracy: 0.4420\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4670 - accuracy: 0.4732 - val_loss: 1.5457 - val_accuracy: 0.4516\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4522 - accuracy: 0.4772 - val_loss: 1.5355 - val_accuracy: 0.4478\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4403 - accuracy: 0.4851 - val_loss: 1.5592 - val_accuracy: 0.4440\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4297 - accuracy: 0.4861 - val_loss: 1.5524 - val_accuracy: 0.4544\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4163 - accuracy: 0.4912 - val_loss: 1.5181 - val_accuracy: 0.4578\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4064 - accuracy: 0.4958 - val_loss: 1.5249 - val_accuracy: 0.4546\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3956 - accuracy: 0.4987 - val_loss: 1.5273 - val_accuracy: 0.4528\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3827 - accuracy: 0.5036 - val_loss: 1.5233 - val_accuracy: 0.4608\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3725 - accuracy: 0.5064 - val_loss: 1.5618 - val_accuracy: 0.4434\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3615 - accuracy: 0.5111 - val_loss: 1.4960 - val_accuracy: 0.4762\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3502 - accuracy: 0.5153 - val_loss: 1.5058 - val_accuracy: 0.4608\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3438 - accuracy: 0.5194 - val_loss: 1.5381 - val_accuracy: 0.4612\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3328 - accuracy: 0.5213 - val_loss: 1.6025 - val_accuracy: 0.4500\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3237 - accuracy: 0.5251 - val_loss: 1.5175 - val_accuracy: 0.4602\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3161 - accuracy: 0.5253 - val_loss: 1.5397 - val_accuracy: 0.4572\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3057 - accuracy: 0.5314 - val_loss: 1.4997 - val_accuracy: 0.4776\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3001 - accuracy: 0.5297 - val_loss: 1.5482 - val_accuracy: 0.4620\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2918 - accuracy: 0.5362 - val_loss: 1.5474 - val_accuracy: 0.4604\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2814 - accuracy: 0.5394 - val_loss: 1.5434 - val_accuracy: 0.4658\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2705 - accuracy: 0.5450 - val_loss: 1.5502 - val_accuracy: 0.4706\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2674 - accuracy: 0.5448 - val_loss: 1.5527 - val_accuracy: 0.4624\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2550 - accuracy: 0.5476 - val_loss: 1.5482 - val_accuracy: 0.4602\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2504 - accuracy: 0.5519 - val_loss: 1.5881 - val_accuracy: 0.4574\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2435 - accuracy: 0.5523 - val_loss: 1.5403 - val_accuracy: 0.4670\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2352 - accuracy: 0.5537 - val_loss: 1.5343 - val_accuracy: 0.4790\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2268 - accuracy: 0.5582 - val_loss: 1.5344 - val_accuracy: 0.4708\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2207 - accuracy: 0.5608 - val_loss: 1.5782 - val_accuracy: 0.4526\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2138 - accuracy: 0.5631 - val_loss: 1.5182 - val_accuracy: 0.4794\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2037 - accuracy: 0.5696 - val_loss: 1.5435 - val_accuracy: 0.4650\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1951 - accuracy: 0.5693 - val_loss: 1.5532 - val_accuracy: 0.4686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe78379dca0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.8411 - accuracy: 0.3426 - val_loss: 1.6602 - val_accuracy: 0.4042\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.6666 - accuracy: 0.4099 - val_loss: 1.5887 - val_accuracy: 0.4304\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5996 - accuracy: 0.4305 - val_loss: 1.5781 - val_accuracy: 0.4326\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5511 - accuracy: 0.4499 - val_loss: 1.5064 - val_accuracy: 0.4676\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5083 - accuracy: 0.4660 - val_loss: 1.4412 - val_accuracy: 0.4844\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4684 - accuracy: 0.4807 - val_loss: 1.4179 - val_accuracy: 0.4984\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4370 - accuracy: 0.4914 - val_loss: 1.4277 - val_accuracy: 0.4906\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4062 - accuracy: 0.5029 - val_loss: 1.3843 - val_accuracy: 0.5130\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3819 - accuracy: 0.5122 - val_loss: 1.3691 - val_accuracy: 0.5108\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3593 - accuracy: 0.5198 - val_loss: 1.3552 - val_accuracy: 0.5226\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3376 - accuracy: 0.5274 - val_loss: 1.3678 - val_accuracy: 0.5142\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3141 - accuracy: 0.5361 - val_loss: 1.3844 - val_accuracy: 0.5080\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2970 - accuracy: 0.5410 - val_loss: 1.3566 - val_accuracy: 0.5164\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2757 - accuracy: 0.5512 - val_loss: 1.3626 - val_accuracy: 0.5248\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2608 - accuracy: 0.5564 - val_loss: 1.3616 - val_accuracy: 0.5276\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2506 - accuracy: 0.5584 - val_loss: 1.3350 - val_accuracy: 0.5286\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2300 - accuracy: 0.5654 - val_loss: 1.3370 - val_accuracy: 0.5408\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2130 - accuracy: 0.5728 - val_loss: 1.3564 - val_accuracy: 0.5294\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2012 - accuracy: 0.5777 - val_loss: 1.3396 - val_accuracy: 0.5342\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1878 - accuracy: 0.5857 - val_loss: 1.3716 - val_accuracy: 0.5196\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1739 - accuracy: 0.5878 - val_loss: 1.3639 - val_accuracy: 0.5258\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1613 - accuracy: 0.5921 - val_loss: 1.3551 - val_accuracy: 0.5222\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1509 - accuracy: 0.5971 - val_loss: 1.3348 - val_accuracy: 0.5386\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1354 - accuracy: 0.5995 - val_loss: 1.3478 - val_accuracy: 0.5388\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1265 - accuracy: 0.6055 - val_loss: 1.3345 - val_accuracy: 0.5392\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1139 - accuracy: 0.6062 - val_loss: 1.3455 - val_accuracy: 0.5368\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1023 - accuracy: 0.6139 - val_loss: 1.3470 - val_accuracy: 0.5388\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0916 - accuracy: 0.6170 - val_loss: 1.3566 - val_accuracy: 0.5338\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0834 - accuracy: 0.6206 - val_loss: 1.3344 - val_accuracy: 0.5398\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0693 - accuracy: 0.6238 - val_loss: 1.3473 - val_accuracy: 0.5378\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0601 - accuracy: 0.6280 - val_loss: 1.3462 - val_accuracy: 0.5442\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0523 - accuracy: 0.6291 - val_loss: 1.3808 - val_accuracy: 0.5338\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0369 - accuracy: 0.6360 - val_loss: 1.3682 - val_accuracy: 0.5450\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0316 - accuracy: 0.6379 - val_loss: 1.3348 - val_accuracy: 0.5458\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0215 - accuracy: 0.6409 - val_loss: 1.3490 - val_accuracy: 0.5440\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0136 - accuracy: 0.6434 - val_loss: 1.3568 - val_accuracy: 0.5408\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9949 - accuracy: 0.6494 - val_loss: 1.3628 - val_accuracy: 0.5396\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9936 - accuracy: 0.6542 - val_loss: 1.3617 - val_accuracy: 0.5482\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9830 - accuracy: 0.6567 - val_loss: 1.3767 - val_accuracy: 0.5446\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9680 - accuracy: 0.6630 - val_loss: 1.4200 - val_accuracy: 0.5314\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9677 - accuracy: 0.6615 - val_loss: 1.3692 - val_accuracy: 0.5450\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9539 - accuracy: 0.6643 - val_loss: 1.3841 - val_accuracy: 0.5310\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9522 - accuracy: 0.6676 - val_loss: 1.4036 - val_accuracy: 0.5382\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9416 - accuracy: 0.6688 - val_loss: 1.4114 - val_accuracy: 0.5236\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9293 - accuracy: 0.6736 - val_loss: 1.4224 - val_accuracy: 0.5324\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9266 - accuracy: 0.6763 - val_loss: 1.3875 - val_accuracy: 0.5442\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9075 - accuracy: 0.6807 - val_loss: 1.4449 - val_accuracy: 0.5280\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9092 - accuracy: 0.6799 - val_loss: 1.4167 - val_accuracy: 0.5338\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8956 - accuracy: 0.6870 - val_loss: 1.4260 - val_accuracy: 0.5294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe730ba64f0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model converging faster than before? Much faster! The previous model took 39 epochs to reach the lowest validation loss, while the new model with BN took 18 epochs. That's more than twice as fast as the previous model. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "\n",
    "Does BN produce a better model? Yes! The final model is also much better, with 52% accuracy instead of 47%. It's still not a very good model, but at least it's much better than before.\n",
    "\n",
    "How does BN affect training speed? Although the model converged twice as fast, each epoch took more time, because of the extra computations required by the BN layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4  - Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network selfnormalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a\n",
    "sequence of dense layers, etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.9303 - accuracy: 0.3043 - val_loss: 1.7878 - val_accuracy: 0.3552\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.7113 - accuracy: 0.3908 - val_loss: 1.7028 - val_accuracy: 0.3828\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6190 - accuracy: 0.4301 - val_loss: 1.6692 - val_accuracy: 0.4022\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5512 - accuracy: 0.4539 - val_loss: 1.6350 - val_accuracy: 0.4300\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4944 - accuracy: 0.4766 - val_loss: 1.5773 - val_accuracy: 0.4356\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4473 - accuracy: 0.4909 - val_loss: 1.5529 - val_accuracy: 0.4630\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4061 - accuracy: 0.5091 - val_loss: 1.5290 - val_accuracy: 0.4682\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3619 - accuracy: 0.5246 - val_loss: 1.4633 - val_accuracy: 0.4792\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3266 - accuracy: 0.5414 - val_loss: 1.4787 - val_accuracy: 0.4776\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2991 - accuracy: 0.5490 - val_loss: 1.4794 - val_accuracy: 0.4934\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2671 - accuracy: 0.5612 - val_loss: 1.5529 - val_accuracy: 0.4982\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2378 - accuracy: 0.5749 - val_loss: 1.4942 - val_accuracy: 0.4902\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2140 - accuracy: 0.5779 - val_loss: 1.4868 - val_accuracy: 0.5024\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1901 - accuracy: 0.5918 - val_loss: 1.4839 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1629 - accuracy: 0.6004 - val_loss: 1.5097 - val_accuracy: 0.4968\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1331 - accuracy: 0.6101 - val_loss: 1.5001 - val_accuracy: 0.5120\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1144 - accuracy: 0.6156 - val_loss: 1.4856 - val_accuracy: 0.5056\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0962 - accuracy: 0.6244 - val_loss: 1.5116 - val_accuracy: 0.4966\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0790 - accuracy: 0.6313 - val_loss: 1.5787 - val_accuracy: 0.5070\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0551 - accuracy: 0.6397 - val_loss: 1.4987 - val_accuracy: 0.5144\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0367 - accuracy: 0.6477 - val_loss: 1.6292 - val_accuracy: 0.4462\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.0812 - accuracy: 0.6301 - val_loss: 1.5280 - val_accuracy: 0.5136\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9976 - accuracy: 0.6617 - val_loss: 1.5392 - val_accuracy: 0.5040\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9829 - accuracy: 0.6654 - val_loss: 1.5505 - val_accuracy: 0.5170\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.1855 - accuracy: 0.6488 - val_loss: 1.5468 - val_accuracy: 0.4992\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0176 - accuracy: 0.6510 - val_loss: 1.5474 - val_accuracy: 0.5020\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9565 - accuracy: 0.6734 - val_loss: 1.5545 - val_accuracy: 0.5076\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9167 - accuracy: 0.6870 - val_loss: 1.5659 - val_accuracy: 0.5138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe7836ab280>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 51.4% accuracy, which is better than the original model, but not quite as good as the model using batch normalization. Moreover, it took 13 epochs to reach the best model, which is much faster than both the original model and the BN model, plus each epoch took a short time just like the original model. So it's by far the fastest model to train (both in terms of epochs and wall time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 - Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.8973 - accuracy: 0.3245 - val_loss: 1.7629 - val_accuracy: 0.3738\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.6695 - accuracy: 0.4080 - val_loss: 1.6333 - val_accuracy: 0.4226\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5810 - accuracy: 0.4441 - val_loss: 1.6475 - val_accuracy: 0.4314\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.5112 - accuracy: 0.4739 - val_loss: 1.5459 - val_accuracy: 0.4624\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4553 - accuracy: 0.4964 - val_loss: 1.5467 - val_accuracy: 0.4726\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.4119 - accuracy: 0.5046 - val_loss: 1.5186 - val_accuracy: 0.4806\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3637 - accuracy: 0.5257 - val_loss: 1.5568 - val_accuracy: 0.4804\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3237 - accuracy: 0.5404 - val_loss: 1.4716 - val_accuracy: 0.4936\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2900 - accuracy: 0.5549 - val_loss: 1.5639 - val_accuracy: 0.4806\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.2594 - accuracy: 0.5633 - val_loss: 1.5023 - val_accuracy: 0.4910\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.2290 - accuracy: 0.5758 - val_loss: 1.5683 - val_accuracy: 0.5048\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1973 - accuracy: 0.5879 - val_loss: 1.5097 - val_accuracy: 0.5012\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1644 - accuracy: 0.5986 - val_loss: 1.5513 - val_accuracy: 0.5036\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.1423 - accuracy: 0.6074 - val_loss: 1.6108 - val_accuracy: 0.4994\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.1110 - accuracy: 0.6208 - val_loss: 1.5413 - val_accuracy: 0.5082\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.0902 - accuracy: 0.6275 - val_loss: 1.5833 - val_accuracy: 0.4988\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.0601 - accuracy: 0.6368 - val_loss: 1.6264 - val_accuracy: 0.5030\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.0411 - accuracy: 0.6447 - val_loss: 1.6524 - val_accuracy: 0.5072\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.0200 - accuracy: 0.6485 - val_loss: 1.6484 - val_accuracy: 0.5036\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9998 - accuracy: 0.6605 - val_loss: 1.6678 - val_accuracy: 0.5030\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 0.9827 - accuracy: 0.6677 - val_loss: 1.6860 - val_accuracy: 0.5018\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9664 - accuracy: 0.6696 - val_loss: 1.7288 - val_accuracy: 0.5082\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9462 - accuracy: 0.6791 - val_loss: 1.6977 - val_accuracy: 0.5090\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9255 - accuracy: 0.6862 - val_loss: 1.7885 - val_accuracy: 0.5112\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.9090 - accuracy: 0.6922 - val_loss: 1.7232 - val_accuracy: 0.5148\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.8840 - accuracy: 0.7026 - val_loss: 1.7941 - val_accuracy: 0.5008\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8733 - accuracy: 0.7056 - val_loss: 1.7456 - val_accuracy: 0.5066\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8617 - accuracy: 0.7093 - val_loss: 1.7443 - val_accuracy: 0.5056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe770af6be0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 50.8% accuracy on the validation set. That's very slightly worse than without dropout (51.4%). With an extensive hyperparameter search, it might be possible to do better, but probably not much better in this case.\n",
    "\n",
    "Let's use MC Dropout now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5048"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "    \n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get virtually no accuracy improvement in this case (from 50.8% to 50.9%).\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
